2025-12-31 08:14:58 - INFO - PRINT: WELCOME TO THE EXPERIMENT: 25_12_31_08:14:58_pet_pika_nearwall_boxfilter_4x_sr
2025-12-31 08:14:58 - INFO - PRINT: Using seed: 42
2025-12-31 08:14:59 - INFO - PRINT: Model param count: 1791238
2025-12-31 08:14:59 - INFO - PRINT: Getting timesteps available for channel_nearwall_filtered_fs4 in /home/rmcconke/orcd/scratch/numpy_4000
2025-12-31 08:14:59 - INFO - PRINT: Number of timesteps available: 4000
2025-12-31 08:14:59 - INFO - PRINT: Number of train samples: 1500
2025-12-31 08:14:59 - INFO - PRINT: Number of val samples: 800
2025-12-31 08:15:45 - INFO - PRINT: Epoch [   0], Train Loss: 0.5394, Validation Loss: 0.3044
2025-12-31 08:15:45 - INFO - PRINT: ----> Saving model from epoch 0 (val loss: 0.3043575096130371). Fragrant!
2025-12-31 08:16:11 - INFO - PRINT: Epoch [   1], Train Loss: 0.2737, Validation Loss: 0.3044
2025-12-31 08:16:38 - INFO - PRINT: Epoch [   2], Train Loss: 0.2462, Validation Loss: 0.3044
2025-12-31 08:17:05 - INFO - PRINT: Epoch [   3], Train Loss: 0.2403, Validation Loss: 0.3044
2025-12-31 08:17:31 - INFO - PRINT: Epoch [   4], Train Loss: 0.2283, Validation Loss: 0.3044
2025-12-31 08:17:58 - INFO - PRINT: Epoch [   5], Train Loss: 0.2150, Validation Loss: 0.3044
2025-12-31 08:18:25 - INFO - PRINT: Epoch [   6], Train Loss: 0.2031, Validation Loss: 0.3044
2025-12-31 08:18:52 - INFO - PRINT: Epoch [   7], Train Loss: 0.1977, Validation Loss: 0.3044
2025-12-31 08:19:18 - INFO - PRINT: Epoch [   8], Train Loss: 0.1889, Validation Loss: 0.3044
2025-12-31 08:19:45 - INFO - PRINT: Epoch [   9], Train Loss: 0.1857, Validation Loss: 0.3044
2025-12-31 08:20:12 - INFO - PRINT: Epoch [  10], Train Loss: 0.1829, Validation Loss: 0.3044
2025-12-31 08:20:39 - INFO - PRINT: Epoch [  11], Train Loss: 0.1816, Validation Loss: 0.3044
2025-12-31 08:21:05 - INFO - PRINT: Epoch [  12], Train Loss: 0.1735, Validation Loss: 0.3044
2025-12-31 08:21:32 - INFO - PRINT: Epoch [  13], Train Loss: 0.1758, Validation Loss: 0.3044
2025-12-31 08:21:59 - INFO - PRINT: Epoch [  14], Train Loss: 0.1698, Validation Loss: 0.3044
2025-12-31 08:22:26 - INFO - PRINT: Epoch [  15], Train Loss: 0.1681, Validation Loss: 0.3044
2025-12-31 08:22:52 - INFO - PRINT: Epoch [  16], Train Loss: 0.1691, Validation Loss: 0.3044
2025-12-31 08:23:19 - INFO - PRINT: Epoch [  17], Train Loss: 0.1662, Validation Loss: 0.3044
2025-12-31 08:23:46 - INFO - PRINT: Epoch [  18], Train Loss: 0.1657, Validation Loss: 0.3044
2025-12-31 08:24:13 - INFO - PRINT: Epoch [  19], Train Loss: 0.1639, Validation Loss: 0.3044
2025-12-31 08:24:44 - INFO - PRINT: Epoch [  20], Train Loss: 0.1618, Validation Loss: 0.1525
2025-12-31 08:25:10 - INFO - PRINT: Epoch [  21], Train Loss: 0.1589, Validation Loss: 0.1525
2025-12-31 08:25:37 - INFO - PRINT: Epoch [  22], Train Loss: 0.1573, Validation Loss: 0.1525
2025-12-31 08:26:04 - INFO - PRINT: Epoch [  23], Train Loss: 0.1574, Validation Loss: 0.1525
2025-12-31 08:26:30 - INFO - PRINT: Epoch [  24], Train Loss: 0.1565, Validation Loss: 0.1525
2025-12-31 08:26:57 - INFO - PRINT: Epoch [  25], Train Loss: 0.1546, Validation Loss: 0.1525
2025-12-31 08:27:24 - INFO - PRINT: Epoch [  26], Train Loss: 0.1526, Validation Loss: 0.1525
2025-12-31 08:27:51 - INFO - PRINT: Epoch [  27], Train Loss: 0.1545, Validation Loss: 0.1525
2025-12-31 08:28:17 - INFO - PRINT: Epoch [  28], Train Loss: 0.1549, Validation Loss: 0.1525
2025-12-31 08:28:44 - INFO - PRINT: Epoch [  29], Train Loss: 0.1514, Validation Loss: 0.1525
2025-12-31 08:29:11 - INFO - PRINT: Epoch [  30], Train Loss: 0.1496, Validation Loss: 0.1525
2025-12-31 08:29:38 - INFO - PRINT: Epoch [  31], Train Loss: 0.1537, Validation Loss: 0.1525
2025-12-31 08:30:04 - INFO - PRINT: Epoch [  32], Train Loss: 0.1509, Validation Loss: 0.1525
2025-12-31 08:30:31 - INFO - PRINT: Epoch [  33], Train Loss: 0.1500, Validation Loss: 0.1525
2025-12-31 08:30:58 - INFO - PRINT: Epoch [  34], Train Loss: 0.1486, Validation Loss: 0.1525
2025-12-31 08:31:25 - INFO - PRINT: Epoch [  35], Train Loss: 0.1476, Validation Loss: 0.1525
2025-12-31 08:31:51 - INFO - PRINT: Epoch [  36], Train Loss: 0.1470, Validation Loss: 0.1525
2025-12-31 08:32:18 - INFO - PRINT: Epoch [  37], Train Loss: 0.1468, Validation Loss: 0.1525
2025-12-31 08:32:45 - INFO - PRINT: Epoch [  38], Train Loss: 0.1466, Validation Loss: 0.1525
2025-12-31 08:33:12 - INFO - PRINT: Epoch [  39], Train Loss: 0.1452, Validation Loss: 0.1525
2025-12-31 08:33:43 - INFO - PRINT: Epoch [  40], Train Loss: 0.1461, Validation Loss: 0.1424
2025-12-31 08:34:09 - INFO - PRINT: Epoch [  41], Train Loss: 0.1419, Validation Loss: 0.1424
2025-12-31 08:34:36 - INFO - PRINT: Epoch [  42], Train Loss: 0.1423, Validation Loss: 0.1424
2025-12-31 08:35:03 - INFO - PRINT: Epoch [  43], Train Loss: 0.1427, Validation Loss: 0.1424
2025-12-31 08:35:29 - INFO - PRINT: Epoch [  44], Train Loss: 0.1456, Validation Loss: 0.1424
2025-12-31 08:35:56 - INFO - PRINT: Epoch [  45], Train Loss: 0.1453, Validation Loss: 0.1424
2025-12-31 08:36:23 - INFO - PRINT: Epoch [  46], Train Loss: 0.1437, Validation Loss: 0.1424
2025-12-31 08:36:50 - INFO - PRINT: Epoch [  47], Train Loss: 0.1402, Validation Loss: 0.1424
2025-12-31 08:37:16 - INFO - PRINT: Epoch [  48], Train Loss: 0.1440, Validation Loss: 0.1424
2025-12-31 08:37:43 - INFO - PRINT: Epoch [  49], Train Loss: 0.1400, Validation Loss: 0.1424
2025-12-31 08:38:10 - INFO - PRINT: Epoch [  50], Train Loss: 0.1409, Validation Loss: 0.1424
2025-12-31 08:38:37 - INFO - PRINT: Epoch [  51], Train Loss: 0.1393, Validation Loss: 0.1424
2025-12-31 08:39:03 - INFO - PRINT: Epoch [  52], Train Loss: 0.1379, Validation Loss: 0.1424
2025-12-31 08:39:30 - INFO - PRINT: Epoch [  53], Train Loss: 0.1375, Validation Loss: 0.1424
2025-12-31 08:39:57 - INFO - PRINT: Epoch [  54], Train Loss: 0.1423, Validation Loss: 0.1424
2025-12-31 08:40:24 - INFO - PRINT: Epoch [  55], Train Loss: 0.1367, Validation Loss: 0.1424
2025-12-31 08:40:50 - INFO - PRINT: Epoch [  56], Train Loss: 0.1401, Validation Loss: 0.1424
2025-12-31 08:41:17 - INFO - PRINT: Epoch [  57], Train Loss: 0.1373, Validation Loss: 0.1424
2025-12-31 08:41:44 - INFO - PRINT: Epoch [  58], Train Loss: 0.1398, Validation Loss: 0.1424
2025-12-31 08:42:11 - INFO - PRINT: Epoch [  59], Train Loss: 0.1406, Validation Loss: 0.1424
2025-12-31 08:42:42 - INFO - PRINT: Epoch [  60], Train Loss: 0.1360, Validation Loss: 0.1316
2025-12-31 08:43:08 - INFO - PRINT: Epoch [  61], Train Loss: 0.1353, Validation Loss: 0.1316
2025-12-31 08:43:35 - INFO - PRINT: Epoch [  62], Train Loss: 0.1352, Validation Loss: 0.1316
2025-12-31 08:44:02 - INFO - PRINT: Epoch [  63], Train Loss: 0.1349, Validation Loss: 0.1316
2025-12-31 08:44:29 - INFO - PRINT: Epoch [  64], Train Loss: 0.1363, Validation Loss: 0.1316
2025-12-31 08:44:55 - INFO - PRINT: Epoch [  65], Train Loss: 0.1394, Validation Loss: 0.1316
2025-12-31 08:45:22 - INFO - PRINT: Epoch [  66], Train Loss: 0.1383, Validation Loss: 0.1316
2025-12-31 08:45:49 - INFO - PRINT: Epoch [  67], Train Loss: 0.1349, Validation Loss: 0.1316
2025-12-31 08:46:16 - INFO - PRINT: Epoch [  68], Train Loss: 0.1355, Validation Loss: 0.1316
2025-12-31 08:46:42 - INFO - PRINT: Epoch [  69], Train Loss: 0.1339, Validation Loss: 0.1316
2025-12-31 08:47:09 - INFO - PRINT: Epoch [  70], Train Loss: 0.1342, Validation Loss: 0.1316
2025-12-31 08:47:36 - INFO - PRINT: Epoch [  71], Train Loss: 0.1360, Validation Loss: 0.1316
2025-12-31 08:48:03 - INFO - PRINT: Epoch [  72], Train Loss: 0.1354, Validation Loss: 0.1316
2025-12-31 08:48:29 - INFO - PRINT: Epoch [  73], Train Loss: 0.1337, Validation Loss: 0.1316
2025-12-31 08:48:56 - INFO - PRINT: Epoch [  74], Train Loss: 0.1338, Validation Loss: 0.1316
2025-12-31 08:49:23 - INFO - PRINT: Epoch [  75], Train Loss: 0.1325, Validation Loss: 0.1316
2025-12-31 08:49:50 - INFO - PRINT: Epoch [  76], Train Loss: 0.1329, Validation Loss: 0.1316
2025-12-31 08:50:16 - INFO - PRINT: Epoch [  77], Train Loss: 0.1339, Validation Loss: 0.1316
2025-12-31 08:50:43 - INFO - PRINT: Epoch [  78], Train Loss: 0.1329, Validation Loss: 0.1316
2025-12-31 08:51:10 - INFO - PRINT: Epoch [  79], Train Loss: 0.1313, Validation Loss: 0.1316
2025-12-31 08:51:41 - INFO - PRINT: Epoch [  80], Train Loss: 0.1317, Validation Loss: 0.1266
2025-12-31 08:52:07 - INFO - PRINT: Epoch [  81], Train Loss: 0.1309, Validation Loss: 0.1266
2025-12-31 08:52:34 - INFO - PRINT: Epoch [  82], Train Loss: 0.1342, Validation Loss: 0.1266
2025-12-31 08:53:01 - INFO - PRINT: Epoch [  83], Train Loss: 0.1347, Validation Loss: 0.1266
2025-12-31 08:53:28 - INFO - PRINT: Epoch [  84], Train Loss: 0.1304, Validation Loss: 0.1266
2025-12-31 08:53:54 - INFO - PRINT: Epoch [  85], Train Loss: 0.1323, Validation Loss: 0.1266
2025-12-31 08:54:21 - INFO - PRINT: Epoch [  86], Train Loss: 0.1292, Validation Loss: 0.1266
2025-12-31 08:54:48 - INFO - PRINT: Epoch [  87], Train Loss: 0.1306, Validation Loss: 0.1266
2025-12-31 08:55:15 - INFO - PRINT: Epoch [  88], Train Loss: 0.1303, Validation Loss: 0.1266
2025-12-31 08:55:41 - INFO - PRINT: Epoch [  89], Train Loss: 0.1293, Validation Loss: 0.1266
2025-12-31 08:56:08 - INFO - PRINT: Epoch [  90], Train Loss: 0.1310, Validation Loss: 0.1266
2025-12-31 08:56:35 - INFO - PRINT: Epoch [  91], Train Loss: 0.1316, Validation Loss: 0.1266
2025-12-31 08:57:02 - INFO - PRINT: Epoch [  92], Train Loss: 0.1315, Validation Loss: 0.1266
2025-12-31 08:57:28 - INFO - PRINT: Epoch [  93], Train Loss: 0.1290, Validation Loss: 0.1266
2025-12-31 08:57:55 - INFO - PRINT: Epoch [  94], Train Loss: 0.1296, Validation Loss: 0.1266
2025-12-31 08:58:22 - INFO - PRINT: Epoch [  95], Train Loss: 0.1295, Validation Loss: 0.1266
2025-12-31 08:58:49 - INFO - PRINT: Epoch [  96], Train Loss: 0.1294, Validation Loss: 0.1266
2025-12-31 08:59:15 - INFO - PRINT: Epoch [  97], Train Loss: 0.1268, Validation Loss: 0.1266
2025-12-31 08:59:42 - INFO - PRINT: Epoch [  98], Train Loss: 0.1315, Validation Loss: 0.1266
2025-12-31 09:00:09 - INFO - PRINT: Epoch [  99], Train Loss: 0.1275, Validation Loss: 0.1266
2025-12-31 09:00:40 - INFO - PRINT: Epoch [ 100], Train Loss: 0.1301, Validation Loss: 0.1268
2025-12-31 09:00:40 - INFO - PRINT: ----> Saving model from epoch 80 (val loss: 0.12659482300281524). Enticing!
2025-12-31 09:01:07 - INFO - PRINT: Epoch [ 101], Train Loss: 0.1311, Validation Loss: 0.1268
2025-12-31 09:01:33 - INFO - PRINT: Epoch [ 102], Train Loss: 0.1293, Validation Loss: 0.1268
2025-12-31 09:02:00 - INFO - PRINT: Epoch [ 103], Train Loss: 0.1269, Validation Loss: 0.1268
2025-12-31 09:02:27 - INFO - PRINT: Epoch [ 104], Train Loss: 0.1275, Validation Loss: 0.1268
2025-12-31 09:02:54 - INFO - PRINT: Epoch [ 105], Train Loss: 0.1266, Validation Loss: 0.1268
2025-12-31 09:03:20 - INFO - PRINT: Epoch [ 106], Train Loss: 0.1287, Validation Loss: 0.1268
2025-12-31 09:03:47 - INFO - PRINT: Epoch [ 107], Train Loss: 0.1273, Validation Loss: 0.1268
2025-12-31 09:04:14 - INFO - PRINT: Epoch [ 108], Train Loss: 0.1265, Validation Loss: 0.1268
2025-12-31 09:04:41 - INFO - PRINT: Epoch [ 109], Train Loss: 0.1260, Validation Loss: 0.1268
2025-12-31 09:05:07 - INFO - PRINT: Epoch [ 110], Train Loss: 0.1303, Validation Loss: 0.1268
2025-12-31 09:05:34 - INFO - PRINT: Epoch [ 111], Train Loss: 0.1278, Validation Loss: 0.1268
2025-12-31 09:06:01 - INFO - PRINT: Epoch [ 112], Train Loss: 0.1248, Validation Loss: 0.1268
2025-12-31 09:06:28 - INFO - PRINT: Epoch [ 113], Train Loss: 0.1261, Validation Loss: 0.1268
2025-12-31 09:06:54 - INFO - PRINT: Epoch [ 114], Train Loss: 0.1257, Validation Loss: 0.1268
2025-12-31 09:07:21 - INFO - PRINT: Epoch [ 115], Train Loss: 0.1263, Validation Loss: 0.1268
2025-12-31 09:07:48 - INFO - PRINT: Epoch [ 116], Train Loss: 0.1275, Validation Loss: 0.1268
2025-12-31 09:08:15 - INFO - PRINT: Epoch [ 117], Train Loss: 0.1259, Validation Loss: 0.1268
2025-12-31 09:08:41 - INFO - PRINT: Epoch [ 118], Train Loss: 0.1241, Validation Loss: 0.1268
2025-12-31 09:09:08 - INFO - PRINT: Epoch [ 119], Train Loss: 0.1263, Validation Loss: 0.1268
2025-12-31 09:09:39 - INFO - PRINT: Epoch [ 120], Train Loss: 0.1263, Validation Loss: 0.1281
2025-12-31 09:10:06 - INFO - PRINT: Epoch [ 121], Train Loss: 0.1261, Validation Loss: 0.1281
2025-12-31 09:10:33 - INFO - PRINT: Epoch [ 122], Train Loss: 0.1255, Validation Loss: 0.1281
2025-12-31 09:10:59 - INFO - PRINT: Epoch [ 123], Train Loss: 0.1251, Validation Loss: 0.1281
2025-12-31 09:11:26 - INFO - PRINT: Epoch [ 124], Train Loss: 0.1244, Validation Loss: 0.1281
2025-12-31 09:11:53 - INFO - PRINT: Epoch [ 125], Train Loss: 0.1241, Validation Loss: 0.1281
2025-12-31 09:12:20 - INFO - PRINT: Epoch [ 126], Train Loss: 0.1253, Validation Loss: 0.1281
2025-12-31 09:12:46 - INFO - PRINT: Epoch [ 127], Train Loss: 0.1270, Validation Loss: 0.1281
2025-12-31 09:13:13 - INFO - PRINT: Epoch [ 128], Train Loss: 0.1261, Validation Loss: 0.1281
2025-12-31 09:13:40 - INFO - PRINT: Epoch [ 129], Train Loss: 0.1231, Validation Loss: 0.1281
2025-12-31 09:14:07 - INFO - PRINT: Epoch [ 130], Train Loss: 0.1236, Validation Loss: 0.1281
2025-12-31 09:14:33 - INFO - PRINT: Epoch [ 131], Train Loss: 0.1225, Validation Loss: 0.1281
2025-12-31 09:15:00 - INFO - PRINT: Epoch [ 132], Train Loss: 0.1256, Validation Loss: 0.1281
2025-12-31 09:15:27 - INFO - PRINT: Epoch [ 133], Train Loss: 0.1240, Validation Loss: 0.1281
2025-12-31 09:15:54 - INFO - PRINT: Epoch [ 134], Train Loss: 0.1251, Validation Loss: 0.1281
2025-12-31 09:16:20 - INFO - PRINT: Epoch [ 135], Train Loss: 0.1238, Validation Loss: 0.1281
2025-12-31 09:16:47 - INFO - PRINT: Epoch [ 136], Train Loss: 0.1236, Validation Loss: 0.1281
2025-12-31 09:17:14 - INFO - PRINT: Epoch [ 137], Train Loss: 0.1235, Validation Loss: 0.1281
2025-12-31 09:17:41 - INFO - PRINT: Epoch [ 138], Train Loss: 0.1246, Validation Loss: 0.1281
2025-12-31 09:18:07 - INFO - PRINT: Epoch [ 139], Train Loss: 0.1232, Validation Loss: 0.1281
2025-12-31 09:18:38 - INFO - PRINT: Epoch [ 140], Train Loss: 0.1220, Validation Loss: 0.1197
2025-12-31 09:19:05 - INFO - PRINT: Epoch [ 141], Train Loss: 0.1230, Validation Loss: 0.1197
2025-12-31 09:19:32 - INFO - PRINT: Epoch [ 142], Train Loss: 0.1236, Validation Loss: 0.1197
2025-12-31 09:19:58 - INFO - PRINT: Epoch [ 143], Train Loss: 0.1220, Validation Loss: 0.1197
2025-12-31 09:20:25 - INFO - PRINT: Epoch [ 144], Train Loss: 0.1236, Validation Loss: 0.1197
2025-12-31 09:20:52 - INFO - PRINT: Epoch [ 145], Train Loss: 0.1227, Validation Loss: 0.1197
2025-12-31 09:21:19 - INFO - PRINT: Epoch [ 146], Train Loss: 0.1225, Validation Loss: 0.1197
2025-12-31 09:21:45 - INFO - PRINT: Epoch [ 147], Train Loss: 0.1217, Validation Loss: 0.1197
2025-12-31 09:22:12 - INFO - PRINT: Epoch [ 148], Train Loss: 0.1218, Validation Loss: 0.1197
2025-12-31 09:22:39 - INFO - PRINT: Epoch [ 149], Train Loss: 0.1231, Validation Loss: 0.1197
2025-12-31 09:23:06 - INFO - PRINT: Epoch [ 150], Train Loss: 0.1225, Validation Loss: 0.1197
2025-12-31 09:23:32 - INFO - PRINT: Epoch [ 151], Train Loss: 0.1235, Validation Loss: 0.1197
2025-12-31 09:23:59 - INFO - PRINT: Epoch [ 152], Train Loss: 0.1229, Validation Loss: 0.1197
2025-12-31 09:24:26 - INFO - PRINT: Epoch [ 153], Train Loss: 0.1215, Validation Loss: 0.1197
2025-12-31 09:24:52 - INFO - PRINT: Epoch [ 154], Train Loss: 0.1222, Validation Loss: 0.1197
2025-12-31 09:25:19 - INFO - PRINT: Epoch [ 155], Train Loss: 0.1218, Validation Loss: 0.1197
2025-12-31 09:25:46 - INFO - PRINT: Epoch [ 156], Train Loss: 0.1198, Validation Loss: 0.1197
2025-12-31 09:26:13 - INFO - PRINT: Epoch [ 157], Train Loss: 0.1204, Validation Loss: 0.1197
2025-12-31 09:26:39 - INFO - PRINT: Epoch [ 158], Train Loss: 0.1222, Validation Loss: 0.1197
2025-12-31 09:27:06 - INFO - PRINT: Epoch [ 159], Train Loss: 0.1203, Validation Loss: 0.1197
2025-12-31 09:27:37 - INFO - PRINT: Epoch [ 160], Train Loss: 0.1210, Validation Loss: 0.1179
2025-12-31 09:28:04 - INFO - PRINT: Epoch [ 161], Train Loss: 0.1208, Validation Loss: 0.1179
2025-12-31 09:28:31 - INFO - PRINT: Epoch [ 162], Train Loss: 0.1203, Validation Loss: 0.1179
2025-12-31 09:28:57 - INFO - PRINT: Epoch [ 163], Train Loss: 0.1207, Validation Loss: 0.1179
2025-12-31 09:29:24 - INFO - PRINT: Epoch [ 164], Train Loss: 0.1196, Validation Loss: 0.1179
2025-12-31 09:29:51 - INFO - PRINT: Epoch [ 165], Train Loss: 0.1201, Validation Loss: 0.1179
2025-12-31 09:30:17 - INFO - PRINT: Epoch [ 166], Train Loss: 0.1193, Validation Loss: 0.1179
2025-12-31 09:30:44 - INFO - PRINT: Epoch [ 167], Train Loss: 0.1196, Validation Loss: 0.1179
2025-12-31 09:31:11 - INFO - PRINT: Epoch [ 168], Train Loss: 0.1205, Validation Loss: 0.1179
2025-12-31 09:31:38 - INFO - PRINT: Epoch [ 169], Train Loss: 0.1190, Validation Loss: 0.1179
2025-12-31 09:32:04 - INFO - PRINT: Epoch [ 170], Train Loss: 0.1204, Validation Loss: 0.1179
2025-12-31 09:32:31 - INFO - PRINT: Epoch [ 171], Train Loss: 0.1193, Validation Loss: 0.1179
2025-12-31 09:32:58 - INFO - PRINT: Epoch [ 172], Train Loss: 0.1199, Validation Loss: 0.1179
2025-12-31 09:33:25 - INFO - PRINT: Epoch [ 173], Train Loss: 0.1200, Validation Loss: 0.1179
2025-12-31 09:33:51 - INFO - PRINT: Epoch [ 174], Train Loss: 0.1208, Validation Loss: 0.1179
2025-12-31 09:34:18 - INFO - PRINT: Epoch [ 175], Train Loss: 0.1192, Validation Loss: 0.1179
2025-12-31 09:34:45 - INFO - PRINT: Epoch [ 176], Train Loss: 0.1184, Validation Loss: 0.1179
2025-12-31 09:35:12 - INFO - PRINT: Epoch [ 177], Train Loss: 0.1204, Validation Loss: 0.1179
2025-12-31 09:35:38 - INFO - PRINT: Epoch [ 178], Train Loss: 0.1178, Validation Loss: 0.1179
2025-12-31 09:36:05 - INFO - PRINT: Epoch [ 179], Train Loss: 0.1203, Validation Loss: 0.1179
2025-12-31 09:36:36 - INFO - PRINT: Epoch [ 180], Train Loss: 0.1193, Validation Loss: 0.1160
2025-12-31 09:37:03 - INFO - PRINT: Epoch [ 181], Train Loss: 0.1209, Validation Loss: 0.1160
2025-12-31 09:37:29 - INFO - PRINT: Epoch [ 182], Train Loss: 0.1187, Validation Loss: 0.1160
2025-12-31 09:37:56 - INFO - PRINT: Epoch [ 183], Train Loss: 0.1183, Validation Loss: 0.1160
2025-12-31 09:38:23 - INFO - PRINT: Epoch [ 184], Train Loss: 0.1177, Validation Loss: 0.1160
2025-12-31 09:38:50 - INFO - PRINT: Epoch [ 185], Train Loss: 0.1176, Validation Loss: 0.1160
2025-12-31 09:39:16 - INFO - PRINT: Epoch [ 186], Train Loss: 0.1180, Validation Loss: 0.1160
2025-12-31 09:39:43 - INFO - PRINT: Epoch [ 187], Train Loss: 0.1191, Validation Loss: 0.1160
2025-12-31 09:40:10 - INFO - PRINT: Epoch [ 188], Train Loss: 0.1192, Validation Loss: 0.1160
2025-12-31 09:40:37 - INFO - PRINT: Epoch [ 189], Train Loss: 0.1170, Validation Loss: 0.1160
2025-12-31 09:41:03 - INFO - PRINT: Epoch [ 190], Train Loss: 0.1169, Validation Loss: 0.1160
2025-12-31 09:41:30 - INFO - PRINT: Epoch [ 191], Train Loss: 0.1186, Validation Loss: 0.1160
2025-12-31 09:41:57 - INFO - PRINT: Epoch [ 192], Train Loss: 0.1189, Validation Loss: 0.1160
2025-12-31 09:42:24 - INFO - PRINT: Epoch [ 193], Train Loss: 0.1169, Validation Loss: 0.1160
2025-12-31 09:42:50 - INFO - PRINT: Epoch [ 194], Train Loss: 0.1168, Validation Loss: 0.1160
2025-12-31 09:43:17 - INFO - PRINT: Epoch [ 195], Train Loss: 0.1156, Validation Loss: 0.1160
2025-12-31 09:43:44 - INFO - PRINT: Epoch [ 196], Train Loss: 0.1182, Validation Loss: 0.1160
2025-12-31 09:44:11 - INFO - PRINT: Epoch [ 197], Train Loss: 0.1197, Validation Loss: 0.1160
2025-12-31 09:44:37 - INFO - PRINT: Epoch [ 198], Train Loss: 0.1163, Validation Loss: 0.1160
2025-12-31 09:45:04 - INFO - PRINT: Epoch [ 199], Train Loss: 0.1173, Validation Loss: 0.1160
2025-12-31 09:45:35 - INFO - PRINT: Epoch [ 200], Train Loss: 0.1171, Validation Loss: 0.1130
2025-12-31 09:45:35 - INFO - PRINT: ----> Saving model from epoch 200 (val loss: 0.11298742711544037). Chewy!
2025-12-31 09:46:02 - INFO - PRINT: Epoch [ 201], Train Loss: 0.1161, Validation Loss: 0.1130
2025-12-31 09:46:28 - INFO - PRINT: Epoch [ 202], Train Loss: 0.1163, Validation Loss: 0.1130
2025-12-31 09:46:55 - INFO - PRINT: Epoch [ 203], Train Loss: 0.1184, Validation Loss: 0.1130
2025-12-31 09:47:22 - INFO - PRINT: Epoch [ 204], Train Loss: 0.1159, Validation Loss: 0.1130
2025-12-31 09:47:49 - INFO - PRINT: Epoch [ 205], Train Loss: 0.1152, Validation Loss: 0.1130
2025-12-31 09:48:15 - INFO - PRINT: Epoch [ 206], Train Loss: 0.1160, Validation Loss: 0.1130
2025-12-31 09:48:42 - INFO - PRINT: Epoch [ 207], Train Loss: 0.1152, Validation Loss: 0.1130
2025-12-31 09:49:09 - INFO - PRINT: Epoch [ 208], Train Loss: 0.1167, Validation Loss: 0.1130
2025-12-31 09:49:36 - INFO - PRINT: Epoch [ 209], Train Loss: 0.1185, Validation Loss: 0.1130
2025-12-31 09:50:02 - INFO - PRINT: Epoch [ 210], Train Loss: 0.1171, Validation Loss: 0.1130
2025-12-31 09:50:29 - INFO - PRINT: Epoch [ 211], Train Loss: 0.1151, Validation Loss: 0.1130
2025-12-31 09:50:56 - INFO - PRINT: Epoch [ 212], Train Loss: 0.1147, Validation Loss: 0.1130
2025-12-31 09:51:23 - INFO - PRINT: Epoch [ 213], Train Loss: 0.1158, Validation Loss: 0.1130
2025-12-31 09:51:49 - INFO - PRINT: Epoch [ 214], Train Loss: 0.1189, Validation Loss: 0.1130
2025-12-31 09:52:16 - INFO - PRINT: Epoch [ 215], Train Loss: 0.1150, Validation Loss: 0.1130
2025-12-31 09:52:43 - INFO - PRINT: Epoch [ 216], Train Loss: 0.1156, Validation Loss: 0.1130
2025-12-31 09:53:10 - INFO - PRINT: Epoch [ 217], Train Loss: 0.1152, Validation Loss: 0.1130
2025-12-31 09:53:36 - INFO - PRINT: Epoch [ 218], Train Loss: 0.1165, Validation Loss: 0.1130
2025-12-31 09:54:03 - INFO - PRINT: Epoch [ 219], Train Loss: 0.1162, Validation Loss: 0.1130
2025-12-31 09:54:34 - INFO - PRINT: Epoch [ 220], Train Loss: 0.1152, Validation Loss: 0.1127
2025-12-31 09:55:01 - INFO - PRINT: Epoch [ 221], Train Loss: 0.1148, Validation Loss: 0.1127
2025-12-31 09:55:27 - INFO - PRINT: Epoch [ 222], Train Loss: 0.1151, Validation Loss: 0.1127
2025-12-31 09:55:54 - INFO - PRINT: Epoch [ 223], Train Loss: 0.1164, Validation Loss: 0.1127
2025-12-31 09:56:21 - INFO - PRINT: Epoch [ 224], Train Loss: 0.1155, Validation Loss: 0.1127
2025-12-31 09:56:48 - INFO - PRINT: Epoch [ 225], Train Loss: 0.1143, Validation Loss: 0.1127
2025-12-31 09:57:14 - INFO - PRINT: Epoch [ 226], Train Loss: 0.1152, Validation Loss: 0.1127
2025-12-31 09:57:41 - INFO - PRINT: Epoch [ 227], Train Loss: 0.1169, Validation Loss: 0.1127
2025-12-31 09:58:08 - INFO - PRINT: Epoch [ 228], Train Loss: 0.1144, Validation Loss: 0.1127
2025-12-31 09:58:35 - INFO - PRINT: Epoch [ 229], Train Loss: 0.1144, Validation Loss: 0.1127
2025-12-31 09:59:02 - INFO - PRINT: Epoch [ 230], Train Loss: 0.1145, Validation Loss: 0.1127
2025-12-31 09:59:28 - INFO - PRINT: Epoch [ 231], Train Loss: 0.1140, Validation Loss: 0.1127
2025-12-31 09:59:55 - INFO - PRINT: Epoch [ 232], Train Loss: 0.1146, Validation Loss: 0.1127
2025-12-31 10:00:22 - INFO - PRINT: Epoch [ 233], Train Loss: 0.1149, Validation Loss: 0.1127
2025-12-31 10:00:49 - INFO - PRINT: Epoch [ 234], Train Loss: 0.1145, Validation Loss: 0.1127
2025-12-31 10:01:15 - INFO - PRINT: Epoch [ 235], Train Loss: 0.1146, Validation Loss: 0.1127
2025-12-31 10:01:42 - INFO - PRINT: Epoch [ 236], Train Loss: 0.1164, Validation Loss: 0.1127
2025-12-31 10:02:09 - INFO - PRINT: Epoch [ 237], Train Loss: 0.1173, Validation Loss: 0.1127
2025-12-31 10:02:36 - INFO - PRINT: Epoch [ 238], Train Loss: 0.1166, Validation Loss: 0.1127
2025-12-31 10:03:02 - INFO - PRINT: Epoch [ 239], Train Loss: 0.1142, Validation Loss: 0.1127
2025-12-31 10:03:33 - INFO - PRINT: Epoch [ 240], Train Loss: 0.1142, Validation Loss: 0.1115
2025-12-31 10:04:00 - INFO - PRINT: Epoch [ 241], Train Loss: 0.1150, Validation Loss: 0.1115
2025-12-31 10:04:27 - INFO - PRINT: Epoch [ 242], Train Loss: 0.1142, Validation Loss: 0.1115
2025-12-31 10:04:53 - INFO - PRINT: Epoch [ 243], Train Loss: 0.1155, Validation Loss: 0.1115
2025-12-31 10:05:20 - INFO - PRINT: Epoch [ 244], Train Loss: 0.1171, Validation Loss: 0.1115
2025-12-31 10:05:47 - INFO - PRINT: Epoch [ 245], Train Loss: 0.1131, Validation Loss: 0.1115
2025-12-31 10:06:14 - INFO - PRINT: Epoch [ 246], Train Loss: 0.1137, Validation Loss: 0.1115
2025-12-31 10:06:40 - INFO - PRINT: Epoch [ 247], Train Loss: 0.1161, Validation Loss: 0.1115
2025-12-31 10:07:07 - INFO - PRINT: Epoch [ 248], Train Loss: 0.1132, Validation Loss: 0.1115
2025-12-31 10:07:34 - INFO - PRINT: Epoch [ 249], Train Loss: 0.1130, Validation Loss: 0.1115
2025-12-31 10:08:00 - INFO - PRINT: Epoch [ 250], Train Loss: 0.1151, Validation Loss: 0.1115
2025-12-31 10:08:27 - INFO - PRINT: Epoch [ 251], Train Loss: 0.1139, Validation Loss: 0.1115
2025-12-31 10:08:54 - INFO - PRINT: Epoch [ 252], Train Loss: 0.1144, Validation Loss: 0.1115
2025-12-31 10:09:21 - INFO - PRINT: Epoch [ 253], Train Loss: 0.1139, Validation Loss: 0.1115
2025-12-31 10:09:47 - INFO - PRINT: Epoch [ 254], Train Loss: 0.1143, Validation Loss: 0.1115
2025-12-31 10:10:14 - INFO - PRINT: Epoch [ 255], Train Loss: 0.1136, Validation Loss: 0.1115
2025-12-31 10:10:41 - INFO - PRINT: Epoch [ 256], Train Loss: 0.1130, Validation Loss: 0.1115
2025-12-31 10:11:08 - INFO - PRINT: Epoch [ 257], Train Loss: 0.1150, Validation Loss: 0.1115
2025-12-31 10:11:34 - INFO - PRINT: Epoch [ 258], Train Loss: 0.1135, Validation Loss: 0.1115
2025-12-31 10:12:01 - INFO - PRINT: Epoch [ 259], Train Loss: 0.1130, Validation Loss: 0.1115
2025-12-31 10:12:32 - INFO - PRINT: Epoch [ 260], Train Loss: 0.1126, Validation Loss: 0.1116
2025-12-31 10:12:59 - INFO - PRINT: Epoch [ 261], Train Loss: 0.1125, Validation Loss: 0.1116
2025-12-31 10:13:26 - INFO - PRINT: Epoch [ 262], Train Loss: 0.1125, Validation Loss: 0.1116
2025-12-31 10:13:52 - INFO - PRINT: Epoch [ 263], Train Loss: 0.1138, Validation Loss: 0.1116
2025-12-31 10:14:19 - INFO - PRINT: Epoch [ 264], Train Loss: 0.1130, Validation Loss: 0.1116
2025-12-31 10:14:46 - INFO - PRINT: Epoch [ 265], Train Loss: 0.1129, Validation Loss: 0.1116
2025-12-31 10:15:13 - INFO - PRINT: Epoch [ 266], Train Loss: 0.1139, Validation Loss: 0.1116
2025-12-31 10:15:39 - INFO - PRINT: Epoch [ 267], Train Loss: 0.1128, Validation Loss: 0.1116
2025-12-31 10:16:06 - INFO - PRINT: Epoch [ 268], Train Loss: 0.1130, Validation Loss: 0.1116
2025-12-31 10:16:33 - INFO - PRINT: Epoch [ 269], Train Loss: 0.1139, Validation Loss: 0.1116
2025-12-31 10:17:00 - INFO - PRINT: Epoch [ 270], Train Loss: 0.1135, Validation Loss: 0.1116
2025-12-31 10:17:26 - INFO - PRINT: Epoch [ 271], Train Loss: 0.1135, Validation Loss: 0.1116
2025-12-31 10:17:53 - INFO - PRINT: Epoch [ 272], Train Loss: 0.1154, Validation Loss: 0.1116
2025-12-31 10:18:20 - INFO - PRINT: Epoch [ 273], Train Loss: 0.1133, Validation Loss: 0.1116
2025-12-31 10:18:47 - INFO - PRINT: Epoch [ 274], Train Loss: 0.1133, Validation Loss: 0.1116
2025-12-31 10:19:14 - INFO - PRINT: Epoch [ 275], Train Loss: 0.1119, Validation Loss: 0.1116
2025-12-31 10:19:40 - INFO - PRINT: Epoch [ 276], Train Loss: 0.1130, Validation Loss: 0.1116
2025-12-31 10:20:07 - INFO - PRINT: Epoch [ 277], Train Loss: 0.1118, Validation Loss: 0.1116
2025-12-31 10:20:34 - INFO - PRINT: Epoch [ 278], Train Loss: 0.1130, Validation Loss: 0.1116
2025-12-31 10:21:01 - INFO - PRINT: Epoch [ 279], Train Loss: 0.1126, Validation Loss: 0.1116
2025-12-31 10:21:32 - INFO - PRINT: Epoch [ 280], Train Loss: 0.1146, Validation Loss: 0.1136
2025-12-31 10:21:58 - INFO - PRINT: Epoch [ 281], Train Loss: 0.1133, Validation Loss: 0.1136
2025-12-31 10:22:25 - INFO - PRINT: Epoch [ 282], Train Loss: 0.1148, Validation Loss: 0.1136
2025-12-31 10:22:52 - INFO - PRINT: Epoch [ 283], Train Loss: 0.1140, Validation Loss: 0.1136
2025-12-31 10:23:18 - INFO - PRINT: Epoch [ 284], Train Loss: 0.1116, Validation Loss: 0.1136
2025-12-31 10:23:45 - INFO - PRINT: Epoch [ 285], Train Loss: 0.1118, Validation Loss: 0.1136
2025-12-31 10:24:12 - INFO - PRINT: Epoch [ 286], Train Loss: 0.1116, Validation Loss: 0.1136
2025-12-31 10:24:39 - INFO - PRINT: Epoch [ 287], Train Loss: 0.1127, Validation Loss: 0.1136
2025-12-31 10:25:05 - INFO - PRINT: Epoch [ 288], Train Loss: 0.1128, Validation Loss: 0.1136
2025-12-31 10:25:32 - INFO - PRINT: Epoch [ 289], Train Loss: 0.1129, Validation Loss: 0.1136
2025-12-31 10:25:59 - INFO - PRINT: Epoch [ 290], Train Loss: 0.1107, Validation Loss: 0.1136
2025-12-31 10:26:26 - INFO - PRINT: Epoch [ 291], Train Loss: 0.1121, Validation Loss: 0.1136
2025-12-31 10:26:52 - INFO - PRINT: Epoch [ 292], Train Loss: 0.1117, Validation Loss: 0.1136
2025-12-31 10:27:19 - INFO - PRINT: Epoch [ 293], Train Loss: 0.1134, Validation Loss: 0.1136
2025-12-31 10:27:46 - INFO - PRINT: Epoch [ 294], Train Loss: 0.1121, Validation Loss: 0.1136
2025-12-31 10:28:13 - INFO - PRINT: Epoch [ 295], Train Loss: 0.1133, Validation Loss: 0.1136
2025-12-31 10:28:39 - INFO - PRINT: Epoch [ 296], Train Loss: 0.1118, Validation Loss: 0.1136
2025-12-31 10:29:06 - INFO - PRINT: Epoch [ 297], Train Loss: 0.1111, Validation Loss: 0.1136
2025-12-31 10:29:33 - INFO - PRINT: Epoch [ 298], Train Loss: 0.1139, Validation Loss: 0.1136
2025-12-31 10:30:00 - INFO - PRINT: Epoch [ 299], Train Loss: 0.1113, Validation Loss: 0.1136
2025-12-31 10:30:31 - INFO - PRINT: Epoch [ 300], Train Loss: 0.1138, Validation Loss: 0.1113
2025-12-31 10:30:31 - INFO - PRINT: ----> Saving model from epoch 300 (val loss: 0.11126299023628235). Well-balanced!
2025-12-31 10:30:57 - INFO - PRINT: Epoch [ 301], Train Loss: 0.1124, Validation Loss: 0.1113
2025-12-31 10:31:24 - INFO - PRINT: Epoch [ 302], Train Loss: 0.1140, Validation Loss: 0.1113
2025-12-31 10:31:51 - INFO - PRINT: Epoch [ 303], Train Loss: 0.1104, Validation Loss: 0.1113
2025-12-31 10:32:18 - INFO - PRINT: Epoch [ 304], Train Loss: 0.1107, Validation Loss: 0.1113
2025-12-31 10:32:44 - INFO - PRINT: Epoch [ 305], Train Loss: 0.1116, Validation Loss: 0.1113
2025-12-31 10:33:11 - INFO - PRINT: Epoch [ 306], Train Loss: 0.1113, Validation Loss: 0.1113
2025-12-31 10:33:38 - INFO - PRINT: Epoch [ 307], Train Loss: 0.1120, Validation Loss: 0.1113
2025-12-31 10:34:05 - INFO - PRINT: Epoch [ 308], Train Loss: 0.1131, Validation Loss: 0.1113
2025-12-31 10:34:31 - INFO - PRINT: Epoch [ 309], Train Loss: 0.1117, Validation Loss: 0.1113
2025-12-31 10:34:58 - INFO - PRINT: Epoch [ 310], Train Loss: 0.1126, Validation Loss: 0.1113
2025-12-31 10:35:25 - INFO - PRINT: Epoch [ 311], Train Loss: 0.1106, Validation Loss: 0.1113
2025-12-31 10:35:52 - INFO - PRINT: Epoch [ 312], Train Loss: 0.1107, Validation Loss: 0.1113
2025-12-31 10:36:18 - INFO - PRINT: Epoch [ 313], Train Loss: 0.1110, Validation Loss: 0.1113
2025-12-31 10:36:45 - INFO - PRINT: Epoch [ 314], Train Loss: 0.1121, Validation Loss: 0.1113
2025-12-31 10:37:12 - INFO - PRINT: Epoch [ 315], Train Loss: 0.1117, Validation Loss: 0.1113
2025-12-31 10:37:38 - INFO - PRINT: Epoch [ 316], Train Loss: 0.1112, Validation Loss: 0.1113
2025-12-31 10:38:05 - INFO - PRINT: Epoch [ 317], Train Loss: 0.1112, Validation Loss: 0.1113
2025-12-31 10:38:32 - INFO - PRINT: Epoch [ 318], Train Loss: 0.1116, Validation Loss: 0.1113
2025-12-31 10:38:59 - INFO - PRINT: Epoch [ 319], Train Loss: 0.1121, Validation Loss: 0.1113
2025-12-31 10:39:30 - INFO - PRINT: Epoch [ 320], Train Loss: 0.1106, Validation Loss: 0.1082
2025-12-31 10:39:56 - INFO - PRINT: Epoch [ 321], Train Loss: 0.1107, Validation Loss: 0.1082
2025-12-31 10:40:23 - INFO - PRINT: Epoch [ 322], Train Loss: 0.1107, Validation Loss: 0.1082
2025-12-31 10:40:50 - INFO - PRINT: Epoch [ 323], Train Loss: 0.1109, Validation Loss: 0.1082
2025-12-31 10:41:17 - INFO - PRINT: Epoch [ 324], Train Loss: 0.1108, Validation Loss: 0.1082
2025-12-31 10:41:43 - INFO - PRINT: Epoch [ 325], Train Loss: 0.1105, Validation Loss: 0.1082
2025-12-31 10:42:10 - INFO - PRINT: Epoch [ 326], Train Loss: 0.1106, Validation Loss: 0.1082
2025-12-31 10:42:37 - INFO - PRINT: Epoch [ 327], Train Loss: 0.1113, Validation Loss: 0.1082
2025-12-31 10:43:04 - INFO - PRINT: Epoch [ 328], Train Loss: 0.1109, Validation Loss: 0.1082
2025-12-31 10:43:30 - INFO - PRINT: Epoch [ 329], Train Loss: 0.1115, Validation Loss: 0.1082
2025-12-31 10:43:57 - INFO - PRINT: Epoch [ 330], Train Loss: 0.1131, Validation Loss: 0.1082
2025-12-31 10:44:24 - INFO - PRINT: Epoch [ 331], Train Loss: 0.1113, Validation Loss: 0.1082
2025-12-31 10:44:51 - INFO - PRINT: Epoch [ 332], Train Loss: 0.1120, Validation Loss: 0.1082
2025-12-31 10:45:17 - INFO - PRINT: Epoch [ 333], Train Loss: 0.1103, Validation Loss: 0.1082
2025-12-31 10:45:44 - INFO - PRINT: Epoch [ 334], Train Loss: 0.1096, Validation Loss: 0.1082
2025-12-31 10:46:11 - INFO - PRINT: Epoch [ 335], Train Loss: 0.1116, Validation Loss: 0.1082
2025-12-31 10:46:38 - INFO - PRINT: Epoch [ 336], Train Loss: 0.1116, Validation Loss: 0.1082
2025-12-31 10:47:04 - INFO - PRINT: Epoch [ 337], Train Loss: 0.1111, Validation Loss: 0.1082
2025-12-31 10:47:31 - INFO - PRINT: Epoch [ 338], Train Loss: 0.1115, Validation Loss: 0.1082
2025-12-31 10:47:58 - INFO - PRINT: Epoch [ 339], Train Loss: 0.1103, Validation Loss: 0.1082
2025-12-31 10:48:29 - INFO - PRINT: Epoch [ 340], Train Loss: 0.1109, Validation Loss: 0.1071
2025-12-31 10:48:56 - INFO - PRINT: Epoch [ 341], Train Loss: 0.1108, Validation Loss: 0.1071
2025-12-31 10:49:22 - INFO - PRINT: Epoch [ 342], Train Loss: 0.1113, Validation Loss: 0.1071
2025-12-31 10:49:49 - INFO - PRINT: Epoch [ 343], Train Loss: 0.1099, Validation Loss: 0.1071
2025-12-31 10:50:16 - INFO - PRINT: Epoch [ 344], Train Loss: 0.1100, Validation Loss: 0.1071
2025-12-31 10:50:43 - INFO - PRINT: Epoch [ 345], Train Loss: 0.1103, Validation Loss: 0.1071
2025-12-31 10:51:09 - INFO - PRINT: Epoch [ 346], Train Loss: 0.1109, Validation Loss: 0.1071
2025-12-31 10:51:36 - INFO - PRINT: Epoch [ 347], Train Loss: 0.1111, Validation Loss: 0.1071
2025-12-31 10:52:03 - INFO - PRINT: Epoch [ 348], Train Loss: 0.1116, Validation Loss: 0.1071
2025-12-31 10:52:30 - INFO - PRINT: Epoch [ 349], Train Loss: 0.1103, Validation Loss: 0.1071
2025-12-31 10:52:56 - INFO - PRINT: Epoch [ 350], Train Loss: 0.1108, Validation Loss: 0.1071
2025-12-31 10:53:23 - INFO - PRINT: Epoch [ 351], Train Loss: 0.1092, Validation Loss: 0.1071
2025-12-31 10:53:50 - INFO - PRINT: Epoch [ 352], Train Loss: 0.1088, Validation Loss: 0.1071
2025-12-31 10:54:17 - INFO - PRINT: Epoch [ 353], Train Loss: 0.1103, Validation Loss: 0.1071
2025-12-31 10:54:43 - INFO - PRINT: Epoch [ 354], Train Loss: 0.1097, Validation Loss: 0.1071
2025-12-31 10:55:10 - INFO - PRINT: Epoch [ 355], Train Loss: 0.1099, Validation Loss: 0.1071
2025-12-31 10:55:37 - INFO - PRINT: Epoch [ 356], Train Loss: 0.1109, Validation Loss: 0.1071
2025-12-31 10:56:04 - INFO - PRINT: Epoch [ 357], Train Loss: 0.1118, Validation Loss: 0.1071
2025-12-31 10:56:30 - INFO - PRINT: Epoch [ 358], Train Loss: 0.1107, Validation Loss: 0.1071
2025-12-31 10:56:57 - INFO - PRINT: Epoch [ 359], Train Loss: 0.1087, Validation Loss: 0.1071
2025-12-31 10:57:28 - INFO - PRINT: Epoch [ 360], Train Loss: 0.1100, Validation Loss: 0.1083
2025-12-31 10:57:55 - INFO - PRINT: Epoch [ 361], Train Loss: 0.1099, Validation Loss: 0.1083
2025-12-31 10:58:22 - INFO - PRINT: Epoch [ 362], Train Loss: 0.1096, Validation Loss: 0.1083
2025-12-31 10:58:48 - INFO - PRINT: Epoch [ 363], Train Loss: 0.1101, Validation Loss: 0.1083
2025-12-31 10:59:15 - INFO - PRINT: Epoch [ 364], Train Loss: 0.1109, Validation Loss: 0.1083
2025-12-31 10:59:42 - INFO - PRINT: Epoch [ 365], Train Loss: 0.1121, Validation Loss: 0.1083
2025-12-31 11:00:09 - INFO - PRINT: Epoch [ 366], Train Loss: 0.1096, Validation Loss: 0.1083
2025-12-31 11:00:35 - INFO - PRINT: Epoch [ 367], Train Loss: 0.1092, Validation Loss: 0.1083
2025-12-31 11:01:02 - INFO - PRINT: Epoch [ 368], Train Loss: 0.1092, Validation Loss: 0.1083
2025-12-31 11:01:29 - INFO - PRINT: Epoch [ 369], Train Loss: 0.1099, Validation Loss: 0.1083
2025-12-31 11:01:56 - INFO - PRINT: Epoch [ 370], Train Loss: 0.1104, Validation Loss: 0.1083
2025-12-31 11:02:22 - INFO - PRINT: Epoch [ 371], Train Loss: 0.1114, Validation Loss: 0.1083
2025-12-31 11:02:49 - INFO - PRINT: Epoch [ 372], Train Loss: 0.1100, Validation Loss: 0.1083
2025-12-31 11:03:16 - INFO - PRINT: Epoch [ 373], Train Loss: 0.1088, Validation Loss: 0.1083
2025-12-31 11:03:42 - INFO - PRINT: Epoch [ 374], Train Loss: 0.1092, Validation Loss: 0.1083
2025-12-31 11:04:09 - INFO - PRINT: Epoch [ 375], Train Loss: 0.1096, Validation Loss: 0.1083
2025-12-31 11:04:36 - INFO - PRINT: Epoch [ 376], Train Loss: 0.1091, Validation Loss: 0.1083
2025-12-31 11:05:03 - INFO - PRINT: Epoch [ 377], Train Loss: 0.1093, Validation Loss: 0.1083
2025-12-31 11:05:30 - INFO - PRINT: Epoch [ 378], Train Loss: 0.1090, Validation Loss: 0.1083
2025-12-31 11:05:56 - INFO - PRINT: Epoch [ 379], Train Loss: 0.1090, Validation Loss: 0.1083
2025-12-31 11:06:28 - INFO - PRINT: Epoch [ 380], Train Loss: 0.1098, Validation Loss: 0.1082
2025-12-31 11:06:54 - INFO - PRINT: Epoch [ 381], Train Loss: 0.1092, Validation Loss: 0.1082
2025-12-31 11:07:21 - INFO - PRINT: Epoch [ 382], Train Loss: 0.1098, Validation Loss: 0.1082
2025-12-31 11:07:47 - INFO - PRINT: Epoch [ 383], Train Loss: 0.1103, Validation Loss: 0.1082
2025-12-31 11:08:14 - INFO - PRINT: Epoch [ 384], Train Loss: 0.1100, Validation Loss: 0.1082
2025-12-31 11:08:41 - INFO - PRINT: Epoch [ 385], Train Loss: 0.1101, Validation Loss: 0.1082
2025-12-31 11:09:08 - INFO - PRINT: Epoch [ 386], Train Loss: 0.1098, Validation Loss: 0.1082
2025-12-31 11:09:34 - INFO - PRINT: Epoch [ 387], Train Loss: 0.1088, Validation Loss: 0.1082
2025-12-31 11:10:01 - INFO - PRINT: Epoch [ 388], Train Loss: 0.1093, Validation Loss: 0.1082
2025-12-31 11:10:28 - INFO - PRINT: Epoch [ 389], Train Loss: 0.1093, Validation Loss: 0.1082
2025-12-31 11:10:55 - INFO - PRINT: Epoch [ 390], Train Loss: 0.1086, Validation Loss: 0.1082
2025-12-31 11:11:21 - INFO - PRINT: Epoch [ 391], Train Loss: 0.1088, Validation Loss: 0.1082
2025-12-31 11:11:48 - INFO - PRINT: Epoch [ 392], Train Loss: 0.1086, Validation Loss: 0.1082
2025-12-31 11:12:15 - INFO - PRINT: Epoch [ 393], Train Loss: 0.1081, Validation Loss: 0.1082
2025-12-31 11:12:42 - INFO - PRINT: Epoch [ 394], Train Loss: 0.1088, Validation Loss: 0.1082
2025-12-31 11:13:08 - INFO - PRINT: Epoch [ 395], Train Loss: 0.1105, Validation Loss: 0.1082
2025-12-31 11:13:35 - INFO - PRINT: Epoch [ 396], Train Loss: 0.1084, Validation Loss: 0.1082
2025-12-31 11:14:02 - INFO - PRINT: Epoch [ 397], Train Loss: 0.1089, Validation Loss: 0.1082
2025-12-31 11:14:29 - INFO - PRINT: Epoch [ 398], Train Loss: 0.1089, Validation Loss: 0.1082
2025-12-31 11:14:55 - INFO - PRINT: Epoch [ 399], Train Loss: 0.1108, Validation Loss: 0.1082
2025-12-31 11:15:27 - INFO - PRINT: Epoch [ 400], Train Loss: 0.1109, Validation Loss: 0.1061
2025-12-31 11:15:27 - INFO - PRINT: ----> Saving model from epoch 400 (val loss: 0.1060670393705368). Luscious!
2025-12-31 11:15:53 - INFO - PRINT: Epoch [ 401], Train Loss: 0.1097, Validation Loss: 0.1061
2025-12-31 11:16:20 - INFO - PRINT: Epoch [ 402], Train Loss: 0.1101, Validation Loss: 0.1061
2025-12-31 11:16:47 - INFO - PRINT: Epoch [ 403], Train Loss: 0.1111, Validation Loss: 0.1061
2025-12-31 11:17:13 - INFO - PRINT: Epoch [ 404], Train Loss: 0.1090, Validation Loss: 0.1061
2025-12-31 11:17:40 - INFO - PRINT: Epoch [ 405], Train Loss: 0.1081, Validation Loss: 0.1061
2025-12-31 11:18:07 - INFO - PRINT: Epoch [ 406], Train Loss: 0.1082, Validation Loss: 0.1061
2025-12-31 11:18:34 - INFO - PRINT: Epoch [ 407], Train Loss: 0.1089, Validation Loss: 0.1061
2025-12-31 11:19:00 - INFO - PRINT: Epoch [ 408], Train Loss: 0.1091, Validation Loss: 0.1061
2025-12-31 11:19:27 - INFO - PRINT: Epoch [ 409], Train Loss: 0.1095, Validation Loss: 0.1061
2025-12-31 11:19:54 - INFO - PRINT: Epoch [ 410], Train Loss: 0.1080, Validation Loss: 0.1061
2025-12-31 11:20:20 - INFO - PRINT: Epoch [ 411], Train Loss: 0.1081, Validation Loss: 0.1061
2025-12-31 11:20:47 - INFO - PRINT: Epoch [ 412], Train Loss: 0.1079, Validation Loss: 0.1061
2025-12-31 11:21:14 - INFO - PRINT: Epoch [ 413], Train Loss: 0.1082, Validation Loss: 0.1061
2025-12-31 11:21:41 - INFO - PRINT: Epoch [ 414], Train Loss: 0.1088, Validation Loss: 0.1061
2025-12-31 11:22:07 - INFO - PRINT: Epoch [ 415], Train Loss: 0.1098, Validation Loss: 0.1061
2025-12-31 11:22:34 - INFO - PRINT: Epoch [ 416], Train Loss: 0.1082, Validation Loss: 0.1061
2025-12-31 11:23:01 - INFO - PRINT: Epoch [ 417], Train Loss: 0.1081, Validation Loss: 0.1061
2025-12-31 11:23:28 - INFO - PRINT: Epoch [ 418], Train Loss: 0.1082, Validation Loss: 0.1061
2025-12-31 11:23:54 - INFO - PRINT: Epoch [ 419], Train Loss: 0.1096, Validation Loss: 0.1061
2025-12-31 11:24:26 - INFO - PRINT: Epoch [ 420], Train Loss: 0.1085, Validation Loss: 0.1057
2025-12-31 11:24:52 - INFO - PRINT: Epoch [ 421], Train Loss: 0.1088, Validation Loss: 0.1057
2025-12-31 11:25:19 - INFO - PRINT: Epoch [ 422], Train Loss: 0.1090, Validation Loss: 0.1057
2025-12-31 11:25:45 - INFO - PRINT: Epoch [ 423], Train Loss: 0.1092, Validation Loss: 0.1057
2025-12-31 11:26:12 - INFO - PRINT: Epoch [ 424], Train Loss: 0.1080, Validation Loss: 0.1057
2025-12-31 11:26:39 - INFO - PRINT: Epoch [ 425], Train Loss: 0.1091, Validation Loss: 0.1057
2025-12-31 11:27:06 - INFO - PRINT: Epoch [ 426], Train Loss: 0.1081, Validation Loss: 0.1057
2025-12-31 11:27:33 - INFO - PRINT: Epoch [ 427], Train Loss: 0.1093, Validation Loss: 0.1057
2025-12-31 11:27:59 - INFO - PRINT: Epoch [ 428], Train Loss: 0.1074, Validation Loss: 0.1057
2025-12-31 11:28:26 - INFO - PRINT: Epoch [ 429], Train Loss: 0.1097, Validation Loss: 0.1057
2025-12-31 11:28:53 - INFO - PRINT: Epoch [ 430], Train Loss: 0.1089, Validation Loss: 0.1057
2025-12-31 11:29:20 - INFO - PRINT: Epoch [ 431], Train Loss: 0.1084, Validation Loss: 0.1057
2025-12-31 11:29:46 - INFO - PRINT: Epoch [ 432], Train Loss: 0.1076, Validation Loss: 0.1057
2025-12-31 11:30:13 - INFO - PRINT: Epoch [ 433], Train Loss: 0.1076, Validation Loss: 0.1057
2025-12-31 11:30:40 - INFO - PRINT: Epoch [ 434], Train Loss: 0.1078, Validation Loss: 0.1057
2025-12-31 11:31:07 - INFO - PRINT: Epoch [ 435], Train Loss: 0.1095, Validation Loss: 0.1057
2025-12-31 11:31:33 - INFO - PRINT: Epoch [ 436], Train Loss: 0.1079, Validation Loss: 0.1057
2025-12-31 11:32:00 - INFO - PRINT: Epoch [ 437], Train Loss: 0.1079, Validation Loss: 0.1057
2025-12-31 11:32:27 - INFO - PRINT: Epoch [ 438], Train Loss: 0.1078, Validation Loss: 0.1057
2025-12-31 11:32:54 - INFO - PRINT: Epoch [ 439], Train Loss: 0.1079, Validation Loss: 0.1057
2025-12-31 11:33:25 - INFO - PRINT: Epoch [ 440], Train Loss: 0.1076, Validation Loss: 0.1060
2025-12-31 11:33:51 - INFO - PRINT: Epoch [ 441], Train Loss: 0.1076, Validation Loss: 0.1060
2025-12-31 11:34:18 - INFO - PRINT: Epoch [ 442], Train Loss: 0.1085, Validation Loss: 0.1060
2025-12-31 11:34:45 - INFO - PRINT: Epoch [ 443], Train Loss: 0.1081, Validation Loss: 0.1060
2025-12-31 11:35:11 - INFO - PRINT: Epoch [ 444], Train Loss: 0.1080, Validation Loss: 0.1060
2025-12-31 11:35:38 - INFO - PRINT: Epoch [ 445], Train Loss: 0.1081, Validation Loss: 0.1060
2025-12-31 11:36:05 - INFO - PRINT: Epoch [ 446], Train Loss: 0.1083, Validation Loss: 0.1060
2025-12-31 11:36:32 - INFO - PRINT: Epoch [ 447], Train Loss: 0.1083, Validation Loss: 0.1060
2025-12-31 11:36:58 - INFO - PRINT: Epoch [ 448], Train Loss: 0.1081, Validation Loss: 0.1060
2025-12-31 11:37:25 - INFO - PRINT: Epoch [ 449], Train Loss: 0.1074, Validation Loss: 0.1060
2025-12-31 11:37:52 - INFO - PRINT: Epoch [ 450], Train Loss: 0.1089, Validation Loss: 0.1060
2025-12-31 11:38:19 - INFO - PRINT: Epoch [ 451], Train Loss: 0.1086, Validation Loss: 0.1060
2025-12-31 11:38:45 - INFO - PRINT: Epoch [ 452], Train Loss: 0.1091, Validation Loss: 0.1060
2025-12-31 11:39:12 - INFO - PRINT: Epoch [ 453], Train Loss: 0.1090, Validation Loss: 0.1060
2025-12-31 11:39:39 - INFO - PRINT: Epoch [ 454], Train Loss: 0.1072, Validation Loss: 0.1060
2025-12-31 11:40:06 - INFO - PRINT: Epoch [ 455], Train Loss: 0.1072, Validation Loss: 0.1060
2025-12-31 11:40:32 - INFO - PRINT: Epoch [ 456], Train Loss: 0.1069, Validation Loss: 0.1060
2025-12-31 11:40:59 - INFO - PRINT: Epoch [ 457], Train Loss: 0.1082, Validation Loss: 0.1060
2025-12-31 11:41:26 - INFO - PRINT: Epoch [ 458], Train Loss: 0.1097, Validation Loss: 0.1060
2025-12-31 11:41:53 - INFO - PRINT: Epoch [ 459], Train Loss: 0.1075, Validation Loss: 0.1060
2025-12-31 11:42:24 - INFO - PRINT: Epoch [ 460], Train Loss: 0.1072, Validation Loss: 0.1063
2025-12-31 11:42:50 - INFO - PRINT: Epoch [ 461], Train Loss: 0.1069, Validation Loss: 0.1063
2025-12-31 11:43:17 - INFO - PRINT: Epoch [ 462], Train Loss: 0.1082, Validation Loss: 0.1063
2025-12-31 11:43:44 - INFO - PRINT: Epoch [ 463], Train Loss: 0.1074, Validation Loss: 0.1063
2025-12-31 11:44:10 - INFO - PRINT: Epoch [ 464], Train Loss: 0.1089, Validation Loss: 0.1063
2025-12-31 11:44:37 - INFO - PRINT: Epoch [ 465], Train Loss: 0.1078, Validation Loss: 0.1063
2025-12-31 11:45:04 - INFO - PRINT: Epoch [ 466], Train Loss: 0.1068, Validation Loss: 0.1063
2025-12-31 11:45:31 - INFO - PRINT: Epoch [ 467], Train Loss: 0.1070, Validation Loss: 0.1063
2025-12-31 11:45:57 - INFO - PRINT: Epoch [ 468], Train Loss: 0.1069, Validation Loss: 0.1063
2025-12-31 11:46:24 - INFO - PRINT: Epoch [ 469], Train Loss: 0.1064, Validation Loss: 0.1063
2025-12-31 11:46:51 - INFO - PRINT: Epoch [ 470], Train Loss: 0.1070, Validation Loss: 0.1063
2025-12-31 11:47:18 - INFO - PRINT: Epoch [ 471], Train Loss: 0.1066, Validation Loss: 0.1063
2025-12-31 11:47:45 - INFO - PRINT: Epoch [ 472], Train Loss: 0.1074, Validation Loss: 0.1063
2025-12-31 11:48:11 - INFO - PRINT: Epoch [ 473], Train Loss: 0.1093, Validation Loss: 0.1063
2025-12-31 11:48:38 - INFO - PRINT: Epoch [ 474], Train Loss: 0.1083, Validation Loss: 0.1063
2025-12-31 11:49:05 - INFO - PRINT: Epoch [ 475], Train Loss: 0.1089, Validation Loss: 0.1063
2025-12-31 11:49:32 - INFO - PRINT: Epoch [ 476], Train Loss: 0.1079, Validation Loss: 0.1063
2025-12-31 11:49:58 - INFO - PRINT: Epoch [ 477], Train Loss: 0.1074, Validation Loss: 0.1063
2025-12-31 11:50:25 - INFO - PRINT: Epoch [ 478], Train Loss: 0.1070, Validation Loss: 0.1063
2025-12-31 11:50:52 - INFO - PRINT: Epoch [ 479], Train Loss: 0.1078, Validation Loss: 0.1063
2025-12-31 11:51:23 - INFO - PRINT: Epoch [ 480], Train Loss: 0.1081, Validation Loss: 0.1041
2025-12-31 11:51:49 - INFO - PRINT: Epoch [ 481], Train Loss: 0.1076, Validation Loss: 0.1041
2025-12-31 11:52:16 - INFO - PRINT: Epoch [ 482], Train Loss: 0.1074, Validation Loss: 0.1041
2025-12-31 11:52:43 - INFO - PRINT: Epoch [ 483], Train Loss: 0.1074, Validation Loss: 0.1041
2025-12-31 11:53:10 - INFO - PRINT: Epoch [ 484], Train Loss: 0.1064, Validation Loss: 0.1041
2025-12-31 11:53:36 - INFO - PRINT: Epoch [ 485], Train Loss: 0.1096, Validation Loss: 0.1041
2025-12-31 11:54:03 - INFO - PRINT: Epoch [ 486], Train Loss: 0.1074, Validation Loss: 0.1041
2025-12-31 11:54:30 - INFO - PRINT: Epoch [ 487], Train Loss: 0.1072, Validation Loss: 0.1041
2025-12-31 11:54:57 - INFO - PRINT: Epoch [ 488], Train Loss: 0.1081, Validation Loss: 0.1041
2025-12-31 11:55:24 - INFO - PRINT: Epoch [ 489], Train Loss: 0.1068, Validation Loss: 0.1041
2025-12-31 11:55:50 - INFO - PRINT: Epoch [ 490], Train Loss: 0.1067, Validation Loss: 0.1041
2025-12-31 11:56:17 - INFO - PRINT: Epoch [ 491], Train Loss: 0.1076, Validation Loss: 0.1041
2025-12-31 11:56:44 - INFO - PRINT: Epoch [ 492], Train Loss: 0.1086, Validation Loss: 0.1041
2025-12-31 11:57:11 - INFO - PRINT: Epoch [ 493], Train Loss: 0.1067, Validation Loss: 0.1041
2025-12-31 11:57:37 - INFO - PRINT: Epoch [ 494], Train Loss: 0.1068, Validation Loss: 0.1041
2025-12-31 11:58:04 - INFO - PRINT: Epoch [ 495], Train Loss: 0.1082, Validation Loss: 0.1041
2025-12-31 11:58:31 - INFO - PRINT: Epoch [ 496], Train Loss: 0.1066, Validation Loss: 0.1041
2025-12-31 11:58:58 - INFO - PRINT: Epoch [ 497], Train Loss: 0.1075, Validation Loss: 0.1041
2025-12-31 11:59:24 - INFO - PRINT: Epoch [ 498], Train Loss: 0.1069, Validation Loss: 0.1041
2025-12-31 11:59:51 - INFO - PRINT: Epoch [ 499], Train Loss: 0.1066, Validation Loss: 0.1041
2025-12-31 12:00:22 - INFO - PRINT: Epoch [ 500], Train Loss: 0.1072, Validation Loss: 0.1055
2025-12-31 12:00:22 - INFO - PRINT: ----> Saving model from epoch 480 (val loss: 0.10412747323513032). Tempting!
2025-12-31 12:00:49 - INFO - PRINT: Epoch [ 501], Train Loss: 0.1067, Validation Loss: 0.1055
2025-12-31 12:01:15 - INFO - PRINT: Epoch [ 502], Train Loss: 0.1076, Validation Loss: 0.1055
2025-12-31 12:01:42 - INFO - PRINT: Epoch [ 503], Train Loss: 0.1075, Validation Loss: 0.1055
2025-12-31 12:02:09 - INFO - PRINT: Epoch [ 504], Train Loss: 0.1072, Validation Loss: 0.1055
2025-12-31 12:02:36 - INFO - PRINT: Epoch [ 505], Train Loss: 0.1074, Validation Loss: 0.1055
2025-12-31 12:03:03 - INFO - PRINT: Epoch [ 506], Train Loss: 0.1081, Validation Loss: 0.1055
2025-12-31 12:03:29 - INFO - PRINT: Epoch [ 507], Train Loss: 0.1068, Validation Loss: 0.1055
2025-12-31 12:03:56 - INFO - PRINT: Epoch [ 508], Train Loss: 0.1062, Validation Loss: 0.1055
2025-12-31 12:04:23 - INFO - PRINT: Epoch [ 509], Train Loss: 0.1074, Validation Loss: 0.1055
2025-12-31 12:04:50 - INFO - PRINT: Epoch [ 510], Train Loss: 0.1062, Validation Loss: 0.1055
2025-12-31 12:05:16 - INFO - PRINT: Epoch [ 511], Train Loss: 0.1062, Validation Loss: 0.1055
2025-12-31 12:05:43 - INFO - PRINT: Epoch [ 512], Train Loss: 0.1064, Validation Loss: 0.1055
2025-12-31 12:06:10 - INFO - PRINT: Epoch [ 513], Train Loss: 0.1066, Validation Loss: 0.1055
2025-12-31 12:06:37 - INFO - PRINT: Epoch [ 514], Train Loss: 0.1073, Validation Loss: 0.1055
2025-12-31 12:07:03 - INFO - PRINT: Epoch [ 515], Train Loss: 0.1071, Validation Loss: 0.1055
2025-12-31 12:07:30 - INFO - PRINT: Epoch [ 516], Train Loss: 0.1064, Validation Loss: 0.1055
2025-12-31 12:07:57 - INFO - PRINT: Epoch [ 517], Train Loss: 0.1065, Validation Loss: 0.1055
2025-12-31 12:08:24 - INFO - PRINT: Epoch [ 518], Train Loss: 0.1060, Validation Loss: 0.1055
2025-12-31 12:08:50 - INFO - PRINT: Epoch [ 519], Train Loss: 0.1072, Validation Loss: 0.1055
2025-12-31 12:09:22 - INFO - PRINT: Epoch [ 520], Train Loss: 0.1065, Validation Loss: 0.1049
2025-12-31 12:09:48 - INFO - PRINT: Epoch [ 521], Train Loss: 0.1074, Validation Loss: 0.1049
2025-12-31 12:10:15 - INFO - PRINT: Epoch [ 522], Train Loss: 0.1072, Validation Loss: 0.1049
2025-12-31 12:10:42 - INFO - PRINT: Epoch [ 523], Train Loss: 0.1069, Validation Loss: 0.1049
2025-12-31 12:11:08 - INFO - PRINT: Epoch [ 524], Train Loss: 0.1065, Validation Loss: 0.1049
2025-12-31 12:11:35 - INFO - PRINT: Epoch [ 525], Train Loss: 0.1067, Validation Loss: 0.1049
2025-12-31 12:12:02 - INFO - PRINT: Epoch [ 526], Train Loss: 0.1070, Validation Loss: 0.1049
2025-12-31 12:12:29 - INFO - PRINT: Epoch [ 527], Train Loss: 0.1071, Validation Loss: 0.1049
2025-12-31 12:12:55 - INFO - PRINT: Epoch [ 528], Train Loss: 0.1076, Validation Loss: 0.1049
2025-12-31 12:13:22 - INFO - PRINT: Epoch [ 529], Train Loss: 0.1062, Validation Loss: 0.1049
2025-12-31 12:13:49 - INFO - PRINT: Epoch [ 530], Train Loss: 0.1066, Validation Loss: 0.1049
2025-12-31 12:14:16 - INFO - PRINT: Epoch [ 531], Train Loss: 0.1091, Validation Loss: 0.1049
2025-12-31 12:14:42 - INFO - PRINT: Epoch [ 532], Train Loss: 0.1071, Validation Loss: 0.1049
2025-12-31 12:15:09 - INFO - PRINT: Epoch [ 533], Train Loss: 0.1059, Validation Loss: 0.1049
2025-12-31 12:15:36 - INFO - PRINT: Epoch [ 534], Train Loss: 0.1068, Validation Loss: 0.1049
2025-12-31 12:16:03 - INFO - PRINT: Epoch [ 535], Train Loss: 0.1066, Validation Loss: 0.1049
2025-12-31 12:16:29 - INFO - PRINT: Epoch [ 536], Train Loss: 0.1057, Validation Loss: 0.1049
2025-12-31 12:16:56 - INFO - PRINT: Epoch [ 537], Train Loss: 0.1070, Validation Loss: 0.1049
2025-12-31 12:17:23 - INFO - PRINT: Epoch [ 538], Train Loss: 0.1056, Validation Loss: 0.1049
2025-12-31 12:17:50 - INFO - PRINT: Epoch [ 539], Train Loss: 0.1067, Validation Loss: 0.1049
2025-12-31 12:18:21 - INFO - PRINT: Epoch [ 540], Train Loss: 0.1060, Validation Loss: 0.1035
2025-12-31 12:18:47 - INFO - PRINT: Epoch [ 541], Train Loss: 0.1060, Validation Loss: 0.1035
2025-12-31 12:19:14 - INFO - PRINT: Epoch [ 542], Train Loss: 0.1060, Validation Loss: 0.1035
2025-12-31 12:19:41 - INFO - PRINT: Epoch [ 543], Train Loss: 0.1068, Validation Loss: 0.1035
2025-12-31 12:20:08 - INFO - PRINT: Epoch [ 544], Train Loss: 0.1085, Validation Loss: 0.1035
2025-12-31 12:20:34 - INFO - PRINT: Epoch [ 545], Train Loss: 0.1073, Validation Loss: 0.1035
2025-12-31 12:21:01 - INFO - PRINT: Epoch [ 546], Train Loss: 0.1063, Validation Loss: 0.1035
2025-12-31 12:21:28 - INFO - PRINT: Epoch [ 547], Train Loss: 0.1058, Validation Loss: 0.1035
2025-12-31 12:21:55 - INFO - PRINT: Epoch [ 548], Train Loss: 0.1073, Validation Loss: 0.1035
2025-12-31 12:22:21 - INFO - PRINT: Epoch [ 549], Train Loss: 0.1056, Validation Loss: 0.1035
2025-12-31 12:22:48 - INFO - PRINT: Epoch [ 550], Train Loss: 0.1058, Validation Loss: 0.1035
2025-12-31 12:23:15 - INFO - PRINT: Epoch [ 551], Train Loss: 0.1064, Validation Loss: 0.1035
2025-12-31 12:23:42 - INFO - PRINT: Epoch [ 552], Train Loss: 0.1056, Validation Loss: 0.1035
2025-12-31 12:24:08 - INFO - PRINT: Epoch [ 553], Train Loss: 0.1075, Validation Loss: 0.1035
2025-12-31 12:24:35 - INFO - PRINT: Epoch [ 554], Train Loss: 0.1068, Validation Loss: 0.1035
2025-12-31 12:25:02 - INFO - PRINT: Epoch [ 555], Train Loss: 0.1065, Validation Loss: 0.1035
2025-12-31 12:25:29 - INFO - PRINT: Epoch [ 556], Train Loss: 0.1065, Validation Loss: 0.1035
2025-12-31 12:25:56 - INFO - PRINT: Epoch [ 557], Train Loss: 0.1084, Validation Loss: 0.1035
2025-12-31 12:26:22 - INFO - PRINT: Epoch [ 558], Train Loss: 0.1069, Validation Loss: 0.1035
2025-12-31 12:26:49 - INFO - PRINT: Epoch [ 559], Train Loss: 0.1066, Validation Loss: 0.1035
2025-12-31 12:27:20 - INFO - PRINT: Epoch [ 560], Train Loss: 0.1057, Validation Loss: 0.1030
2025-12-31 12:27:47 - INFO - PRINT: Epoch [ 561], Train Loss: 0.1051, Validation Loss: 0.1030
2025-12-31 12:28:13 - INFO - PRINT: Epoch [ 562], Train Loss: 0.1068, Validation Loss: 0.1030
2025-12-31 12:28:40 - INFO - PRINT: Epoch [ 563], Train Loss: 0.1061, Validation Loss: 0.1030
2025-12-31 12:29:07 - INFO - PRINT: Epoch [ 564], Train Loss: 0.1053, Validation Loss: 0.1030
2025-12-31 12:29:34 - INFO - PRINT: Epoch [ 565], Train Loss: 0.1052, Validation Loss: 0.1030
2025-12-31 12:30:00 - INFO - PRINT: Epoch [ 566], Train Loss: 0.1084, Validation Loss: 0.1030
2025-12-31 12:30:27 - INFO - PRINT: Epoch [ 567], Train Loss: 0.1070, Validation Loss: 0.1030
2025-12-31 12:30:54 - INFO - PRINT: Epoch [ 568], Train Loss: 0.1052, Validation Loss: 0.1030
2025-12-31 12:31:21 - INFO - PRINT: Epoch [ 569], Train Loss: 0.1053, Validation Loss: 0.1030
2025-12-31 12:31:47 - INFO - PRINT: Epoch [ 570], Train Loss: 0.1052, Validation Loss: 0.1030
2025-12-31 12:32:14 - INFO - PRINT: Epoch [ 571], Train Loss: 0.1071, Validation Loss: 0.1030
2025-12-31 12:32:41 - INFO - PRINT: Epoch [ 572], Train Loss: 0.1070, Validation Loss: 0.1030
2025-12-31 12:33:08 - INFO - PRINT: Epoch [ 573], Train Loss: 0.1053, Validation Loss: 0.1030
2025-12-31 12:33:34 - INFO - PRINT: Epoch [ 574], Train Loss: 0.1064, Validation Loss: 0.1030
2025-12-31 12:34:01 - INFO - PRINT: Epoch [ 575], Train Loss: 0.1055, Validation Loss: 0.1030
2025-12-31 12:34:28 - INFO - PRINT: Epoch [ 576], Train Loss: 0.1062, Validation Loss: 0.1030
2025-12-31 12:34:55 - INFO - PRINT: Epoch [ 577], Train Loss: 0.1058, Validation Loss: 0.1030
2025-12-31 12:35:21 - INFO - PRINT: Epoch [ 578], Train Loss: 0.1056, Validation Loss: 0.1030
2025-12-31 12:35:48 - INFO - PRINT: Epoch [ 579], Train Loss: 0.1059, Validation Loss: 0.1030
2025-12-31 12:36:19 - INFO - PRINT: Epoch [ 580], Train Loss: 0.1070, Validation Loss: 0.1041
2025-12-31 12:36:46 - INFO - PRINT: Epoch [ 581], Train Loss: 0.1062, Validation Loss: 0.1041
2025-12-31 12:37:12 - INFO - PRINT: Epoch [ 582], Train Loss: 0.1053, Validation Loss: 0.1041
2025-12-31 12:37:39 - INFO - PRINT: Epoch [ 583], Train Loss: 0.1077, Validation Loss: 0.1041
2025-12-31 12:38:06 - INFO - PRINT: Epoch [ 584], Train Loss: 0.1073, Validation Loss: 0.1041
2025-12-31 12:38:33 - INFO - PRINT: Epoch [ 585], Train Loss: 0.1061, Validation Loss: 0.1041
2025-12-31 12:38:59 - INFO - PRINT: Epoch [ 586], Train Loss: 0.1056, Validation Loss: 0.1041
2025-12-31 12:39:26 - INFO - PRINT: Epoch [ 587], Train Loss: 0.1054, Validation Loss: 0.1041
2025-12-31 12:39:53 - INFO - PRINT: Epoch [ 588], Train Loss: 0.1048, Validation Loss: 0.1041
2025-12-31 12:40:20 - INFO - PRINT: Epoch [ 589], Train Loss: 0.1051, Validation Loss: 0.1041
2025-12-31 12:40:46 - INFO - PRINT: Epoch [ 590], Train Loss: 0.1061, Validation Loss: 0.1041
2025-12-31 12:41:13 - INFO - PRINT: Epoch [ 591], Train Loss: 0.1062, Validation Loss: 0.1041
2025-12-31 12:41:40 - INFO - PRINT: Epoch [ 592], Train Loss: 0.1052, Validation Loss: 0.1041
2025-12-31 12:42:07 - INFO - PRINT: Epoch [ 593], Train Loss: 0.1057, Validation Loss: 0.1041
2025-12-31 12:42:33 - INFO - PRINT: Epoch [ 594], Train Loss: 0.1051, Validation Loss: 0.1041
2025-12-31 12:43:00 - INFO - PRINT: Epoch [ 595], Train Loss: 0.1068, Validation Loss: 0.1041
2025-12-31 12:43:27 - INFO - PRINT: Epoch [ 596], Train Loss: 0.1061, Validation Loss: 0.1041
2025-12-31 12:43:54 - INFO - PRINT: Epoch [ 597], Train Loss: 0.1060, Validation Loss: 0.1041
2025-12-31 12:44:20 - INFO - PRINT: Epoch [ 598], Train Loss: 0.1053, Validation Loss: 0.1041
2025-12-31 12:44:47 - INFO - PRINT: Epoch [ 599], Train Loss: 0.1071, Validation Loss: 0.1041
2025-12-31 12:45:18 - INFO - PRINT: Epoch [ 600], Train Loss: 0.1056, Validation Loss: 0.1022
2025-12-31 12:45:18 - INFO - PRINT: ----> Saving model from epoch 600 (val loss: 0.10222669497132301). Decadent!
2025-12-31 12:45:45 - INFO - PRINT: Epoch [ 601], Train Loss: 0.1069, Validation Loss: 0.1022
2025-12-31 12:46:12 - INFO - PRINT: Epoch [ 602], Train Loss: 0.1053, Validation Loss: 0.1022
2025-12-31 12:46:38 - INFO - PRINT: Epoch [ 603], Train Loss: 0.1058, Validation Loss: 0.1022
2025-12-31 12:47:05 - INFO - PRINT: Epoch [ 604], Train Loss: 0.1053, Validation Loss: 0.1022
2025-12-31 12:47:32 - INFO - PRINT: Epoch [ 605], Train Loss: 0.1054, Validation Loss: 0.1022
2025-12-31 12:47:59 - INFO - PRINT: Epoch [ 606], Train Loss: 0.1054, Validation Loss: 0.1022
2025-12-31 12:48:25 - INFO - PRINT: Epoch [ 607], Train Loss: 0.1051, Validation Loss: 0.1022
2025-12-31 12:48:52 - INFO - PRINT: Epoch [ 608], Train Loss: 0.1065, Validation Loss: 0.1022
2025-12-31 12:49:19 - INFO - PRINT: Epoch [ 609], Train Loss: 0.1060, Validation Loss: 0.1022
2025-12-31 12:49:46 - INFO - PRINT: Epoch [ 610], Train Loss: 0.1051, Validation Loss: 0.1022
2025-12-31 12:50:13 - INFO - PRINT: Epoch [ 611], Train Loss: 0.1054, Validation Loss: 0.1022
2025-12-31 12:50:39 - INFO - PRINT: Epoch [ 612], Train Loss: 0.1060, Validation Loss: 0.1022
2025-12-31 12:51:06 - INFO - PRINT: Epoch [ 613], Train Loss: 0.1054, Validation Loss: 0.1022
2025-12-31 12:51:33 - INFO - PRINT: Epoch [ 614], Train Loss: 0.1058, Validation Loss: 0.1022
2025-12-31 12:52:00 - INFO - PRINT: Epoch [ 615], Train Loss: 0.1056, Validation Loss: 0.1022
2025-12-31 12:52:26 - INFO - PRINT: Epoch [ 616], Train Loss: 0.1052, Validation Loss: 0.1022
2025-12-31 12:52:53 - INFO - PRINT: Epoch [ 617], Train Loss: 0.1065, Validation Loss: 0.1022
2025-12-31 12:53:20 - INFO - PRINT: Epoch [ 618], Train Loss: 0.1070, Validation Loss: 0.1022
2025-12-31 12:53:47 - INFO - PRINT: Epoch [ 619], Train Loss: 0.1049, Validation Loss: 0.1022
2025-12-31 12:54:18 - INFO - PRINT: Epoch [ 620], Train Loss: 0.1050, Validation Loss: 0.1021
2025-12-31 12:54:44 - INFO - PRINT: Epoch [ 621], Train Loss: 0.1049, Validation Loss: 0.1021
2025-12-31 12:55:11 - INFO - PRINT: Epoch [ 622], Train Loss: 0.1052, Validation Loss: 0.1021
2025-12-31 12:55:38 - INFO - PRINT: Epoch [ 623], Train Loss: 0.1051, Validation Loss: 0.1021
2025-12-31 12:56:04 - INFO - PRINT: Epoch [ 624], Train Loss: 0.1064, Validation Loss: 0.1021
2025-12-31 12:56:31 - INFO - PRINT: Epoch [ 625], Train Loss: 0.1052, Validation Loss: 0.1021
2025-12-31 12:56:58 - INFO - PRINT: Epoch [ 626], Train Loss: 0.1048, Validation Loss: 0.1021
2025-12-31 12:57:25 - INFO - PRINT: Epoch [ 627], Train Loss: 0.1052, Validation Loss: 0.1021
2025-12-31 12:57:51 - INFO - PRINT: Epoch [ 628], Train Loss: 0.1059, Validation Loss: 0.1021
2025-12-31 12:58:18 - INFO - PRINT: Epoch [ 629], Train Loss: 0.1053, Validation Loss: 0.1021
2025-12-31 12:58:45 - INFO - PRINT: Epoch [ 630], Train Loss: 0.1052, Validation Loss: 0.1021
2025-12-31 12:59:12 - INFO - PRINT: Epoch [ 631], Train Loss: 0.1054, Validation Loss: 0.1021
2025-12-31 12:59:38 - INFO - PRINT: Epoch [ 632], Train Loss: 0.1050, Validation Loss: 0.1021
2025-12-31 13:00:05 - INFO - PRINT: Epoch [ 633], Train Loss: 0.1054, Validation Loss: 0.1021
2025-12-31 13:00:32 - INFO - PRINT: Epoch [ 634], Train Loss: 0.1057, Validation Loss: 0.1021
2025-12-31 13:00:59 - INFO - PRINT: Epoch [ 635], Train Loss: 0.1046, Validation Loss: 0.1021
2025-12-31 13:01:25 - INFO - PRINT: Epoch [ 636], Train Loss: 0.1051, Validation Loss: 0.1021
2025-12-31 13:01:52 - INFO - PRINT: Epoch [ 637], Train Loss: 0.1062, Validation Loss: 0.1021
2025-12-31 13:02:19 - INFO - PRINT: Epoch [ 638], Train Loss: 0.1050, Validation Loss: 0.1021
2025-12-31 13:02:46 - INFO - PRINT: Epoch [ 639], Train Loss: 0.1062, Validation Loss: 0.1021
2025-12-31 13:03:17 - INFO - PRINT: Epoch [ 640], Train Loss: 0.1048, Validation Loss: 0.1023
2025-12-31 13:03:43 - INFO - PRINT: Epoch [ 641], Train Loss: 0.1050, Validation Loss: 0.1023
2025-12-31 13:04:10 - INFO - PRINT: Epoch [ 642], Train Loss: 0.1050, Validation Loss: 0.1023
2025-12-31 13:04:37 - INFO - PRINT: Epoch [ 643], Train Loss: 0.1058, Validation Loss: 0.1023
2025-12-31 13:05:03 - INFO - PRINT: Epoch [ 644], Train Loss: 0.1057, Validation Loss: 0.1023
2025-12-31 13:05:30 - INFO - PRINT: Epoch [ 645], Train Loss: 0.1052, Validation Loss: 0.1023
2025-12-31 13:05:57 - INFO - PRINT: Epoch [ 646], Train Loss: 0.1050, Validation Loss: 0.1023
2025-12-31 13:06:24 - INFO - PRINT: Epoch [ 647], Train Loss: 0.1045, Validation Loss: 0.1023
2025-12-31 13:06:50 - INFO - PRINT: Epoch [ 648], Train Loss: 0.1060, Validation Loss: 0.1023
2025-12-31 13:07:17 - INFO - PRINT: Epoch [ 649], Train Loss: 0.1051, Validation Loss: 0.1023
2025-12-31 13:07:44 - INFO - PRINT: Epoch [ 650], Train Loss: 0.1049, Validation Loss: 0.1023
2025-12-31 13:08:11 - INFO - PRINT: Epoch [ 651], Train Loss: 0.1057, Validation Loss: 0.1023
2025-12-31 13:08:37 - INFO - PRINT: Epoch [ 652], Train Loss: 0.1050, Validation Loss: 0.1023
2025-12-31 13:09:04 - INFO - PRINT: Epoch [ 653], Train Loss: 0.1063, Validation Loss: 0.1023
2025-12-31 13:09:31 - INFO - PRINT: Epoch [ 654], Train Loss: 0.1050, Validation Loss: 0.1023
2025-12-31 13:09:58 - INFO - PRINT: Epoch [ 655], Train Loss: 0.1050, Validation Loss: 0.1023
2025-12-31 13:10:24 - INFO - PRINT: Epoch [ 656], Train Loss: 0.1047, Validation Loss: 0.1023
2025-12-31 13:10:51 - INFO - PRINT: Epoch [ 657], Train Loss: 0.1051, Validation Loss: 0.1023
2025-12-31 13:11:18 - INFO - PRINT: Epoch [ 658], Train Loss: 0.1059, Validation Loss: 0.1023
2025-12-31 13:11:45 - INFO - PRINT: Epoch [ 659], Train Loss: 0.1047, Validation Loss: 0.1023
2025-12-31 13:12:16 - INFO - PRINT: Epoch [ 660], Train Loss: 0.1042, Validation Loss: 0.1023
2025-12-31 13:12:42 - INFO - PRINT: Epoch [ 661], Train Loss: 0.1048, Validation Loss: 0.1023
2025-12-31 13:13:09 - INFO - PRINT: Epoch [ 662], Train Loss: 0.1043, Validation Loss: 0.1023
2025-12-31 13:13:36 - INFO - PRINT: Epoch [ 663], Train Loss: 0.1045, Validation Loss: 0.1023
2025-12-31 13:14:02 - INFO - PRINT: Epoch [ 664], Train Loss: 0.1049, Validation Loss: 0.1023
2025-12-31 13:14:29 - INFO - PRINT: Epoch [ 665], Train Loss: 0.1049, Validation Loss: 0.1023
2025-12-31 13:14:56 - INFO - PRINT: Epoch [ 666], Train Loss: 0.1060, Validation Loss: 0.1023
2025-12-31 13:15:23 - INFO - PRINT: Epoch [ 667], Train Loss: 0.1043, Validation Loss: 0.1023
2025-12-31 13:15:49 - INFO - PRINT: Epoch [ 668], Train Loss: 0.1049, Validation Loss: 0.1023
2025-12-31 13:16:16 - INFO - PRINT: Epoch [ 669], Train Loss: 0.1049, Validation Loss: 0.1023
2025-12-31 13:16:43 - INFO - PRINT: Epoch [ 670], Train Loss: 0.1059, Validation Loss: 0.1023
2025-12-31 13:17:10 - INFO - PRINT: Epoch [ 671], Train Loss: 0.1057, Validation Loss: 0.1023
2025-12-31 13:17:36 - INFO - PRINT: Epoch [ 672], Train Loss: 0.1055, Validation Loss: 0.1023
2025-12-31 13:18:03 - INFO - PRINT: Epoch [ 673], Train Loss: 0.1048, Validation Loss: 0.1023
2025-12-31 13:18:30 - INFO - PRINT: Epoch [ 674], Train Loss: 0.1071, Validation Loss: 0.1023
2025-12-31 13:18:57 - INFO - PRINT: Epoch [ 675], Train Loss: 0.1051, Validation Loss: 0.1023
2025-12-31 13:19:23 - INFO - PRINT: Epoch [ 676], Train Loss: 0.1040, Validation Loss: 0.1023
2025-12-31 13:19:50 - INFO - PRINT: Epoch [ 677], Train Loss: 0.1042, Validation Loss: 0.1023
2025-12-31 13:20:17 - INFO - PRINT: Epoch [ 678], Train Loss: 0.1043, Validation Loss: 0.1023
2025-12-31 13:20:44 - INFO - PRINT: Epoch [ 679], Train Loss: 0.1045, Validation Loss: 0.1023
2025-12-31 13:21:15 - INFO - PRINT: Epoch [ 680], Train Loss: 0.1045, Validation Loss: 0.1020
2025-12-31 13:21:41 - INFO - PRINT: Epoch [ 681], Train Loss: 0.1050, Validation Loss: 0.1020
2025-12-31 13:22:08 - INFO - PRINT: Epoch [ 682], Train Loss: 0.1044, Validation Loss: 0.1020
2025-12-31 13:22:35 - INFO - PRINT: Epoch [ 683], Train Loss: 0.1050, Validation Loss: 0.1020
2025-12-31 13:23:01 - INFO - PRINT: Epoch [ 684], Train Loss: 0.1053, Validation Loss: 0.1020
2025-12-31 13:23:28 - INFO - PRINT: Epoch [ 685], Train Loss: 0.1062, Validation Loss: 0.1020
2025-12-31 13:23:55 - INFO - PRINT: Epoch [ 686], Train Loss: 0.1043, Validation Loss: 0.1020
2025-12-31 13:24:22 - INFO - PRINT: Epoch [ 687], Train Loss: 0.1049, Validation Loss: 0.1020
2025-12-31 13:24:48 - INFO - PRINT: Epoch [ 688], Train Loss: 0.1050, Validation Loss: 0.1020
2025-12-31 13:25:15 - INFO - PRINT: Epoch [ 689], Train Loss: 0.1055, Validation Loss: 0.1020
2025-12-31 13:25:42 - INFO - PRINT: Epoch [ 690], Train Loss: 0.1048, Validation Loss: 0.1020
2025-12-31 13:26:09 - INFO - PRINT: Epoch [ 691], Train Loss: 0.1045, Validation Loss: 0.1020
2025-12-31 13:26:35 - INFO - PRINT: Epoch [ 692], Train Loss: 0.1048, Validation Loss: 0.1020
2025-12-31 13:27:02 - INFO - PRINT: Epoch [ 693], Train Loss: 0.1052, Validation Loss: 0.1020
2025-12-31 13:27:29 - INFO - PRINT: Epoch [ 694], Train Loss: 0.1041, Validation Loss: 0.1020
2025-12-31 13:27:56 - INFO - PRINT: Epoch [ 695], Train Loss: 0.1050, Validation Loss: 0.1020
2025-12-31 13:28:22 - INFO - PRINT: Epoch [ 696], Train Loss: 0.1050, Validation Loss: 0.1020
2025-12-31 13:28:49 - INFO - PRINT: Epoch [ 697], Train Loss: 0.1044, Validation Loss: 0.1020
2025-12-31 13:29:16 - INFO - PRINT: Epoch [ 698], Train Loss: 0.1040, Validation Loss: 0.1020
2025-12-31 13:29:43 - INFO - PRINT: Epoch [ 699], Train Loss: 0.1040, Validation Loss: 0.1020
2025-12-31 13:30:14 - INFO - PRINT: Epoch [ 700], Train Loss: 0.1040, Validation Loss: 0.1034
2025-12-31 13:30:14 - INFO - PRINT: ----> Saving model from epoch 680 (val loss: 0.10200222939252854). Syrupy!
2025-12-31 13:30:40 - INFO - PRINT: Epoch [ 701], Train Loss: 0.1063, Validation Loss: 0.1034
2025-12-31 13:31:07 - INFO - PRINT: Epoch [ 702], Train Loss: 0.1059, Validation Loss: 0.1034
2025-12-31 13:31:34 - INFO - PRINT: Epoch [ 703], Train Loss: 0.1051, Validation Loss: 0.1034
2025-12-31 13:32:00 - INFO - PRINT: Epoch [ 704], Train Loss: 0.1043, Validation Loss: 0.1034
2025-12-31 13:32:27 - INFO - PRINT: Epoch [ 705], Train Loss: 0.1064, Validation Loss: 0.1034
2025-12-31 13:32:54 - INFO - PRINT: Epoch [ 706], Train Loss: 0.1060, Validation Loss: 0.1034
2025-12-31 13:33:21 - INFO - PRINT: Epoch [ 707], Train Loss: 0.1054, Validation Loss: 0.1034
2025-12-31 13:33:47 - INFO - PRINT: Epoch [ 708], Train Loss: 0.1042, Validation Loss: 0.1034
2025-12-31 13:34:14 - INFO - PRINT: Epoch [ 709], Train Loss: 0.1043, Validation Loss: 0.1034
2025-12-31 13:34:41 - INFO - PRINT: Epoch [ 710], Train Loss: 0.1036, Validation Loss: 0.1034
2025-12-31 13:35:08 - INFO - PRINT: Epoch [ 711], Train Loss: 0.1056, Validation Loss: 0.1034
2025-12-31 13:35:34 - INFO - PRINT: Epoch [ 712], Train Loss: 0.1058, Validation Loss: 0.1034
2025-12-31 13:36:01 - INFO - PRINT: Epoch [ 713], Train Loss: 0.1041, Validation Loss: 0.1034
2025-12-31 13:36:28 - INFO - PRINT: Epoch [ 714], Train Loss: 0.1036, Validation Loss: 0.1034
2025-12-31 13:36:55 - INFO - PRINT: Epoch [ 715], Train Loss: 0.1041, Validation Loss: 0.1034
2025-12-31 13:37:21 - INFO - PRINT: Epoch [ 716], Train Loss: 0.1045, Validation Loss: 0.1034
2025-12-31 13:37:48 - INFO - PRINT: Epoch [ 717], Train Loss: 0.1038, Validation Loss: 0.1034
2025-12-31 13:38:15 - INFO - PRINT: Epoch [ 718], Train Loss: 0.1052, Validation Loss: 0.1034
2025-12-31 13:38:42 - INFO - PRINT: Epoch [ 719], Train Loss: 0.1044, Validation Loss: 0.1034
2025-12-31 13:39:13 - INFO - PRINT: Epoch [ 720], Train Loss: 0.1041, Validation Loss: 0.1016
2025-12-31 13:39:39 - INFO - PRINT: Epoch [ 721], Train Loss: 0.1050, Validation Loss: 0.1016
2025-12-31 13:40:06 - INFO - PRINT: Epoch [ 722], Train Loss: 0.1058, Validation Loss: 0.1016
2025-12-31 13:40:33 - INFO - PRINT: Epoch [ 723], Train Loss: 0.1044, Validation Loss: 0.1016
2025-12-31 13:40:59 - INFO - PRINT: Epoch [ 724], Train Loss: 0.1037, Validation Loss: 0.1016
2025-12-31 13:41:26 - INFO - PRINT: Epoch [ 725], Train Loss: 0.1041, Validation Loss: 0.1016
2025-12-31 13:41:53 - INFO - PRINT: Epoch [ 726], Train Loss: 0.1050, Validation Loss: 0.1016
2025-12-31 13:42:20 - INFO - PRINT: Epoch [ 727], Train Loss: 0.1045, Validation Loss: 0.1016
2025-12-31 13:42:46 - INFO - PRINT: Epoch [ 728], Train Loss: 0.1052, Validation Loss: 0.1016
2025-12-31 13:43:13 - INFO - PRINT: Epoch [ 729], Train Loss: 0.1046, Validation Loss: 0.1016
2025-12-31 13:43:40 - INFO - PRINT: Epoch [ 730], Train Loss: 0.1046, Validation Loss: 0.1016
2025-12-31 13:44:07 - INFO - PRINT: Epoch [ 731], Train Loss: 0.1044, Validation Loss: 0.1016
2025-12-31 13:44:33 - INFO - PRINT: Epoch [ 732], Train Loss: 0.1040, Validation Loss: 0.1016
2025-12-31 13:45:00 - INFO - PRINT: Epoch [ 733], Train Loss: 0.1037, Validation Loss: 0.1016
2025-12-31 13:45:27 - INFO - PRINT: Epoch [ 734], Train Loss: 0.1039, Validation Loss: 0.1016
2025-12-31 13:45:54 - INFO - PRINT: Epoch [ 735], Train Loss: 0.1044, Validation Loss: 0.1016
2025-12-31 13:46:20 - INFO - PRINT: Epoch [ 736], Train Loss: 0.1054, Validation Loss: 0.1016
2025-12-31 13:46:47 - INFO - PRINT: Epoch [ 737], Train Loss: 0.1047, Validation Loss: 0.1016
2025-12-31 13:47:14 - INFO - PRINT: Epoch [ 738], Train Loss: 0.1041, Validation Loss: 0.1016
2025-12-31 13:47:41 - INFO - PRINT: Epoch [ 739], Train Loss: 0.1047, Validation Loss: 0.1016
2025-12-31 13:48:12 - INFO - PRINT: Epoch [ 740], Train Loss: 0.1042, Validation Loss: 0.1029
2025-12-31 13:48:38 - INFO - PRINT: Epoch [ 741], Train Loss: 0.1047, Validation Loss: 0.1029
2025-12-31 13:49:05 - INFO - PRINT: Epoch [ 742], Train Loss: 0.1037, Validation Loss: 0.1029
2025-12-31 13:49:32 - INFO - PRINT: Epoch [ 743], Train Loss: 0.1045, Validation Loss: 0.1029
2025-12-31 13:49:58 - INFO - PRINT: Epoch [ 744], Train Loss: 0.1042, Validation Loss: 0.1029
2025-12-31 13:50:25 - INFO - PRINT: Epoch [ 745], Train Loss: 0.1043, Validation Loss: 0.1029
2025-12-31 13:50:52 - INFO - PRINT: Epoch [ 746], Train Loss: 0.1041, Validation Loss: 0.1029
2025-12-31 13:51:19 - INFO - PRINT: Epoch [ 747], Train Loss: 0.1035, Validation Loss: 0.1029
2025-12-31 13:51:45 - INFO - PRINT: Epoch [ 748], Train Loss: 0.1038, Validation Loss: 0.1029
2025-12-31 13:52:12 - INFO - PRINT: Epoch [ 749], Train Loss: 0.1055, Validation Loss: 0.1029
2025-12-31 13:52:39 - INFO - PRINT: Epoch [ 750], Train Loss: 0.1049, Validation Loss: 0.1029
2025-12-31 13:53:06 - INFO - PRINT: Epoch [ 751], Train Loss: 0.1045, Validation Loss: 0.1029
2025-12-31 13:53:32 - INFO - PRINT: Epoch [ 752], Train Loss: 0.1039, Validation Loss: 0.1029
2025-12-31 13:53:59 - INFO - PRINT: Epoch [ 753], Train Loss: 0.1048, Validation Loss: 0.1029
2025-12-31 13:54:26 - INFO - PRINT: Epoch [ 754], Train Loss: 0.1055, Validation Loss: 0.1029
2025-12-31 13:54:53 - INFO - PRINT: Epoch [ 755], Train Loss: 0.1046, Validation Loss: 0.1029
2025-12-31 13:55:19 - INFO - PRINT: Epoch [ 756], Train Loss: 0.1034, Validation Loss: 0.1029
2025-12-31 13:55:46 - INFO - PRINT: Epoch [ 757], Train Loss: 0.1038, Validation Loss: 0.1029
2025-12-31 13:56:13 - INFO - PRINT: Epoch [ 758], Train Loss: 0.1046, Validation Loss: 0.1029
2025-12-31 13:56:40 - INFO - PRINT: Epoch [ 759], Train Loss: 0.1043, Validation Loss: 0.1029
2025-12-31 13:57:11 - INFO - PRINT: Epoch [ 760], Train Loss: 0.1037, Validation Loss: 0.1034
2025-12-31 13:57:37 - INFO - PRINT: Epoch [ 761], Train Loss: 0.1049, Validation Loss: 0.1034
2025-12-31 13:58:04 - INFO - PRINT: Epoch [ 762], Train Loss: 0.1042, Validation Loss: 0.1034
2025-12-31 13:58:31 - INFO - PRINT: Epoch [ 763], Train Loss: 0.1040, Validation Loss: 0.1034
2025-12-31 13:58:57 - INFO - PRINT: Epoch [ 764], Train Loss: 0.1046, Validation Loss: 0.1034
2025-12-31 13:59:24 - INFO - PRINT: Epoch [ 765], Train Loss: 0.1034, Validation Loss: 0.1034
2025-12-31 13:59:51 - INFO - PRINT: Epoch [ 766], Train Loss: 0.1043, Validation Loss: 0.1034
2025-12-31 14:00:18 - INFO - PRINT: Epoch [ 767], Train Loss: 0.1051, Validation Loss: 0.1034
2025-12-31 14:00:44 - INFO - PRINT: Epoch [ 768], Train Loss: 0.1039, Validation Loss: 0.1034
2025-12-31 14:01:11 - INFO - PRINT: Epoch [ 769], Train Loss: 0.1037, Validation Loss: 0.1034
2025-12-31 14:01:38 - INFO - PRINT: Epoch [ 770], Train Loss: 0.1035, Validation Loss: 0.1034
2025-12-31 14:02:05 - INFO - PRINT: Epoch [ 771], Train Loss: 0.1043, Validation Loss: 0.1034
2025-12-31 14:02:31 - INFO - PRINT: Epoch [ 772], Train Loss: 0.1037, Validation Loss: 0.1034
2025-12-31 14:02:58 - INFO - PRINT: Epoch [ 773], Train Loss: 0.1043, Validation Loss: 0.1034
2025-12-31 14:03:25 - INFO - PRINT: Epoch [ 774], Train Loss: 0.1053, Validation Loss: 0.1034
2025-12-31 14:03:52 - INFO - PRINT: Epoch [ 775], Train Loss: 0.1040, Validation Loss: 0.1034
2025-12-31 14:04:18 - INFO - PRINT: Epoch [ 776], Train Loss: 0.1033, Validation Loss: 0.1034
2025-12-31 14:04:45 - INFO - PRINT: Epoch [ 777], Train Loss: 0.1033, Validation Loss: 0.1034
2025-12-31 14:05:12 - INFO - PRINT: Epoch [ 778], Train Loss: 0.1037, Validation Loss: 0.1034
2025-12-31 14:05:39 - INFO - PRINT: Epoch [ 779], Train Loss: 0.1040, Validation Loss: 0.1034
2025-12-31 14:06:10 - INFO - PRINT: Epoch [ 780], Train Loss: 0.1039, Validation Loss: 0.1043
2025-12-31 14:06:36 - INFO - PRINT: Epoch [ 781], Train Loss: 0.1040, Validation Loss: 0.1043
2025-12-31 14:07:03 - INFO - PRINT: Epoch [ 782], Train Loss: 0.1038, Validation Loss: 0.1043
2025-12-31 14:07:30 - INFO - PRINT: Epoch [ 783], Train Loss: 0.1063, Validation Loss: 0.1043
2025-12-31 14:07:56 - INFO - PRINT: Epoch [ 784], Train Loss: 0.1053, Validation Loss: 0.1043
2025-12-31 14:08:23 - INFO - PRINT: Epoch [ 785], Train Loss: 0.1039, Validation Loss: 0.1043
2025-12-31 14:08:50 - INFO - PRINT: Epoch [ 786], Train Loss: 0.1033, Validation Loss: 0.1043
2025-12-31 14:09:17 - INFO - PRINT: Epoch [ 787], Train Loss: 0.1034, Validation Loss: 0.1043
2025-12-31 14:09:43 - INFO - PRINT: Epoch [ 788], Train Loss: 0.1037, Validation Loss: 0.1043
2025-12-31 14:10:10 - INFO - PRINT: Epoch [ 789], Train Loss: 0.1039, Validation Loss: 0.1043
2025-12-31 14:10:37 - INFO - PRINT: Epoch [ 790], Train Loss: 0.1040, Validation Loss: 0.1043
2025-12-31 14:11:04 - INFO - PRINT: Epoch [ 791], Train Loss: 0.1044, Validation Loss: 0.1043
2025-12-31 14:11:30 - INFO - PRINT: Epoch [ 792], Train Loss: 0.1039, Validation Loss: 0.1043
2025-12-31 14:11:57 - INFO - PRINT: Epoch [ 793], Train Loss: 0.1035, Validation Loss: 0.1043
2025-12-31 14:12:24 - INFO - PRINT: Epoch [ 794], Train Loss: 0.1038, Validation Loss: 0.1043
2025-12-31 14:12:51 - INFO - PRINT: Epoch [ 795], Train Loss: 0.1048, Validation Loss: 0.1043
2025-12-31 14:13:17 - INFO - PRINT: Epoch [ 796], Train Loss: 0.1044, Validation Loss: 0.1043
2025-12-31 14:13:44 - INFO - PRINT: Epoch [ 797], Train Loss: 0.1035, Validation Loss: 0.1043
2025-12-31 14:14:11 - INFO - PRINT: Epoch [ 798], Train Loss: 0.1037, Validation Loss: 0.1043
2025-12-31 14:14:38 - INFO - PRINT: Epoch [ 799], Train Loss: 0.1035, Validation Loss: 0.1043
2025-12-31 14:15:09 - INFO - PRINT: Epoch [ 800], Train Loss: 0.1045, Validation Loss: 0.1021
2025-12-31 14:15:09 - INFO - PRINT: ----> Saving model from epoch 720 (val loss: 0.10157190725207328). Irresistible!
2025-12-31 14:15:35 - INFO - PRINT: Epoch [ 801], Train Loss: 0.1048, Validation Loss: 0.1021
2025-12-31 14:16:02 - INFO - PRINT: Epoch [ 802], Train Loss: 0.1035, Validation Loss: 0.1021
2025-12-31 14:16:29 - INFO - PRINT: Epoch [ 803], Train Loss: 0.1041, Validation Loss: 0.1021
2025-12-31 14:16:55 - INFO - PRINT: Epoch [ 804], Train Loss: 0.1044, Validation Loss: 0.1021
2025-12-31 14:17:22 - INFO - PRINT: Epoch [ 805], Train Loss: 0.1055, Validation Loss: 0.1021
2025-12-31 14:17:49 - INFO - PRINT: Epoch [ 806], Train Loss: 0.1037, Validation Loss: 0.1021
2025-12-31 14:18:16 - INFO - PRINT: Epoch [ 807], Train Loss: 0.1039, Validation Loss: 0.1021
2025-12-31 14:18:42 - INFO - PRINT: Epoch [ 808], Train Loss: 0.1031, Validation Loss: 0.1021
2025-12-31 14:19:09 - INFO - PRINT: Epoch [ 809], Train Loss: 0.1033, Validation Loss: 0.1021
2025-12-31 14:19:36 - INFO - PRINT: Epoch [ 810], Train Loss: 0.1035, Validation Loss: 0.1021
2025-12-31 14:20:03 - INFO - PRINT: Epoch [ 811], Train Loss: 0.1040, Validation Loss: 0.1021
2025-12-31 14:20:29 - INFO - PRINT: Epoch [ 812], Train Loss: 0.1045, Validation Loss: 0.1021
2025-12-31 14:20:56 - INFO - PRINT: Epoch [ 813], Train Loss: 0.1038, Validation Loss: 0.1021
2025-12-31 14:21:23 - INFO - PRINT: Epoch [ 814], Train Loss: 0.1032, Validation Loss: 0.1021
2025-12-31 14:21:50 - INFO - PRINT: Epoch [ 815], Train Loss: 0.1033, Validation Loss: 0.1021
2025-12-31 14:22:16 - INFO - PRINT: Epoch [ 816], Train Loss: 0.1039, Validation Loss: 0.1021
2025-12-31 14:22:43 - INFO - PRINT: Epoch [ 817], Train Loss: 0.1038, Validation Loss: 0.1021
2025-12-31 14:23:10 - INFO - PRINT: Epoch [ 818], Train Loss: 0.1040, Validation Loss: 0.1021
2025-12-31 14:23:37 - INFO - PRINT: Epoch [ 819], Train Loss: 0.1030, Validation Loss: 0.1021
2025-12-31 14:24:08 - INFO - PRINT: Epoch [ 820], Train Loss: 0.1037, Validation Loss: 0.1017
2025-12-31 14:24:34 - INFO - PRINT: Epoch [ 821], Train Loss: 0.1043, Validation Loss: 0.1017
2025-12-31 14:25:01 - INFO - PRINT: Epoch [ 822], Train Loss: 0.1043, Validation Loss: 0.1017
2025-12-31 14:25:28 - INFO - PRINT: Epoch [ 823], Train Loss: 0.1043, Validation Loss: 0.1017
2025-12-31 14:25:54 - INFO - PRINT: Epoch [ 824], Train Loss: 0.1043, Validation Loss: 0.1017
2025-12-31 14:26:21 - INFO - PRINT: Epoch [ 825], Train Loss: 0.1039, Validation Loss: 0.1017
2025-12-31 14:26:48 - INFO - PRINT: Epoch [ 826], Train Loss: 0.1033, Validation Loss: 0.1017
2025-12-31 14:27:15 - INFO - PRINT: Epoch [ 827], Train Loss: 0.1039, Validation Loss: 0.1017
2025-12-31 14:27:41 - INFO - PRINT: Epoch [ 828], Train Loss: 0.1039, Validation Loss: 0.1017
2025-12-31 14:28:08 - INFO - PRINT: Epoch [ 829], Train Loss: 0.1030, Validation Loss: 0.1017
2025-12-31 14:28:35 - INFO - PRINT: Epoch [ 830], Train Loss: 0.1036, Validation Loss: 0.1017
2025-12-31 14:29:02 - INFO - PRINT: Epoch [ 831], Train Loss: 0.1040, Validation Loss: 0.1017
2025-12-31 14:29:28 - INFO - PRINT: Epoch [ 832], Train Loss: 0.1034, Validation Loss: 0.1017
2025-12-31 14:29:55 - INFO - PRINT: Epoch [ 833], Train Loss: 0.1032, Validation Loss: 0.1017
2025-12-31 14:30:22 - INFO - PRINT: Epoch [ 834], Train Loss: 0.1031, Validation Loss: 0.1017
2025-12-31 14:30:49 - INFO - PRINT: Epoch [ 835], Train Loss: 0.1038, Validation Loss: 0.1017
2025-12-31 14:31:15 - INFO - PRINT: Epoch [ 836], Train Loss: 0.1039, Validation Loss: 0.1017
2025-12-31 14:31:42 - INFO - PRINT: Epoch [ 837], Train Loss: 0.1033, Validation Loss: 0.1017
2025-12-31 14:32:09 - INFO - PRINT: Epoch [ 838], Train Loss: 0.1035, Validation Loss: 0.1017
2025-12-31 14:32:36 - INFO - PRINT: Epoch [ 839], Train Loss: 0.1037, Validation Loss: 0.1017
2025-12-31 14:33:07 - INFO - PRINT: Epoch [ 840], Train Loss: 0.1030, Validation Loss: 0.1011
2025-12-31 14:33:33 - INFO - PRINT: Epoch [ 841], Train Loss: 0.1044, Validation Loss: 0.1011
2025-12-31 14:34:00 - INFO - PRINT: Epoch [ 842], Train Loss: 0.1036, Validation Loss: 0.1011
2025-12-31 14:34:27 - INFO - PRINT: Epoch [ 843], Train Loss: 0.1038, Validation Loss: 0.1011
2025-12-31 14:34:53 - INFO - PRINT: Epoch [ 844], Train Loss: 0.1033, Validation Loss: 0.1011
2025-12-31 14:35:20 - INFO - PRINT: Epoch [ 845], Train Loss: 0.1042, Validation Loss: 0.1011
2025-12-31 14:35:47 - INFO - PRINT: Epoch [ 846], Train Loss: 0.1038, Validation Loss: 0.1011
2025-12-31 14:36:14 - INFO - PRINT: Epoch [ 847], Train Loss: 0.1047, Validation Loss: 0.1011
2025-12-31 14:36:40 - INFO - PRINT: Epoch [ 848], Train Loss: 0.1033, Validation Loss: 0.1011
2025-12-31 14:37:07 - INFO - PRINT: Epoch [ 849], Train Loss: 0.1033, Validation Loss: 0.1011
2025-12-31 14:37:34 - INFO - PRINT: Epoch [ 850], Train Loss: 0.1033, Validation Loss: 0.1011
2025-12-31 14:38:01 - INFO - PRINT: Epoch [ 851], Train Loss: 0.1032, Validation Loss: 0.1011
2025-12-31 14:38:27 - INFO - PRINT: Epoch [ 852], Train Loss: 0.1037, Validation Loss: 0.1011
2025-12-31 14:38:54 - INFO - PRINT: Epoch [ 853], Train Loss: 0.1034, Validation Loss: 0.1011
2025-12-31 14:39:21 - INFO - PRINT: Epoch [ 854], Train Loss: 0.1039, Validation Loss: 0.1011
2025-12-31 14:39:48 - INFO - PRINT: Epoch [ 855], Train Loss: 0.1028, Validation Loss: 0.1011
2025-12-31 14:40:14 - INFO - PRINT: Epoch [ 856], Train Loss: 0.1031, Validation Loss: 0.1011
2025-12-31 14:40:41 - INFO - PRINT: Epoch [ 857], Train Loss: 0.1034, Validation Loss: 0.1011
2025-12-31 14:41:08 - INFO - PRINT: Epoch [ 858], Train Loss: 0.1046, Validation Loss: 0.1011
2025-12-31 14:41:35 - INFO - PRINT: Epoch [ 859], Train Loss: 0.1042, Validation Loss: 0.1011
2025-12-31 14:42:06 - INFO - PRINT: Epoch [ 860], Train Loss: 0.1032, Validation Loss: 0.1007
2025-12-31 14:42:32 - INFO - PRINT: Epoch [ 861], Train Loss: 0.1037, Validation Loss: 0.1007
2025-12-31 14:42:59 - INFO - PRINT: Epoch [ 862], Train Loss: 0.1038, Validation Loss: 0.1007
2025-12-31 14:43:26 - INFO - PRINT: Epoch [ 863], Train Loss: 0.1036, Validation Loss: 0.1007
2025-12-31 14:43:52 - INFO - PRINT: Epoch [ 864], Train Loss: 0.1026, Validation Loss: 0.1007
2025-12-31 14:44:19 - INFO - PRINT: Epoch [ 865], Train Loss: 0.1037, Validation Loss: 0.1007
2025-12-31 14:44:46 - INFO - PRINT: Epoch [ 866], Train Loss: 0.1035, Validation Loss: 0.1007
2025-12-31 14:45:13 - INFO - PRINT: Epoch [ 867], Train Loss: 0.1046, Validation Loss: 0.1007
2025-12-31 14:45:39 - INFO - PRINT: Epoch [ 868], Train Loss: 0.1032, Validation Loss: 0.1007
2025-12-31 14:46:06 - INFO - PRINT: Epoch [ 869], Train Loss: 0.1036, Validation Loss: 0.1007
2025-12-31 14:46:33 - INFO - PRINT: Epoch [ 870], Train Loss: 0.1032, Validation Loss: 0.1007
2025-12-31 14:47:00 - INFO - PRINT: Epoch [ 871], Train Loss: 0.1050, Validation Loss: 0.1007
2025-12-31 14:47:26 - INFO - PRINT: Epoch [ 872], Train Loss: 0.1037, Validation Loss: 0.1007
2025-12-31 14:47:53 - INFO - PRINT: Epoch [ 873], Train Loss: 0.1043, Validation Loss: 0.1007
2025-12-31 14:48:20 - INFO - PRINT: Epoch [ 874], Train Loss: 0.1030, Validation Loss: 0.1007
2025-12-31 14:48:47 - INFO - PRINT: Epoch [ 875], Train Loss: 0.1028, Validation Loss: 0.1007
2025-12-31 14:49:13 - INFO - PRINT: Epoch [ 876], Train Loss: 0.1041, Validation Loss: 0.1007
2025-12-31 14:49:40 - INFO - PRINT: Epoch [ 877], Train Loss: 0.1033, Validation Loss: 0.1007
2025-12-31 14:50:07 - INFO - PRINT: Epoch [ 878], Train Loss: 0.1038, Validation Loss: 0.1007
2025-12-31 14:50:34 - INFO - PRINT: Epoch [ 879], Train Loss: 0.1037, Validation Loss: 0.1007
2025-12-31 14:51:05 - INFO - PRINT: Epoch [ 880], Train Loss: 0.1031, Validation Loss: 0.1013
2025-12-31 14:51:31 - INFO - PRINT: Epoch [ 881], Train Loss: 0.1035, Validation Loss: 0.1013
2025-12-31 14:51:58 - INFO - PRINT: Epoch [ 882], Train Loss: 0.1047, Validation Loss: 0.1013
2025-12-31 14:52:25 - INFO - PRINT: Epoch [ 883], Train Loss: 0.1050, Validation Loss: 0.1013
2025-12-31 14:52:51 - INFO - PRINT: Epoch [ 884], Train Loss: 0.1030, Validation Loss: 0.1013
2025-12-31 14:53:18 - INFO - PRINT: Epoch [ 885], Train Loss: 0.1037, Validation Loss: 0.1013
2025-12-31 14:53:45 - INFO - PRINT: Epoch [ 886], Train Loss: 0.1028, Validation Loss: 0.1013
2025-12-31 14:54:12 - INFO - PRINT: Epoch [ 887], Train Loss: 0.1032, Validation Loss: 0.1013
2025-12-31 14:54:38 - INFO - PRINT: Epoch [ 888], Train Loss: 0.1030, Validation Loss: 0.1013
2025-12-31 14:55:05 - INFO - PRINT: Epoch [ 889], Train Loss: 0.1030, Validation Loss: 0.1013
2025-12-31 14:55:32 - INFO - PRINT: Epoch [ 890], Train Loss: 0.1038, Validation Loss: 0.1013
2025-12-31 14:55:58 - INFO - PRINT: Epoch [ 891], Train Loss: 0.1039, Validation Loss: 0.1013
2025-12-31 14:56:25 - INFO - PRINT: Epoch [ 892], Train Loss: 0.1035, Validation Loss: 0.1013
2025-12-31 14:56:52 - INFO - PRINT: Epoch [ 893], Train Loss: 0.1036, Validation Loss: 0.1013
2025-12-31 14:57:19 - INFO - PRINT: Epoch [ 894], Train Loss: 0.1036, Validation Loss: 0.1013
2025-12-31 14:57:45 - INFO - PRINT: Epoch [ 895], Train Loss: 0.1028, Validation Loss: 0.1013
2025-12-31 14:58:12 - INFO - PRINT: Epoch [ 896], Train Loss: 0.1044, Validation Loss: 0.1013
2025-12-31 14:58:39 - INFO - PRINT: Epoch [ 897], Train Loss: 0.1033, Validation Loss: 0.1013
2025-12-31 14:59:06 - INFO - PRINT: Epoch [ 898], Train Loss: 0.1026, Validation Loss: 0.1013
2025-12-31 14:59:33 - INFO - PRINT: Epoch [ 899], Train Loss: 0.1034, Validation Loss: 0.1013
2025-12-31 15:00:04 - INFO - PRINT: Epoch [ 900], Train Loss: 0.1034, Validation Loss: 0.1007
2025-12-31 15:00:04 - INFO - PRINT: ----> Saving model from epoch 900 (val loss: 0.10070091322064399). Sublime!
2025-12-31 15:00:30 - INFO - PRINT: Epoch [ 901], Train Loss: 0.1023, Validation Loss: 0.1007
2025-12-31 15:00:57 - INFO - PRINT: Epoch [ 902], Train Loss: 0.1030, Validation Loss: 0.1007
2025-12-31 15:01:24 - INFO - PRINT: Epoch [ 903], Train Loss: 0.1031, Validation Loss: 0.1007
2025-12-31 15:01:50 - INFO - PRINT: Epoch [ 904], Train Loss: 0.1046, Validation Loss: 0.1007
2025-12-31 15:02:17 - INFO - PRINT: Epoch [ 905], Train Loss: 0.1028, Validation Loss: 0.1007
2025-12-31 15:02:44 - INFO - PRINT: Epoch [ 906], Train Loss: 0.1036, Validation Loss: 0.1007
2025-12-31 15:03:11 - INFO - PRINT: Epoch [ 907], Train Loss: 0.1057, Validation Loss: 0.1007
2025-12-31 15:03:37 - INFO - PRINT: Epoch [ 908], Train Loss: 0.1026, Validation Loss: 0.1007
2025-12-31 15:04:04 - INFO - PRINT: Epoch [ 909], Train Loss: 0.1034, Validation Loss: 0.1007
2025-12-31 15:04:31 - INFO - PRINT: Epoch [ 910], Train Loss: 0.1028, Validation Loss: 0.1007
2025-12-31 15:04:58 - INFO - PRINT: Epoch [ 911], Train Loss: 0.1028, Validation Loss: 0.1007
2025-12-31 15:05:25 - INFO - PRINT: Epoch [ 912], Train Loss: 0.1030, Validation Loss: 0.1007
2025-12-31 15:05:51 - INFO - PRINT: Epoch [ 913], Train Loss: 0.1036, Validation Loss: 0.1007
2025-12-31 15:06:18 - INFO - PRINT: Epoch [ 914], Train Loss: 0.1025, Validation Loss: 0.1007
2025-12-31 15:06:45 - INFO - PRINT: Epoch [ 915], Train Loss: 0.1031, Validation Loss: 0.1007
2025-12-31 15:07:12 - INFO - PRINT: Epoch [ 916], Train Loss: 0.1025, Validation Loss: 0.1007
2025-12-31 15:07:38 - INFO - PRINT: Epoch [ 917], Train Loss: 0.1026, Validation Loss: 0.1007
2025-12-31 15:08:05 - INFO - PRINT: Epoch [ 918], Train Loss: 0.1033, Validation Loss: 0.1007
2025-12-31 15:08:32 - INFO - PRINT: Epoch [ 919], Train Loss: 0.1035, Validation Loss: 0.1007
2025-12-31 15:09:03 - INFO - PRINT: Epoch [ 920], Train Loss: 0.1044, Validation Loss: 0.1023
2025-12-31 15:09:29 - INFO - PRINT: Epoch [ 921], Train Loss: 0.1035, Validation Loss: 0.1023
2025-12-31 15:09:56 - INFO - PRINT: Epoch [ 922], Train Loss: 0.1040, Validation Loss: 0.1023
2025-12-31 15:10:23 - INFO - PRINT: Epoch [ 923], Train Loss: 0.1030, Validation Loss: 0.1023
2025-12-31 15:10:50 - INFO - PRINT: Epoch [ 924], Train Loss: 0.1034, Validation Loss: 0.1023
2025-12-31 15:11:16 - INFO - PRINT: Epoch [ 925], Train Loss: 0.1037, Validation Loss: 0.1023
2025-12-31 15:11:43 - INFO - PRINT: Epoch [ 926], Train Loss: 0.1030, Validation Loss: 0.1023
2025-12-31 15:12:10 - INFO - PRINT: Epoch [ 927], Train Loss: 0.1033, Validation Loss: 0.1023
2025-12-31 15:12:37 - INFO - PRINT: Epoch [ 928], Train Loss: 0.1030, Validation Loss: 0.1023
2025-12-31 15:13:03 - INFO - PRINT: Epoch [ 929], Train Loss: 0.1041, Validation Loss: 0.1023
2025-12-31 15:13:30 - INFO - PRINT: Epoch [ 930], Train Loss: 0.1028, Validation Loss: 0.1023
2025-12-31 15:13:57 - INFO - PRINT: Epoch [ 931], Train Loss: 0.1038, Validation Loss: 0.1023
2025-12-31 15:14:24 - INFO - PRINT: Epoch [ 932], Train Loss: 0.1035, Validation Loss: 0.1023
2025-12-31 15:14:50 - INFO - PRINT: Epoch [ 933], Train Loss: 0.1032, Validation Loss: 0.1023
2025-12-31 15:15:17 - INFO - PRINT: Epoch [ 934], Train Loss: 0.1039, Validation Loss: 0.1023
2025-12-31 15:15:44 - INFO - PRINT: Epoch [ 935], Train Loss: 0.1034, Validation Loss: 0.1023
2025-12-31 15:16:11 - INFO - PRINT: Epoch [ 936], Train Loss: 0.1043, Validation Loss: 0.1023
2025-12-31 15:16:37 - INFO - PRINT: Epoch [ 937], Train Loss: 0.1030, Validation Loss: 0.1023
2025-12-31 15:17:04 - INFO - PRINT: Epoch [ 938], Train Loss: 0.1033, Validation Loss: 0.1023
2025-12-31 15:17:31 - INFO - PRINT: Epoch [ 939], Train Loss: 0.1029, Validation Loss: 0.1023
2025-12-31 15:18:02 - INFO - PRINT: Epoch [ 940], Train Loss: 0.1033, Validation Loss: 0.1012
2025-12-31 15:18:28 - INFO - PRINT: Epoch [ 941], Train Loss: 0.1030, Validation Loss: 0.1012
2025-12-31 15:18:55 - INFO - PRINT: Epoch [ 942], Train Loss: 0.1022, Validation Loss: 0.1012
2025-12-31 15:19:22 - INFO - PRINT: Epoch [ 943], Train Loss: 0.1025, Validation Loss: 0.1012
2025-12-31 15:19:49 - INFO - PRINT: Epoch [ 944], Train Loss: 0.1027, Validation Loss: 0.1012
2025-12-31 15:20:15 - INFO - PRINT: Epoch [ 945], Train Loss: 0.1033, Validation Loss: 0.1012
2025-12-31 15:20:42 - INFO - PRINT: Epoch [ 946], Train Loss: 0.1034, Validation Loss: 0.1012
2025-12-31 15:21:09 - INFO - PRINT: Epoch [ 947], Train Loss: 0.1027, Validation Loss: 0.1012
2025-12-31 15:21:36 - INFO - PRINT: Epoch [ 948], Train Loss: 0.1031, Validation Loss: 0.1012
2025-12-31 15:22:02 - INFO - PRINT: Epoch [ 949], Train Loss: 0.1049, Validation Loss: 0.1012
2025-12-31 15:22:29 - INFO - PRINT: Epoch [ 950], Train Loss: 0.1030, Validation Loss: 0.1012
2025-12-31 15:22:56 - INFO - PRINT: Epoch [ 951], Train Loss: 0.1040, Validation Loss: 0.1012
2025-12-31 15:23:23 - INFO - PRINT: Epoch [ 952], Train Loss: 0.1023, Validation Loss: 0.1012
2025-12-31 15:23:49 - INFO - PRINT: Epoch [ 953], Train Loss: 0.1030, Validation Loss: 0.1012
2025-12-31 15:24:16 - INFO - PRINT: Epoch [ 954], Train Loss: 0.1033, Validation Loss: 0.1012
2025-12-31 15:24:43 - INFO - PRINT: Epoch [ 955], Train Loss: 0.1027, Validation Loss: 0.1012
2025-12-31 15:25:10 - INFO - PRINT: Epoch [ 956], Train Loss: 0.1035, Validation Loss: 0.1012
2025-12-31 15:25:36 - INFO - PRINT: Epoch [ 957], Train Loss: 0.1034, Validation Loss: 0.1012
2025-12-31 15:26:03 - INFO - PRINT: Epoch [ 958], Train Loss: 0.1034, Validation Loss: 0.1012
2025-12-31 15:26:30 - INFO - PRINT: Epoch [ 959], Train Loss: 0.1034, Validation Loss: 0.1012
2025-12-31 15:27:01 - INFO - PRINT: Epoch [ 960], Train Loss: 0.1031, Validation Loss: 0.1008
2025-12-31 15:27:27 - INFO - PRINT: Epoch [ 961], Train Loss: 0.1052, Validation Loss: 0.1008
2025-12-31 15:27:54 - INFO - PRINT: Epoch [ 962], Train Loss: 0.1027, Validation Loss: 0.1008
2025-12-31 15:28:21 - INFO - PRINT: Epoch [ 963], Train Loss: 0.1028, Validation Loss: 0.1008
2025-12-31 15:28:48 - INFO - PRINT: Epoch [ 964], Train Loss: 0.1038, Validation Loss: 0.1008
2025-12-31 15:29:14 - INFO - PRINT: Epoch [ 965], Train Loss: 0.1028, Validation Loss: 0.1008
2025-12-31 15:29:41 - INFO - PRINT: Epoch [ 966], Train Loss: 0.1030, Validation Loss: 0.1008
2025-12-31 15:30:08 - INFO - PRINT: Epoch [ 967], Train Loss: 0.1028, Validation Loss: 0.1008
2025-12-31 15:30:35 - INFO - PRINT: Epoch [ 968], Train Loss: 0.1021, Validation Loss: 0.1008
2025-12-31 15:31:01 - INFO - PRINT: Epoch [ 969], Train Loss: 0.1028, Validation Loss: 0.1008
2025-12-31 15:31:28 - INFO - PRINT: Epoch [ 970], Train Loss: 0.1034, Validation Loss: 0.1008
2025-12-31 15:31:55 - INFO - PRINT: Epoch [ 971], Train Loss: 0.1020, Validation Loss: 0.1008
2025-12-31 15:32:22 - INFO - PRINT: Epoch [ 972], Train Loss: 0.1028, Validation Loss: 0.1008
2025-12-31 15:32:48 - INFO - PRINT: Epoch [ 973], Train Loss: 0.1026, Validation Loss: 0.1008
2025-12-31 15:33:15 - INFO - PRINT: Epoch [ 974], Train Loss: 0.1021, Validation Loss: 0.1008
2025-12-31 15:33:42 - INFO - PRINT: Epoch [ 975], Train Loss: 0.1031, Validation Loss: 0.1008
2025-12-31 15:34:09 - INFO - PRINT: Epoch [ 976], Train Loss: 0.1033, Validation Loss: 0.1008
2025-12-31 15:34:35 - INFO - PRINT: Epoch [ 977], Train Loss: 0.1043, Validation Loss: 0.1008
2025-12-31 15:35:02 - INFO - PRINT: Epoch [ 978], Train Loss: 0.1021, Validation Loss: 0.1008
2025-12-31 15:35:29 - INFO - PRINT: Epoch [ 979], Train Loss: 0.1026, Validation Loss: 0.1008
2025-12-31 15:36:00 - INFO - PRINT: Epoch [ 980], Train Loss: 0.1024, Validation Loss: 0.1014
2025-12-31 15:36:26 - INFO - PRINT: Epoch [ 981], Train Loss: 0.1025, Validation Loss: 0.1014
2025-12-31 15:36:53 - INFO - PRINT: Epoch [ 982], Train Loss: 0.1036, Validation Loss: 0.1014
2025-12-31 15:37:20 - INFO - PRINT: Epoch [ 983], Train Loss: 0.1030, Validation Loss: 0.1014
2025-12-31 15:37:47 - INFO - PRINT: Epoch [ 984], Train Loss: 0.1026, Validation Loss: 0.1014
2025-12-31 15:38:13 - INFO - PRINT: Epoch [ 985], Train Loss: 0.1037, Validation Loss: 0.1014
2025-12-31 15:38:40 - INFO - PRINT: Epoch [ 986], Train Loss: 0.1026, Validation Loss: 0.1014
2025-12-31 15:39:07 - INFO - PRINT: Epoch [ 987], Train Loss: 0.1025, Validation Loss: 0.1014
2025-12-31 15:39:33 - INFO - PRINT: Epoch [ 988], Train Loss: 0.1031, Validation Loss: 0.1014
2025-12-31 15:40:00 - INFO - PRINT: Epoch [ 989], Train Loss: 0.1019, Validation Loss: 0.1014
2025-12-31 15:40:27 - INFO - PRINT: Epoch [ 990], Train Loss: 0.1032, Validation Loss: 0.1014
2025-12-31 15:40:54 - INFO - PRINT: Epoch [ 991], Train Loss: 0.1034, Validation Loss: 0.1014
2025-12-31 15:41:20 - INFO - PRINT: Epoch [ 992], Train Loss: 0.1043, Validation Loss: 0.1014
2025-12-31 15:41:47 - INFO - PRINT: Epoch [ 993], Train Loss: 0.1038, Validation Loss: 0.1014
2025-12-31 15:42:14 - INFO - PRINT: Epoch [ 994], Train Loss: 0.1032, Validation Loss: 0.1014
2025-12-31 15:42:41 - INFO - PRINT: Epoch [ 995], Train Loss: 0.1022, Validation Loss: 0.1014
2025-12-31 15:43:07 - INFO - PRINT: Epoch [ 996], Train Loss: 0.1024, Validation Loss: 0.1014
2025-12-31 15:43:34 - INFO - PRINT: Epoch [ 997], Train Loss: 0.1026, Validation Loss: 0.1014
2025-12-31 15:44:01 - INFO - PRINT: Epoch [ 998], Train Loss: 0.1032, Validation Loss: 0.1014
2025-12-31 15:44:28 - INFO - PRINT: Epoch [ 999], Train Loss: 0.1030, Validation Loss: 0.1014
2025-12-31 15:44:59 - INFO - PRINT: Epoch [1000], Train Loss: 0.1027, Validation Loss: 0.1004
2025-12-31 15:44:59 - INFO - PRINT: ----> Saving model from epoch 1000 (val loss: 0.10039761647582054). Elegant!
2025-12-31 15:45:25 - INFO - PRINT: Epoch [1001], Train Loss: 0.1023, Validation Loss: 0.1004
2025-12-31 15:45:52 - INFO - PRINT: Epoch [1002], Train Loss: 0.1025, Validation Loss: 0.1004
2025-12-31 15:46:19 - INFO - PRINT: Epoch [1003], Train Loss: 0.1029, Validation Loss: 0.1004
2025-12-31 15:46:46 - INFO - PRINT: Epoch [1004], Train Loss: 0.1027, Validation Loss: 0.1004
2025-12-31 15:47:12 - INFO - PRINT: Epoch [1005], Train Loss: 0.1022, Validation Loss: 0.1004
2025-12-31 15:47:39 - INFO - PRINT: Epoch [1006], Train Loss: 0.1035, Validation Loss: 0.1004
2025-12-31 15:48:06 - INFO - PRINT: Epoch [1007], Train Loss: 0.1038, Validation Loss: 0.1004
2025-12-31 15:48:33 - INFO - PRINT: Epoch [1008], Train Loss: 0.1028, Validation Loss: 0.1004
2025-12-31 15:48:59 - INFO - PRINT: Epoch [1009], Train Loss: 0.1025, Validation Loss: 0.1004
2025-12-31 15:49:26 - INFO - PRINT: Epoch [1010], Train Loss: 0.1022, Validation Loss: 0.1004
2025-12-31 15:49:53 - INFO - PRINT: Epoch [1011], Train Loss: 0.1022, Validation Loss: 0.1004
2025-12-31 15:50:20 - INFO - PRINT: Epoch [1012], Train Loss: 0.1033, Validation Loss: 0.1004
2025-12-31 15:50:46 - INFO - PRINT: Epoch [1013], Train Loss: 0.1025, Validation Loss: 0.1004
2025-12-31 15:51:13 - INFO - PRINT: Epoch [1014], Train Loss: 0.1027, Validation Loss: 0.1004
2025-12-31 15:51:40 - INFO - PRINT: Epoch [1015], Train Loss: 0.1032, Validation Loss: 0.1004
2025-12-31 15:52:07 - INFO - PRINT: Epoch [1016], Train Loss: 0.1048, Validation Loss: 0.1004
2025-12-31 15:52:33 - INFO - PRINT: Epoch [1017], Train Loss: 0.1031, Validation Loss: 0.1004
2025-12-31 15:53:00 - INFO - PRINT: Epoch [1018], Train Loss: 0.1024, Validation Loss: 0.1004
2025-12-31 15:53:27 - INFO - PRINT: Epoch [1019], Train Loss: 0.1031, Validation Loss: 0.1004
2025-12-31 15:53:58 - INFO - PRINT: Epoch [1020], Train Loss: 0.1027, Validation Loss: 0.1024
2025-12-31 15:54:24 - INFO - PRINT: Epoch [1021], Train Loss: 0.1028, Validation Loss: 0.1024
2025-12-31 15:54:51 - INFO - PRINT: Epoch [1022], Train Loss: 0.1023, Validation Loss: 0.1024
2025-12-31 15:55:18 - INFO - PRINT: Epoch [1023], Train Loss: 0.1031, Validation Loss: 0.1024
2025-12-31 15:55:45 - INFO - PRINT: Epoch [1024], Train Loss: 0.1036, Validation Loss: 0.1024
2025-12-31 15:56:11 - INFO - PRINT: Epoch [1025], Train Loss: 0.1039, Validation Loss: 0.1024
2025-12-31 15:56:38 - INFO - PRINT: Epoch [1026], Train Loss: 0.1023, Validation Loss: 0.1024
2025-12-31 15:57:05 - INFO - PRINT: Epoch [1027], Train Loss: 0.1026, Validation Loss: 0.1024
2025-12-31 15:57:31 - INFO - PRINT: Epoch [1028], Train Loss: 0.1021, Validation Loss: 0.1024
2025-12-31 15:57:58 - INFO - PRINT: Epoch [1029], Train Loss: 0.1026, Validation Loss: 0.1024
2025-12-31 15:58:25 - INFO - PRINT: Epoch [1030], Train Loss: 0.1022, Validation Loss: 0.1024
2025-12-31 15:58:52 - INFO - PRINT: Epoch [1031], Train Loss: 0.1035, Validation Loss: 0.1024
2025-12-31 15:59:18 - INFO - PRINT: Epoch [1032], Train Loss: 0.1034, Validation Loss: 0.1024
2025-12-31 15:59:45 - INFO - PRINT: Epoch [1033], Train Loss: 0.1031, Validation Loss: 0.1024
2025-12-31 16:00:12 - INFO - PRINT: Epoch [1034], Train Loss: 0.1023, Validation Loss: 0.1024
2025-12-31 16:00:39 - INFO - PRINT: Epoch [1035], Train Loss: 0.1025, Validation Loss: 0.1024
2025-12-31 16:01:05 - INFO - PRINT: Epoch [1036], Train Loss: 0.1023, Validation Loss: 0.1024
2025-12-31 16:01:32 - INFO - PRINT: Epoch [1037], Train Loss: 0.1021, Validation Loss: 0.1024
2025-12-31 16:01:59 - INFO - PRINT: Epoch [1038], Train Loss: 0.1024, Validation Loss: 0.1024
2025-12-31 16:02:26 - INFO - PRINT: Epoch [1039], Train Loss: 0.1031, Validation Loss: 0.1024
2025-12-31 16:02:57 - INFO - PRINT: Epoch [1040], Train Loss: 0.1029, Validation Loss: 0.1001
2025-12-31 16:03:23 - INFO - PRINT: Epoch [1041], Train Loss: 0.1021, Validation Loss: 0.1001
2025-12-31 16:03:50 - INFO - PRINT: Epoch [1042], Train Loss: 0.1040, Validation Loss: 0.1001
2025-12-31 16:04:16 - INFO - PRINT: Epoch [1043], Train Loss: 0.1029, Validation Loss: 0.1001
2025-12-31 16:04:43 - INFO - PRINT: Epoch [1044], Train Loss: 0.1028, Validation Loss: 0.1001
2025-12-31 16:05:10 - INFO - PRINT: Epoch [1045], Train Loss: 0.1022, Validation Loss: 0.1001
2025-12-31 16:05:37 - INFO - PRINT: Epoch [1046], Train Loss: 0.1025, Validation Loss: 0.1001
2025-12-31 16:06:03 - INFO - PRINT: Epoch [1047], Train Loss: 0.1030, Validation Loss: 0.1001
2025-12-31 16:06:30 - INFO - PRINT: Epoch [1048], Train Loss: 0.1031, Validation Loss: 0.1001
2025-12-31 16:06:57 - INFO - PRINT: Epoch [1049], Train Loss: 0.1040, Validation Loss: 0.1001
2025-12-31 16:07:24 - INFO - PRINT: Epoch [1050], Train Loss: 0.1019, Validation Loss: 0.1001
2025-12-31 16:07:50 - INFO - PRINT: Epoch [1051], Train Loss: 0.1016, Validation Loss: 0.1001
2025-12-31 16:08:17 - INFO - PRINT: Epoch [1052], Train Loss: 0.1030, Validation Loss: 0.1001
2025-12-31 16:08:44 - INFO - PRINT: Epoch [1053], Train Loss: 0.1031, Validation Loss: 0.1001
2025-12-31 16:09:11 - INFO - PRINT: Epoch [1054], Train Loss: 0.1025, Validation Loss: 0.1001
2025-12-31 16:09:37 - INFO - PRINT: Epoch [1055], Train Loss: 0.1024, Validation Loss: 0.1001
2025-12-31 16:10:04 - INFO - PRINT: Epoch [1056], Train Loss: 0.1022, Validation Loss: 0.1001
2025-12-31 16:10:31 - INFO - PRINT: Epoch [1057], Train Loss: 0.1034, Validation Loss: 0.1001
2025-12-31 16:10:58 - INFO - PRINT: Epoch [1058], Train Loss: 0.1024, Validation Loss: 0.1001
2025-12-31 16:11:24 - INFO - PRINT: Epoch [1059], Train Loss: 0.1027, Validation Loss: 0.1001
2025-12-31 16:11:55 - INFO - PRINT: Epoch [1060], Train Loss: 0.1025, Validation Loss: 0.1010
2025-12-31 16:12:22 - INFO - PRINT: Epoch [1061], Train Loss: 0.1023, Validation Loss: 0.1010
2025-12-31 16:12:49 - INFO - PRINT: Epoch [1062], Train Loss: 0.1026, Validation Loss: 0.1010
2025-12-31 16:13:15 - INFO - PRINT: Epoch [1063], Train Loss: 0.1026, Validation Loss: 0.1010
2025-12-31 16:13:42 - INFO - PRINT: Epoch [1064], Train Loss: 0.1035, Validation Loss: 0.1010
2025-12-31 16:14:09 - INFO - PRINT: Epoch [1065], Train Loss: 0.1023, Validation Loss: 0.1010
2025-12-31 16:14:36 - INFO - PRINT: Epoch [1066], Train Loss: 0.1025, Validation Loss: 0.1010
2025-12-31 16:15:02 - INFO - PRINT: Epoch [1067], Train Loss: 0.1022, Validation Loss: 0.1010
2025-12-31 16:15:29 - INFO - PRINT: Epoch [1068], Train Loss: 0.1033, Validation Loss: 0.1010
2025-12-31 16:15:56 - INFO - PRINT: Epoch [1069], Train Loss: 0.1031, Validation Loss: 0.1010
2025-12-31 16:16:23 - INFO - PRINT: Epoch [1070], Train Loss: 0.1031, Validation Loss: 0.1010
2025-12-31 16:16:49 - INFO - PRINT: Epoch [1071], Train Loss: 0.1023, Validation Loss: 0.1010
2025-12-31 16:17:16 - INFO - PRINT: Epoch [1072], Train Loss: 0.1039, Validation Loss: 0.1010
2025-12-31 16:17:43 - INFO - PRINT: Epoch [1073], Train Loss: 0.1019, Validation Loss: 0.1010
2025-12-31 16:18:10 - INFO - PRINT: Epoch [1074], Train Loss: 0.1021, Validation Loss: 0.1010
2025-12-31 16:18:36 - INFO - PRINT: Epoch [1075], Train Loss: 0.1022, Validation Loss: 0.1010
2025-12-31 16:19:03 - INFO - PRINT: Epoch [1076], Train Loss: 0.1027, Validation Loss: 0.1010
2025-12-31 16:19:30 - INFO - PRINT: Epoch [1077], Train Loss: 0.1027, Validation Loss: 0.1010
2025-12-31 16:19:57 - INFO - PRINT: Epoch [1078], Train Loss: 0.1029, Validation Loss: 0.1010
2025-12-31 16:20:23 - INFO - PRINT: Epoch [1079], Train Loss: 0.1025, Validation Loss: 0.1010
2025-12-31 16:20:54 - INFO - PRINT: Epoch [1080], Train Loss: 0.1043, Validation Loss: 0.1014
2025-12-31 16:21:21 - INFO - PRINT: Epoch [1081], Train Loss: 0.1025, Validation Loss: 0.1014
2025-12-31 16:21:48 - INFO - PRINT: Epoch [1082], Train Loss: 0.1021, Validation Loss: 0.1014
2025-12-31 16:22:14 - INFO - PRINT: Epoch [1083], Train Loss: 0.1022, Validation Loss: 0.1014
2025-12-31 16:22:41 - INFO - PRINT: Epoch [1084], Train Loss: 0.1021, Validation Loss: 0.1014
2025-12-31 16:23:08 - INFO - PRINT: Epoch [1085], Train Loss: 0.1025, Validation Loss: 0.1014
2025-12-31 16:23:35 - INFO - PRINT: Epoch [1086], Train Loss: 0.1023, Validation Loss: 0.1014
2025-12-31 16:24:01 - INFO - PRINT: Epoch [1087], Train Loss: 0.1044, Validation Loss: 0.1014
2025-12-31 16:24:28 - INFO - PRINT: Epoch [1088], Train Loss: 0.1030, Validation Loss: 0.1014
2025-12-31 16:24:55 - INFO - PRINT: Epoch [1089], Train Loss: 0.1023, Validation Loss: 0.1014
2025-12-31 16:25:22 - INFO - PRINT: Epoch [1090], Train Loss: 0.1032, Validation Loss: 0.1014
2025-12-31 16:25:48 - INFO - PRINT: Epoch [1091], Train Loss: 0.1033, Validation Loss: 0.1014
2025-12-31 16:26:15 - INFO - PRINT: Epoch [1092], Train Loss: 0.1027, Validation Loss: 0.1014
2025-12-31 16:26:42 - INFO - PRINT: Epoch [1093], Train Loss: 0.1016, Validation Loss: 0.1014
2025-12-31 16:27:09 - INFO - PRINT: Epoch [1094], Train Loss: 0.1035, Validation Loss: 0.1014
2025-12-31 16:27:35 - INFO - PRINT: Epoch [1095], Train Loss: 0.1028, Validation Loss: 0.1014
2025-12-31 16:28:02 - INFO - PRINT: Epoch [1096], Train Loss: 0.1018, Validation Loss: 0.1014
2025-12-31 16:28:29 - INFO - PRINT: Epoch [1097], Train Loss: 0.1027, Validation Loss: 0.1014
2025-12-31 16:28:56 - INFO - PRINT: Epoch [1098], Train Loss: 0.1040, Validation Loss: 0.1014
2025-12-31 16:29:22 - INFO - PRINT: Epoch [1099], Train Loss: 0.1042, Validation Loss: 0.1014
2025-12-31 16:29:53 - INFO - PRINT: Epoch [1100], Train Loss: 0.1024, Validation Loss: 0.0996
2025-12-31 16:29:53 - INFO - PRINT: ----> Saving model from epoch 1100 (val loss: 0.09962821692228317). Lip-smacking!
2025-12-31 16:30:20 - INFO - PRINT: Epoch [1101], Train Loss: 0.1021, Validation Loss: 0.0996
2025-12-31 16:30:47 - INFO - PRINT: Epoch [1102], Train Loss: 0.1021, Validation Loss: 0.0996
2025-12-31 16:31:13 - INFO - PRINT: Epoch [1103], Train Loss: 0.1021, Validation Loss: 0.0996
2025-12-31 16:31:40 - INFO - PRINT: Epoch [1104], Train Loss: 0.1027, Validation Loss: 0.0996
2025-12-31 16:32:07 - INFO - PRINT: Epoch [1105], Train Loss: 0.1022, Validation Loss: 0.0996
2025-12-31 16:32:34 - INFO - PRINT: Epoch [1106], Train Loss: 0.1017, Validation Loss: 0.0996
2025-12-31 16:33:00 - INFO - PRINT: Epoch [1107], Train Loss: 0.1027, Validation Loss: 0.0996
2025-12-31 16:33:27 - INFO - PRINT: Epoch [1108], Train Loss: 0.1034, Validation Loss: 0.0996
2025-12-31 16:33:54 - INFO - PRINT: Epoch [1109], Train Loss: 0.1027, Validation Loss: 0.0996
2025-12-31 16:34:21 - INFO - PRINT: Epoch [1110], Train Loss: 0.1026, Validation Loss: 0.0996
2025-12-31 16:34:47 - INFO - PRINT: Epoch [1111], Train Loss: 0.1023, Validation Loss: 0.0996
2025-12-31 16:35:14 - INFO - PRINT: Epoch [1112], Train Loss: 0.1019, Validation Loss: 0.0996
2025-12-31 16:35:41 - INFO - PRINT: Epoch [1113], Train Loss: 0.1018, Validation Loss: 0.0996
2025-12-31 16:36:08 - INFO - PRINT: Epoch [1114], Train Loss: 0.1023, Validation Loss: 0.0996
2025-12-31 16:36:34 - INFO - PRINT: Epoch [1115], Train Loss: 0.1033, Validation Loss: 0.0996
2025-12-31 16:37:01 - INFO - PRINT: Epoch [1116], Train Loss: 0.1026, Validation Loss: 0.0996
2025-12-31 16:37:28 - INFO - PRINT: Epoch [1117], Train Loss: 0.1022, Validation Loss: 0.0996
2025-12-31 16:37:55 - INFO - PRINT: Epoch [1118], Train Loss: 0.1045, Validation Loss: 0.0996
2025-12-31 16:38:21 - INFO - PRINT: Epoch [1119], Train Loss: 0.1016, Validation Loss: 0.0996
2025-12-31 16:38:53 - INFO - PRINT: Epoch [1120], Train Loss: 0.1020, Validation Loss: 0.0996
2025-12-31 16:39:19 - INFO - PRINT: Epoch [1121], Train Loss: 0.1027, Validation Loss: 0.0996
2025-12-31 16:39:46 - INFO - PRINT: Epoch [1122], Train Loss: 0.1022, Validation Loss: 0.0996
2025-12-31 16:40:13 - INFO - PRINT: Epoch [1123], Train Loss: 0.1034, Validation Loss: 0.0996
2025-12-31 16:40:39 - INFO - PRINT: Epoch [1124], Train Loss: 0.1037, Validation Loss: 0.0996
2025-12-31 16:41:06 - INFO - PRINT: Epoch [1125], Train Loss: 0.1017, Validation Loss: 0.0996
2025-12-31 16:41:33 - INFO - PRINT: Epoch [1126], Train Loss: 0.1016, Validation Loss: 0.0996
2025-12-31 16:42:00 - INFO - PRINT: Epoch [1127], Train Loss: 0.1020, Validation Loss: 0.0996
2025-12-31 16:42:26 - INFO - PRINT: Epoch [1128], Train Loss: 0.1029, Validation Loss: 0.0996
2025-12-31 16:42:53 - INFO - PRINT: Epoch [1129], Train Loss: 0.1020, Validation Loss: 0.0996
2025-12-31 16:43:20 - INFO - PRINT: Epoch [1130], Train Loss: 0.1021, Validation Loss: 0.0996
2025-12-31 16:43:47 - INFO - PRINT: Epoch [1131], Train Loss: 0.1028, Validation Loss: 0.0996
2025-12-31 16:44:13 - INFO - PRINT: Epoch [1132], Train Loss: 0.1022, Validation Loss: 0.0996
2025-12-31 16:44:40 - INFO - PRINT: Epoch [1133], Train Loss: 0.1022, Validation Loss: 0.0996
2025-12-31 16:45:07 - INFO - PRINT: Epoch [1134], Train Loss: 0.1023, Validation Loss: 0.0996
2025-12-31 16:45:34 - INFO - PRINT: Epoch [1135], Train Loss: 0.1025, Validation Loss: 0.0996
2025-12-31 16:46:00 - INFO - PRINT: Epoch [1136], Train Loss: 0.1025, Validation Loss: 0.0996
2025-12-31 16:46:27 - INFO - PRINT: Epoch [1137], Train Loss: 0.1020, Validation Loss: 0.0996
2025-12-31 16:46:54 - INFO - PRINT: Epoch [1138], Train Loss: 0.1016, Validation Loss: 0.0996
2025-12-31 16:47:21 - INFO - PRINT: Epoch [1139], Train Loss: 0.1023, Validation Loss: 0.0996
2025-12-31 16:47:52 - INFO - PRINT: Epoch [1140], Train Loss: 0.1030, Validation Loss: 0.1006
2025-12-31 16:48:18 - INFO - PRINT: Epoch [1141], Train Loss: 0.1024, Validation Loss: 0.1006
2025-12-31 16:48:45 - INFO - PRINT: Epoch [1142], Train Loss: 0.1028, Validation Loss: 0.1006
2025-12-31 16:49:12 - INFO - PRINT: Epoch [1143], Train Loss: 0.1021, Validation Loss: 0.1006
2025-12-31 16:49:38 - INFO - PRINT: Epoch [1144], Train Loss: 0.1027, Validation Loss: 0.1006
2025-12-31 16:50:05 - INFO - PRINT: Epoch [1145], Train Loss: 0.1015, Validation Loss: 0.1006
2025-12-31 16:50:32 - INFO - PRINT: Epoch [1146], Train Loss: 0.1023, Validation Loss: 0.1006
2025-12-31 16:50:59 - INFO - PRINT: Epoch [1147], Train Loss: 0.1025, Validation Loss: 0.1006
2025-12-31 16:51:25 - INFO - PRINT: Epoch [1148], Train Loss: 0.1025, Validation Loss: 0.1006
2025-12-31 16:51:52 - INFO - PRINT: Epoch [1149], Train Loss: 0.1019, Validation Loss: 0.1006
2025-12-31 16:52:19 - INFO - PRINT: Epoch [1150], Train Loss: 0.1020, Validation Loss: 0.1006
2025-12-31 16:52:46 - INFO - PRINT: Epoch [1151], Train Loss: 0.1024, Validation Loss: 0.1006
2025-12-31 16:53:12 - INFO - PRINT: Epoch [1152], Train Loss: 0.1024, Validation Loss: 0.1006
2025-12-31 16:53:39 - INFO - PRINT: Epoch [1153], Train Loss: 0.1021, Validation Loss: 0.1006
2025-12-31 16:54:06 - INFO - PRINT: Epoch [1154], Train Loss: 0.1038, Validation Loss: 0.1006
2025-12-31 16:54:33 - INFO - PRINT: Epoch [1155], Train Loss: 0.1030, Validation Loss: 0.1006
2025-12-31 16:54:59 - INFO - PRINT: Epoch [1156], Train Loss: 0.1028, Validation Loss: 0.1006
2025-12-31 16:55:26 - INFO - PRINT: Epoch [1157], Train Loss: 0.1024, Validation Loss: 0.1006
2025-12-31 16:55:53 - INFO - PRINT: Epoch [1158], Train Loss: 0.1022, Validation Loss: 0.1006
2025-12-31 16:56:20 - INFO - PRINT: Epoch [1159], Train Loss: 0.1022, Validation Loss: 0.1006
2025-12-31 16:56:51 - INFO - PRINT: Epoch [1160], Train Loss: 0.1019, Validation Loss: 0.1002
2025-12-31 16:57:17 - INFO - PRINT: Epoch [1161], Train Loss: 0.1023, Validation Loss: 0.1002
2025-12-31 16:57:44 - INFO - PRINT: Epoch [1162], Train Loss: 0.1021, Validation Loss: 0.1002
2025-12-31 16:58:11 - INFO - PRINT: Epoch [1163], Train Loss: 0.1017, Validation Loss: 0.1002
2025-12-31 16:58:37 - INFO - PRINT: Epoch [1164], Train Loss: 0.1019, Validation Loss: 0.1002
2025-12-31 16:59:04 - INFO - PRINT: Epoch [1165], Train Loss: 0.1023, Validation Loss: 0.1002
2025-12-31 16:59:31 - INFO - PRINT: Epoch [1166], Train Loss: 0.1024, Validation Loss: 0.1002
2025-12-31 16:59:58 - INFO - PRINT: Epoch [1167], Train Loss: 0.1026, Validation Loss: 0.1002
2025-12-31 17:00:24 - INFO - PRINT: Epoch [1168], Train Loss: 0.1036, Validation Loss: 0.1002
2025-12-31 17:00:51 - INFO - PRINT: Epoch [1169], Train Loss: 0.1027, Validation Loss: 0.1002
2025-12-31 17:01:18 - INFO - PRINT: Epoch [1170], Train Loss: 0.1019, Validation Loss: 0.1002
2025-12-31 17:01:44 - INFO - PRINT: Epoch [1171], Train Loss: 0.1031, Validation Loss: 0.1002
2025-12-31 17:02:11 - INFO - PRINT: Epoch [1172], Train Loss: 0.1022, Validation Loss: 0.1002
2025-12-31 17:02:38 - INFO - PRINT: Epoch [1173], Train Loss: 0.1036, Validation Loss: 0.1002
2025-12-31 17:03:05 - INFO - PRINT: Epoch [1174], Train Loss: 0.1023, Validation Loss: 0.1002
2025-12-31 17:03:31 - INFO - PRINT: Epoch [1175], Train Loss: 0.1019, Validation Loss: 0.1002
2025-12-31 17:03:58 - INFO - PRINT: Epoch [1176], Train Loss: 0.1022, Validation Loss: 0.1002
2025-12-31 17:04:25 - INFO - PRINT: Epoch [1177], Train Loss: 0.1024, Validation Loss: 0.1002
2025-12-31 17:04:52 - INFO - PRINT: Epoch [1178], Train Loss: 0.1020, Validation Loss: 0.1002
2025-12-31 17:05:19 - INFO - PRINT: Epoch [1179], Train Loss: 0.1019, Validation Loss: 0.1002
2025-12-31 17:05:51 - INFO - PRINT: Epoch [1180], Train Loss: 0.1025, Validation Loss: 0.1012
2025-12-31 17:06:17 - INFO - PRINT: Epoch [1181], Train Loss: 0.1022, Validation Loss: 0.1012
2025-12-31 17:06:44 - INFO - PRINT: Epoch [1182], Train Loss: 0.1024, Validation Loss: 0.1012
2025-12-31 17:07:11 - INFO - PRINT: Epoch [1183], Train Loss: 0.1021, Validation Loss: 0.1012
2025-12-31 17:07:37 - INFO - PRINT: Epoch [1184], Train Loss: 0.1016, Validation Loss: 0.1012
2025-12-31 17:08:04 - INFO - PRINT: Epoch [1185], Train Loss: 0.1029, Validation Loss: 0.1012
2025-12-31 17:08:31 - INFO - PRINT: Epoch [1186], Train Loss: 0.1015, Validation Loss: 0.1012
2025-12-31 17:08:58 - INFO - PRINT: Epoch [1187], Train Loss: 0.1023, Validation Loss: 0.1012
2025-12-31 17:09:24 - INFO - PRINT: Epoch [1188], Train Loss: 0.1031, Validation Loss: 0.1012
2025-12-31 17:09:51 - INFO - PRINT: Epoch [1189], Train Loss: 0.1015, Validation Loss: 0.1012
2025-12-31 17:10:18 - INFO - PRINT: Epoch [1190], Train Loss: 0.1016, Validation Loss: 0.1012
2025-12-31 17:10:45 - INFO - PRINT: Epoch [1191], Train Loss: 0.1021, Validation Loss: 0.1012
2025-12-31 17:11:11 - INFO - PRINT: Epoch [1192], Train Loss: 0.1017, Validation Loss: 0.1012
2025-12-31 17:11:38 - INFO - PRINT: Epoch [1193], Train Loss: 0.1020, Validation Loss: 0.1012
2025-12-31 17:12:05 - INFO - PRINT: Epoch [1194], Train Loss: 0.1022, Validation Loss: 0.1012
2025-12-31 17:12:32 - INFO - PRINT: Epoch [1195], Train Loss: 0.1028, Validation Loss: 0.1012
2025-12-31 17:12:58 - INFO - PRINT: Epoch [1196], Train Loss: 0.1020, Validation Loss: 0.1012
2025-12-31 17:13:25 - INFO - PRINT: Epoch [1197], Train Loss: 0.1016, Validation Loss: 0.1012
2025-12-31 17:13:52 - INFO - PRINT: Epoch [1198], Train Loss: 0.1039, Validation Loss: 0.1012
2025-12-31 17:14:18 - INFO - PRINT: Epoch [1199], Train Loss: 0.1017, Validation Loss: 0.1012
2025-12-31 17:14:50 - INFO - PRINT: Epoch [1200], Train Loss: 0.1045, Validation Loss: 0.1004
2025-12-31 17:14:50 - INFO - PRINT: ----> Saving model from epoch 1120 (val loss: 0.09955967620015144). Juicy!
2025-12-31 17:15:16 - INFO - PRINT: Epoch [1201], Train Loss: 0.1032, Validation Loss: 0.1004
2025-12-31 17:15:43 - INFO - PRINT: Epoch [1202], Train Loss: 0.1020, Validation Loss: 0.1004
2025-12-31 17:16:09 - INFO - PRINT: Epoch [1203], Train Loss: 0.1015, Validation Loss: 0.1004
2025-12-31 17:16:36 - INFO - PRINT: Epoch [1204], Train Loss: 0.1027, Validation Loss: 0.1004
2025-12-31 17:17:03 - INFO - PRINT: Epoch [1205], Train Loss: 0.1023, Validation Loss: 0.1004
2025-12-31 17:17:30 - INFO - PRINT: Epoch [1206], Train Loss: 0.1012, Validation Loss: 0.1004
2025-12-31 17:17:56 - INFO - PRINT: Epoch [1207], Train Loss: 0.1022, Validation Loss: 0.1004
2025-12-31 17:18:23 - INFO - PRINT: Epoch [1208], Train Loss: 0.1019, Validation Loss: 0.1004
2025-12-31 17:18:50 - INFO - PRINT: Epoch [1209], Train Loss: 0.1027, Validation Loss: 0.1004
2025-12-31 17:19:17 - INFO - PRINT: Epoch [1210], Train Loss: 0.1020, Validation Loss: 0.1004
2025-12-31 17:19:43 - INFO - PRINT: Epoch [1211], Train Loss: 0.1022, Validation Loss: 0.1004
2025-12-31 17:20:10 - INFO - PRINT: Epoch [1212], Train Loss: 0.1017, Validation Loss: 0.1004
2025-12-31 17:20:37 - INFO - PRINT: Epoch [1213], Train Loss: 0.1024, Validation Loss: 0.1004
2025-12-31 17:21:04 - INFO - PRINT: Epoch [1214], Train Loss: 0.1018, Validation Loss: 0.1004
2025-12-31 17:21:30 - INFO - PRINT: Epoch [1215], Train Loss: 0.1020, Validation Loss: 0.1004
2025-12-31 17:21:57 - INFO - PRINT: Epoch [1216], Train Loss: 0.1016, Validation Loss: 0.1004
2025-12-31 17:22:24 - INFO - PRINT: Epoch [1217], Train Loss: 0.1017, Validation Loss: 0.1004
2025-12-31 17:22:51 - INFO - PRINT: Epoch [1218], Train Loss: 0.1024, Validation Loss: 0.1004
2025-12-31 17:23:17 - INFO - PRINT: Epoch [1219], Train Loss: 0.1064, Validation Loss: 0.1004
2025-12-31 17:23:49 - INFO - PRINT: Epoch [1220], Train Loss: 0.1027, Validation Loss: 0.1004
2025-12-31 17:24:15 - INFO - PRINT: Epoch [1221], Train Loss: 0.1019, Validation Loss: 0.1004
2025-12-31 17:24:42 - INFO - PRINT: Epoch [1222], Train Loss: 0.1019, Validation Loss: 0.1004
2025-12-31 17:25:09 - INFO - PRINT: Epoch [1223], Train Loss: 0.1027, Validation Loss: 0.1004
2025-12-31 17:25:35 - INFO - PRINT: Epoch [1224], Train Loss: 0.1026, Validation Loss: 0.1004
2025-12-31 17:26:02 - INFO - PRINT: Epoch [1225], Train Loss: 0.1020, Validation Loss: 0.1004
2025-12-31 17:26:29 - INFO - PRINT: Epoch [1226], Train Loss: 0.1024, Validation Loss: 0.1004
2025-12-31 17:26:55 - INFO - PRINT: Epoch [1227], Train Loss: 0.1023, Validation Loss: 0.1004
2025-12-31 17:27:22 - INFO - PRINT: Epoch [1228], Train Loss: 0.1031, Validation Loss: 0.1004
2025-12-31 17:27:49 - INFO - PRINT: Epoch [1229], Train Loss: 0.1028, Validation Loss: 0.1004
2025-12-31 17:28:16 - INFO - PRINT: Epoch [1230], Train Loss: 0.1022, Validation Loss: 0.1004
2025-12-31 17:28:42 - INFO - PRINT: Epoch [1231], Train Loss: 0.1025, Validation Loss: 0.1004
2025-12-31 17:29:09 - INFO - PRINT: Epoch [1232], Train Loss: 0.1012, Validation Loss: 0.1004
2025-12-31 17:29:36 - INFO - PRINT: Epoch [1233], Train Loss: 0.1020, Validation Loss: 0.1004
2025-12-31 17:30:03 - INFO - PRINT: Epoch [1234], Train Loss: 0.1030, Validation Loss: 0.1004
2025-12-31 17:30:29 - INFO - PRINT: Epoch [1235], Train Loss: 0.1022, Validation Loss: 0.1004
2025-12-31 17:30:56 - INFO - PRINT: Epoch [1236], Train Loss: 0.1017, Validation Loss: 0.1004
2025-12-31 17:31:23 - INFO - PRINT: Epoch [1237], Train Loss: 0.1032, Validation Loss: 0.1004
2025-12-31 17:31:50 - INFO - PRINT: Epoch [1238], Train Loss: 0.1029, Validation Loss: 0.1004
2025-12-31 17:32:16 - INFO - PRINT: Epoch [1239], Train Loss: 0.1021, Validation Loss: 0.1004
2025-12-31 17:32:48 - INFO - PRINT: Epoch [1240], Train Loss: 0.1023, Validation Loss: 0.1000
2025-12-31 17:33:14 - INFO - PRINT: Epoch [1241], Train Loss: 0.1021, Validation Loss: 0.1000
2025-12-31 17:33:41 - INFO - PRINT: Epoch [1242], Train Loss: 0.1025, Validation Loss: 0.1000
2025-12-31 17:34:07 - INFO - PRINT: Epoch [1243], Train Loss: 0.1026, Validation Loss: 0.1000
2025-12-31 17:34:34 - INFO - PRINT: Epoch [1244], Train Loss: 0.1014, Validation Loss: 0.1000
2025-12-31 17:35:01 - INFO - PRINT: Epoch [1245], Train Loss: 0.1019, Validation Loss: 0.1000
2025-12-31 17:35:28 - INFO - PRINT: Epoch [1246], Train Loss: 0.1034, Validation Loss: 0.1000
2025-12-31 17:35:54 - INFO - PRINT: Epoch [1247], Train Loss: 0.1018, Validation Loss: 0.1000
2025-12-31 17:36:21 - INFO - PRINT: Epoch [1248], Train Loss: 0.1013, Validation Loss: 0.1000
2025-12-31 17:36:48 - INFO - PRINT: Epoch [1249], Train Loss: 0.1028, Validation Loss: 0.1000
2025-12-31 17:37:15 - INFO - PRINT: Epoch [1250], Train Loss: 0.1015, Validation Loss: 0.1000
2025-12-31 17:37:41 - INFO - PRINT: Epoch [1251], Train Loss: 0.1018, Validation Loss: 0.1000
2025-12-31 17:38:08 - INFO - PRINT: Epoch [1252], Train Loss: 0.1018, Validation Loss: 0.1000
2025-12-31 17:38:35 - INFO - PRINT: Epoch [1253], Train Loss: 0.1020, Validation Loss: 0.1000
2025-12-31 17:39:02 - INFO - PRINT: Epoch [1254], Train Loss: 0.1022, Validation Loss: 0.1000
2025-12-31 17:39:28 - INFO - PRINT: Epoch [1255], Train Loss: 0.1030, Validation Loss: 0.1000
2025-12-31 17:39:55 - INFO - PRINT: Epoch [1256], Train Loss: 0.1024, Validation Loss: 0.1000
2025-12-31 17:40:22 - INFO - PRINT: Epoch [1257], Train Loss: 0.1017, Validation Loss: 0.1000
2025-12-31 17:40:49 - INFO - PRINT: Epoch [1258], Train Loss: 0.1027, Validation Loss: 0.1000
2025-12-31 17:41:15 - INFO - PRINT: Epoch [1259], Train Loss: 0.1020, Validation Loss: 0.1000
2025-12-31 17:41:47 - INFO - PRINT: Epoch [1260], Train Loss: 0.1017, Validation Loss: 0.0995
2025-12-31 17:42:13 - INFO - PRINT: Epoch [1261], Train Loss: 0.1015, Validation Loss: 0.0995
2025-12-31 17:42:40 - INFO - PRINT: Epoch [1262], Train Loss: 0.1018, Validation Loss: 0.0995
2025-12-31 17:43:06 - INFO - PRINT: Epoch [1263], Train Loss: 0.1044, Validation Loss: 0.0995
2025-12-31 17:43:33 - INFO - PRINT: Epoch [1264], Train Loss: 0.1035, Validation Loss: 0.0995
2025-12-31 17:44:00 - INFO - PRINT: Epoch [1265], Train Loss: 0.1025, Validation Loss: 0.0995
2025-12-31 17:44:27 - INFO - PRINT: Epoch [1266], Train Loss: 0.1016, Validation Loss: 0.0995
2025-12-31 17:44:53 - INFO - PRINT: Epoch [1267], Train Loss: 0.1013, Validation Loss: 0.0995
2025-12-31 17:45:20 - INFO - PRINT: Epoch [1268], Train Loss: 0.1022, Validation Loss: 0.0995
2025-12-31 17:45:47 - INFO - PRINT: Epoch [1269], Train Loss: 0.1024, Validation Loss: 0.0995
2025-12-31 17:46:14 - INFO - PRINT: Epoch [1270], Train Loss: 0.1031, Validation Loss: 0.0995
2025-12-31 17:46:40 - INFO - PRINT: Epoch [1271], Train Loss: 0.1020, Validation Loss: 0.0995
2025-12-31 17:47:07 - INFO - PRINT: Epoch [1272], Train Loss: 0.1014, Validation Loss: 0.0995
2025-12-31 17:47:34 - INFO - PRINT: Epoch [1273], Train Loss: 0.1014, Validation Loss: 0.0995
2025-12-31 17:48:01 - INFO - PRINT: Epoch [1274], Train Loss: 0.1015, Validation Loss: 0.0995
2025-12-31 17:48:27 - INFO - PRINT: Epoch [1275], Train Loss: 0.1026, Validation Loss: 0.0995
2025-12-31 17:48:54 - INFO - PRINT: Epoch [1276], Train Loss: 0.1027, Validation Loss: 0.0995
2025-12-31 17:49:21 - INFO - PRINT: Epoch [1277], Train Loss: 0.1016, Validation Loss: 0.0995
2025-12-31 17:49:48 - INFO - PRINT: Epoch [1278], Train Loss: 0.1018, Validation Loss: 0.0995
2025-12-31 17:50:14 - INFO - PRINT: Epoch [1279], Train Loss: 0.1025, Validation Loss: 0.0995
2025-12-31 17:50:46 - INFO - PRINT: Epoch [1280], Train Loss: 0.1029, Validation Loss: 0.1006
2025-12-31 17:51:12 - INFO - PRINT: Epoch [1281], Train Loss: 0.1023, Validation Loss: 0.1006
2025-12-31 17:51:39 - INFO - PRINT: Epoch [1282], Train Loss: 0.1022, Validation Loss: 0.1006
2025-12-31 17:52:05 - INFO - PRINT: Epoch [1283], Train Loss: 0.1015, Validation Loss: 0.1006
2025-12-31 17:52:32 - INFO - PRINT: Epoch [1284], Train Loss: 0.1018, Validation Loss: 0.1006
2025-12-31 17:52:59 - INFO - PRINT: Epoch [1285], Train Loss: 0.1029, Validation Loss: 0.1006
2025-12-31 17:53:26 - INFO - PRINT: Epoch [1286], Train Loss: 0.1021, Validation Loss: 0.1006
2025-12-31 17:53:52 - INFO - PRINT: Epoch [1287], Train Loss: 0.1018, Validation Loss: 0.1006
2025-12-31 17:54:19 - INFO - PRINT: Epoch [1288], Train Loss: 0.1017, Validation Loss: 0.1006
2025-12-31 17:54:46 - INFO - PRINT: Epoch [1289], Train Loss: 0.1021, Validation Loss: 0.1006
2025-12-31 17:55:13 - INFO - PRINT: Epoch [1290], Train Loss: 0.1015, Validation Loss: 0.1006
2025-12-31 17:55:39 - INFO - PRINT: Epoch [1291], Train Loss: 0.1020, Validation Loss: 0.1006
2025-12-31 17:56:06 - INFO - PRINT: Epoch [1292], Train Loss: 0.1016, Validation Loss: 0.1006
2025-12-31 17:56:33 - INFO - PRINT: Epoch [1293], Train Loss: 0.1016, Validation Loss: 0.1006
2025-12-31 17:57:00 - INFO - PRINT: Epoch [1294], Train Loss: 0.1034, Validation Loss: 0.1006
2025-12-31 17:57:26 - INFO - PRINT: Epoch [1295], Train Loss: 0.1016, Validation Loss: 0.1006
2025-12-31 17:57:53 - INFO - PRINT: Epoch [1296], Train Loss: 0.1027, Validation Loss: 0.1006
2025-12-31 17:58:20 - INFO - PRINT: Epoch [1297], Train Loss: 0.1021, Validation Loss: 0.1006
2025-12-31 17:58:47 - INFO - PRINT: Epoch [1298], Train Loss: 0.1018, Validation Loss: 0.1006
2025-12-31 17:59:13 - INFO - PRINT: Epoch [1299], Train Loss: 0.1014, Validation Loss: 0.1006
2025-12-31 17:59:44 - INFO - PRINT: Epoch [1300], Train Loss: 0.1027, Validation Loss: 0.1022
2025-12-31 17:59:44 - INFO - PRINT: ----> Saving model from epoch 1260 (val loss: 0.09954271703958512). Syrupy!
2025-12-31 18:00:11 - INFO - PRINT: Epoch [1301], Train Loss: 0.1026, Validation Loss: 0.1022
2025-12-31 18:00:38 - INFO - PRINT: Epoch [1302], Train Loss: 0.1022, Validation Loss: 0.1022
2025-12-31 18:01:04 - INFO - PRINT: Epoch [1303], Train Loss: 0.1020, Validation Loss: 0.1022
2025-12-31 18:01:31 - INFO - PRINT: Epoch [1304], Train Loss: 0.1022, Validation Loss: 0.1022
2025-12-31 18:01:58 - INFO - PRINT: Epoch [1305], Train Loss: 0.1015, Validation Loss: 0.1022
2025-12-31 18:02:25 - INFO - PRINT: Epoch [1306], Train Loss: 0.1023, Validation Loss: 0.1022
2025-12-31 18:02:51 - INFO - PRINT: Epoch [1307], Train Loss: 0.1019, Validation Loss: 0.1022
2025-12-31 18:03:18 - INFO - PRINT: Epoch [1308], Train Loss: 0.1019, Validation Loss: 0.1022
2025-12-31 18:03:45 - INFO - PRINT: Epoch [1309], Train Loss: 0.1017, Validation Loss: 0.1022
2025-12-31 18:04:12 - INFO - PRINT: Epoch [1310], Train Loss: 0.1020, Validation Loss: 0.1022
2025-12-31 18:04:38 - INFO - PRINT: Epoch [1311], Train Loss: 0.1027, Validation Loss: 0.1022
2025-12-31 18:05:05 - INFO - PRINT: Epoch [1312], Train Loss: 0.1019, Validation Loss: 0.1022
2025-12-31 18:05:32 - INFO - PRINT: Epoch [1313], Train Loss: 0.1013, Validation Loss: 0.1022
2025-12-31 18:05:59 - INFO - PRINT: Epoch [1314], Train Loss: 0.1018, Validation Loss: 0.1022
2025-12-31 18:06:25 - INFO - PRINT: Epoch [1315], Train Loss: 0.1035, Validation Loss: 0.1022
2025-12-31 18:06:52 - INFO - PRINT: Epoch [1316], Train Loss: 0.1017, Validation Loss: 0.1022
2025-12-31 18:07:19 - INFO - PRINT: Epoch [1317], Train Loss: 0.1015, Validation Loss: 0.1022
2025-12-31 18:07:46 - INFO - PRINT: Epoch [1318], Train Loss: 0.1023, Validation Loss: 0.1022
2025-12-31 18:08:12 - INFO - PRINT: Epoch [1319], Train Loss: 0.1018, Validation Loss: 0.1022
2025-12-31 18:08:44 - INFO - PRINT: Epoch [1320], Train Loss: 0.1017, Validation Loss: 0.1018
2025-12-31 18:09:10 - INFO - PRINT: Epoch [1321], Train Loss: 0.1021, Validation Loss: 0.1018
2025-12-31 18:09:37 - INFO - PRINT: Epoch [1322], Train Loss: 0.1026, Validation Loss: 0.1018
2025-12-31 18:10:03 - INFO - PRINT: Epoch [1323], Train Loss: 0.1027, Validation Loss: 0.1018
2025-12-31 18:10:30 - INFO - PRINT: Epoch [1324], Train Loss: 0.1021, Validation Loss: 0.1018
2025-12-31 18:10:57 - INFO - PRINT: Epoch [1325], Train Loss: 0.1012, Validation Loss: 0.1018
2025-12-31 18:11:24 - INFO - PRINT: Epoch [1326], Train Loss: 0.1031, Validation Loss: 0.1018
2025-12-31 18:11:50 - INFO - PRINT: Epoch [1327], Train Loss: 0.1024, Validation Loss: 0.1018
2025-12-31 18:12:17 - INFO - PRINT: Epoch [1328], Train Loss: 0.1021, Validation Loss: 0.1018
2025-12-31 18:12:44 - INFO - PRINT: Epoch [1329], Train Loss: 0.1020, Validation Loss: 0.1018
2025-12-31 18:13:11 - INFO - PRINT: Epoch [1330], Train Loss: 0.1013, Validation Loss: 0.1018
2025-12-31 18:13:37 - INFO - PRINT: Epoch [1331], Train Loss: 0.1025, Validation Loss: 0.1018
2025-12-31 18:14:04 - INFO - PRINT: Epoch [1332], Train Loss: 0.1018, Validation Loss: 0.1018
2025-12-31 18:14:31 - INFO - PRINT: Epoch [1333], Train Loss: 0.1024, Validation Loss: 0.1018
2025-12-31 18:14:57 - INFO - PRINT: Epoch [1334], Train Loss: 0.1014, Validation Loss: 0.1018
2025-12-31 18:15:24 - INFO - PRINT: Epoch [1335], Train Loss: 0.1015, Validation Loss: 0.1018
2025-12-31 18:15:51 - INFO - PRINT: Epoch [1336], Train Loss: 0.1020, Validation Loss: 0.1018
2025-12-31 18:16:18 - INFO - PRINT: Epoch [1337], Train Loss: 0.1016, Validation Loss: 0.1018
2025-12-31 18:16:44 - INFO - PRINT: Epoch [1338], Train Loss: 0.1020, Validation Loss: 0.1018
2025-12-31 18:17:11 - INFO - PRINT: Epoch [1339], Train Loss: 0.1024, Validation Loss: 0.1018
2025-12-31 18:17:42 - INFO - PRINT: Epoch [1340], Train Loss: 0.1020, Validation Loss: 0.1047
2025-12-31 18:18:09 - INFO - PRINT: Epoch [1341], Train Loss: 0.1015, Validation Loss: 0.1047
2025-12-31 18:18:35 - INFO - PRINT: Epoch [1342], Train Loss: 0.1015, Validation Loss: 0.1047
2025-12-31 18:19:02 - INFO - PRINT: Epoch [1343], Train Loss: 0.1020, Validation Loss: 0.1047
2025-12-31 18:19:29 - INFO - PRINT: Epoch [1344], Train Loss: 0.1015, Validation Loss: 0.1047
2025-12-31 18:19:56 - INFO - PRINT: Epoch [1345], Train Loss: 0.1016, Validation Loss: 0.1047
2025-12-31 18:20:23 - INFO - PRINT: Epoch [1346], Train Loss: 0.1025, Validation Loss: 0.1047
2025-12-31 18:20:49 - INFO - PRINT: Epoch [1347], Train Loss: 0.1016, Validation Loss: 0.1047
2025-12-31 18:21:16 - INFO - PRINT: Epoch [1348], Train Loss: 0.1021, Validation Loss: 0.1047
2025-12-31 18:21:43 - INFO - PRINT: Epoch [1349], Train Loss: 0.1025, Validation Loss: 0.1047
2025-12-31 18:22:10 - INFO - PRINT: Epoch [1350], Train Loss: 0.1012, Validation Loss: 0.1047
2025-12-31 18:22:36 - INFO - PRINT: Epoch [1351], Train Loss: 0.1020, Validation Loss: 0.1047
2025-12-31 18:23:03 - INFO - PRINT: Epoch [1352], Train Loss: 0.1014, Validation Loss: 0.1047
2025-12-31 18:23:30 - INFO - PRINT: Epoch [1353], Train Loss: 0.1051, Validation Loss: 0.1047
2025-12-31 18:23:57 - INFO - PRINT: Epoch [1354], Train Loss: 0.1021, Validation Loss: 0.1047
2025-12-31 18:24:23 - INFO - PRINT: Epoch [1355], Train Loss: 0.1015, Validation Loss: 0.1047
2025-12-31 18:24:50 - INFO - PRINT: Epoch [1356], Train Loss: 0.1014, Validation Loss: 0.1047
2025-12-31 18:25:17 - INFO - PRINT: Epoch [1357], Train Loss: 0.1013, Validation Loss: 0.1047
2025-12-31 18:25:43 - INFO - PRINT: Epoch [1358], Train Loss: 0.1011, Validation Loss: 0.1047
2025-12-31 18:26:10 - INFO - PRINT: Epoch [1359], Train Loss: 0.1024, Validation Loss: 0.1047
2025-12-31 18:26:41 - INFO - PRINT: Epoch [1360], Train Loss: 0.1027, Validation Loss: 0.1003
2025-12-31 18:27:08 - INFO - PRINT: Epoch [1361], Train Loss: 0.1020, Validation Loss: 0.1003
2025-12-31 18:27:34 - INFO - PRINT: Epoch [1362], Train Loss: 0.1017, Validation Loss: 0.1003
2025-12-31 18:28:01 - INFO - PRINT: Epoch [1363], Train Loss: 0.1015, Validation Loss: 0.1003
2025-12-31 18:28:28 - INFO - PRINT: Epoch [1364], Train Loss: 0.1012, Validation Loss: 0.1003
2025-12-31 18:28:55 - INFO - PRINT: Epoch [1365], Train Loss: 0.1012, Validation Loss: 0.1003
2025-12-31 18:29:21 - INFO - PRINT: Epoch [1366], Train Loss: 0.1022, Validation Loss: 0.1003
2025-12-31 18:29:48 - INFO - PRINT: Epoch [1367], Train Loss: 0.1020, Validation Loss: 0.1003
2025-12-31 18:30:15 - INFO - PRINT: Epoch [1368], Train Loss: 0.1012, Validation Loss: 0.1003
2025-12-31 18:30:42 - INFO - PRINT: Epoch [1369], Train Loss: 0.1028, Validation Loss: 0.1003
2025-12-31 18:31:08 - INFO - PRINT: Epoch [1370], Train Loss: 0.1029, Validation Loss: 0.1003
2025-12-31 18:31:35 - INFO - PRINT: Epoch [1371], Train Loss: 0.1015, Validation Loss: 0.1003
2025-12-31 18:32:02 - INFO - PRINT: Epoch [1372], Train Loss: 0.1014, Validation Loss: 0.1003
2025-12-31 18:32:29 - INFO - PRINT: Epoch [1373], Train Loss: 0.1017, Validation Loss: 0.1003
2025-12-31 18:32:55 - INFO - PRINT: Epoch [1374], Train Loss: 0.1020, Validation Loss: 0.1003
2025-12-31 18:33:22 - INFO - PRINT: Epoch [1375], Train Loss: 0.1027, Validation Loss: 0.1003
2025-12-31 18:33:49 - INFO - PRINT: Epoch [1376], Train Loss: 0.1015, Validation Loss: 0.1003
2025-12-31 18:34:16 - INFO - PRINT: Epoch [1377], Train Loss: 0.1020, Validation Loss: 0.1003
2025-12-31 18:34:42 - INFO - PRINT: Epoch [1378], Train Loss: 0.1026, Validation Loss: 0.1003
2025-12-31 18:35:09 - INFO - PRINT: Epoch [1379], Train Loss: 0.1034, Validation Loss: 0.1003
2025-12-31 18:35:40 - INFO - PRINT: Epoch [1380], Train Loss: 0.1013, Validation Loss: 0.0993
2025-12-31 18:36:07 - INFO - PRINT: Epoch [1381], Train Loss: 0.1021, Validation Loss: 0.0993
2025-12-31 18:36:33 - INFO - PRINT: Epoch [1382], Train Loss: 0.1009, Validation Loss: 0.0993
2025-12-31 18:37:00 - INFO - PRINT: Epoch [1383], Train Loss: 0.1014, Validation Loss: 0.0993
2025-12-31 18:37:27 - INFO - PRINT: Epoch [1384], Train Loss: 0.1016, Validation Loss: 0.0993
2025-12-31 18:37:54 - INFO - PRINT: Epoch [1385], Train Loss: 0.1009, Validation Loss: 0.0993
2025-12-31 18:38:20 - INFO - PRINT: Epoch [1386], Train Loss: 0.1015, Validation Loss: 0.0993
2025-12-31 18:38:47 - INFO - PRINT: Epoch [1387], Train Loss: 0.1016, Validation Loss: 0.0993
2025-12-31 18:39:14 - INFO - PRINT: Epoch [1388], Train Loss: 0.1012, Validation Loss: 0.0993
2025-12-31 18:39:41 - INFO - PRINT: Epoch [1389], Train Loss: 0.1021, Validation Loss: 0.0993
2025-12-31 18:40:07 - INFO - PRINT: Epoch [1390], Train Loss: 0.1014, Validation Loss: 0.0993
2025-12-31 18:40:34 - INFO - PRINT: Epoch [1391], Train Loss: 0.1015, Validation Loss: 0.0993
2025-12-31 18:41:01 - INFO - PRINT: Epoch [1392], Train Loss: 0.1013, Validation Loss: 0.0993
2025-12-31 18:41:28 - INFO - PRINT: Epoch [1393], Train Loss: 0.1016, Validation Loss: 0.0993
2025-12-31 18:41:54 - INFO - PRINT: Epoch [1394], Train Loss: 0.1016, Validation Loss: 0.0993
2025-12-31 18:42:21 - INFO - PRINT: Epoch [1395], Train Loss: 0.1023, Validation Loss: 0.0993
2025-12-31 18:42:48 - INFO - PRINT: Epoch [1396], Train Loss: 0.1022, Validation Loss: 0.0993
2025-12-31 18:43:15 - INFO - PRINT: Epoch [1397], Train Loss: 0.1014, Validation Loss: 0.0993
2025-12-31 18:43:41 - INFO - PRINT: Epoch [1398], Train Loss: 0.1026, Validation Loss: 0.0993
2025-12-31 18:44:08 - INFO - PRINT: Epoch [1399], Train Loss: 0.1015, Validation Loss: 0.0993
2025-12-31 18:44:39 - INFO - PRINT: Epoch [1400], Train Loss: 0.1020, Validation Loss: 0.0994
2025-12-31 18:44:39 - INFO - PRINT: ----> Saving model from epoch 1380 (val loss: 0.09929491922259331). Pungent!
2025-12-31 18:45:06 - INFO - PRINT: Epoch [1401], Train Loss: 0.1016, Validation Loss: 0.0994
2025-12-31 18:45:32 - INFO - PRINT: Epoch [1402], Train Loss: 0.1016, Validation Loss: 0.0994
2025-12-31 18:45:59 - INFO - PRINT: Epoch [1403], Train Loss: 0.1020, Validation Loss: 0.0994
2025-12-31 18:46:26 - INFO - PRINT: Epoch [1404], Train Loss: 0.1010, Validation Loss: 0.0994
2025-12-31 18:46:52 - INFO - PRINT: Epoch [1405], Train Loss: 0.1020, Validation Loss: 0.0994
2025-12-31 18:47:19 - INFO - PRINT: Epoch [1406], Train Loss: 0.1020, Validation Loss: 0.0994
2025-12-31 18:47:46 - INFO - PRINT: Epoch [1407], Train Loss: 0.1025, Validation Loss: 0.0994
2025-12-31 18:48:13 - INFO - PRINT: Epoch [1408], Train Loss: 0.1026, Validation Loss: 0.0994
2025-12-31 18:48:39 - INFO - PRINT: Epoch [1409], Train Loss: 0.1014, Validation Loss: 0.0994
2025-12-31 18:49:06 - INFO - PRINT: Epoch [1410], Train Loss: 0.1024, Validation Loss: 0.0994
2025-12-31 18:49:33 - INFO - PRINT: Epoch [1411], Train Loss: 0.1017, Validation Loss: 0.0994
2025-12-31 18:50:00 - INFO - PRINT: Epoch [1412], Train Loss: 0.1025, Validation Loss: 0.0994
2025-12-31 18:50:26 - INFO - PRINT: Epoch [1413], Train Loss: 0.1017, Validation Loss: 0.0994
2025-12-31 18:50:53 - INFO - PRINT: Epoch [1414], Train Loss: 0.1019, Validation Loss: 0.0994
2025-12-31 18:51:20 - INFO - PRINT: Epoch [1415], Train Loss: 0.1012, Validation Loss: 0.0994
2025-12-31 18:51:47 - INFO - PRINT: Epoch [1416], Train Loss: 0.1016, Validation Loss: 0.0994
2025-12-31 18:52:13 - INFO - PRINT: Epoch [1417], Train Loss: 0.1017, Validation Loss: 0.0994
2025-12-31 18:52:40 - INFO - PRINT: Epoch [1418], Train Loss: 0.1020, Validation Loss: 0.0994
2025-12-31 18:53:07 - INFO - PRINT: Epoch [1419], Train Loss: 0.1016, Validation Loss: 0.0994
2025-12-31 18:53:38 - INFO - PRINT: Epoch [1420], Train Loss: 0.1018, Validation Loss: 0.0992
2025-12-31 18:54:05 - INFO - PRINT: Epoch [1421], Train Loss: 0.1018, Validation Loss: 0.0992
2025-12-31 18:54:31 - INFO - PRINT: Epoch [1422], Train Loss: 0.1020, Validation Loss: 0.0992
2025-12-31 18:54:58 - INFO - PRINT: Epoch [1423], Train Loss: 0.1015, Validation Loss: 0.0992
2025-12-31 18:55:25 - INFO - PRINT: Epoch [1424], Train Loss: 0.1022, Validation Loss: 0.0992
2025-12-31 18:55:52 - INFO - PRINT: Epoch [1425], Train Loss: 0.1015, Validation Loss: 0.0992
2025-12-31 18:56:18 - INFO - PRINT: Epoch [1426], Train Loss: 0.1015, Validation Loss: 0.0992
2025-12-31 18:56:45 - INFO - PRINT: Epoch [1427], Train Loss: 0.1018, Validation Loss: 0.0992
2025-12-31 18:57:12 - INFO - PRINT: Epoch [1428], Train Loss: 0.1021, Validation Loss: 0.0992
2025-12-31 18:57:39 - INFO - PRINT: Epoch [1429], Train Loss: 0.1012, Validation Loss: 0.0992
2025-12-31 18:58:05 - INFO - PRINT: Epoch [1430], Train Loss: 0.1020, Validation Loss: 0.0992
2025-12-31 18:58:32 - INFO - PRINT: Epoch [1431], Train Loss: 0.1010, Validation Loss: 0.0992
2025-12-31 18:58:59 - INFO - PRINT: Epoch [1432], Train Loss: 0.1018, Validation Loss: 0.0992
2025-12-31 18:59:25 - INFO - PRINT: Epoch [1433], Train Loss: 0.1015, Validation Loss: 0.0992
2025-12-31 18:59:52 - INFO - PRINT: Epoch [1434], Train Loss: 0.1018, Validation Loss: 0.0992
2025-12-31 19:00:19 - INFO - PRINT: Epoch [1435], Train Loss: 0.1021, Validation Loss: 0.0992
2025-12-31 19:00:46 - INFO - PRINT: Epoch [1436], Train Loss: 0.1021, Validation Loss: 0.0992
2025-12-31 19:01:12 - INFO - PRINT: Epoch [1437], Train Loss: 0.1016, Validation Loss: 0.0992
2025-12-31 19:01:39 - INFO - PRINT: Epoch [1438], Train Loss: 0.1018, Validation Loss: 0.0992
2025-12-31 19:02:06 - INFO - PRINT: Epoch [1439], Train Loss: 0.1021, Validation Loss: 0.0992
2025-12-31 19:02:37 - INFO - PRINT: Epoch [1440], Train Loss: 0.1016, Validation Loss: 0.0990
2025-12-31 19:03:04 - INFO - PRINT: Epoch [1441], Train Loss: 0.1013, Validation Loss: 0.0990
2025-12-31 19:03:30 - INFO - PRINT: Epoch [1442], Train Loss: 0.1015, Validation Loss: 0.0990
2025-12-31 19:03:57 - INFO - PRINT: Epoch [1443], Train Loss: 0.1013, Validation Loss: 0.0990
2025-12-31 19:04:24 - INFO - PRINT: Epoch [1444], Train Loss: 0.1018, Validation Loss: 0.0990
2025-12-31 19:04:51 - INFO - PRINT: Epoch [1445], Train Loss: 0.1021, Validation Loss: 0.0990
2025-12-31 19:05:17 - INFO - PRINT: Epoch [1446], Train Loss: 0.1023, Validation Loss: 0.0990
2025-12-31 19:05:44 - INFO - PRINT: Epoch [1447], Train Loss: 0.1012, Validation Loss: 0.0990
2025-12-31 19:06:11 - INFO - PRINT: Epoch [1448], Train Loss: 0.1008, Validation Loss: 0.0990
2025-12-31 19:06:38 - INFO - PRINT: Epoch [1449], Train Loss: 0.1015, Validation Loss: 0.0990
2025-12-31 19:07:04 - INFO - PRINT: Epoch [1450], Train Loss: 0.1015, Validation Loss: 0.0990
2025-12-31 19:07:31 - INFO - PRINT: Epoch [1451], Train Loss: 0.1025, Validation Loss: 0.0990
2025-12-31 19:07:58 - INFO - PRINT: Epoch [1452], Train Loss: 0.1016, Validation Loss: 0.0990
2025-12-31 19:08:25 - INFO - PRINT: Epoch [1453], Train Loss: 0.1028, Validation Loss: 0.0990
2025-12-31 19:08:51 - INFO - PRINT: Epoch [1454], Train Loss: 0.1014, Validation Loss: 0.0990
2025-12-31 19:09:18 - INFO - PRINT: Epoch [1455], Train Loss: 0.1013, Validation Loss: 0.0990
2025-12-31 19:09:45 - INFO - PRINT: Epoch [1456], Train Loss: 0.1018, Validation Loss: 0.0990
2025-12-31 19:10:12 - INFO - PRINT: Epoch [1457], Train Loss: 0.1017, Validation Loss: 0.0990
2025-12-31 19:10:38 - INFO - PRINT: Epoch [1458], Train Loss: 0.1016, Validation Loss: 0.0990
2025-12-31 19:11:05 - INFO - PRINT: Epoch [1459], Train Loss: 0.1009, Validation Loss: 0.0990
2025-12-31 19:11:36 - INFO - PRINT: Epoch [1460], Train Loss: 0.1016, Validation Loss: 0.1000
2025-12-31 19:12:03 - INFO - PRINT: Epoch [1461], Train Loss: 0.1009, Validation Loss: 0.1000
2025-12-31 19:12:29 - INFO - PRINT: Epoch [1462], Train Loss: 0.1015, Validation Loss: 0.1000
2025-12-31 19:12:56 - INFO - PRINT: Epoch [1463], Train Loss: 0.1018, Validation Loss: 0.1000
2025-12-31 19:13:23 - INFO - PRINT: Epoch [1464], Train Loss: 0.1015, Validation Loss: 0.1000
2025-12-31 19:13:50 - INFO - PRINT: Epoch [1465], Train Loss: 0.1019, Validation Loss: 0.1000
2025-12-31 19:14:16 - INFO - PRINT: Epoch [1466], Train Loss: 0.1019, Validation Loss: 0.1000
2025-12-31 19:14:43 - INFO - PRINT: Epoch [1467], Train Loss: 0.1012, Validation Loss: 0.1000
2025-12-31 19:15:10 - INFO - PRINT: Epoch [1468], Train Loss: 0.1019, Validation Loss: 0.1000
2025-12-31 19:15:36 - INFO - PRINT: Epoch [1469], Train Loss: 0.1017, Validation Loss: 0.1000
2025-12-31 19:16:03 - INFO - PRINT: Epoch [1470], Train Loss: 0.1022, Validation Loss: 0.1000
2025-12-31 19:16:30 - INFO - PRINT: Epoch [1471], Train Loss: 0.1020, Validation Loss: 0.1000
2025-12-31 19:16:57 - INFO - PRINT: Epoch [1472], Train Loss: 0.1025, Validation Loss: 0.1000
2025-12-31 19:17:23 - INFO - PRINT: Epoch [1473], Train Loss: 0.1015, Validation Loss: 0.1000
2025-12-31 19:17:50 - INFO - PRINT: Epoch [1474], Train Loss: 0.1017, Validation Loss: 0.1000
2025-12-31 19:18:17 - INFO - PRINT: Epoch [1475], Train Loss: 0.1033, Validation Loss: 0.1000
2025-12-31 19:18:43 - INFO - PRINT: Epoch [1476], Train Loss: 0.1015, Validation Loss: 0.1000
2025-12-31 19:19:10 - INFO - PRINT: Epoch [1477], Train Loss: 0.1012, Validation Loss: 0.1000
2025-12-31 19:19:37 - INFO - PRINT: Epoch [1478], Train Loss: 0.1015, Validation Loss: 0.1000
2025-12-31 19:20:04 - INFO - PRINT: Epoch [1479], Train Loss: 0.1017, Validation Loss: 0.1000
2025-12-31 19:20:35 - INFO - PRINT: Epoch [1480], Train Loss: 0.1014, Validation Loss: 0.1008
2025-12-31 19:21:01 - INFO - PRINT: Epoch [1481], Train Loss: 0.1023, Validation Loss: 0.1008
2025-12-31 19:21:28 - INFO - PRINT: Epoch [1482], Train Loss: 0.1016, Validation Loss: 0.1008
2025-12-31 19:21:54 - INFO - PRINT: Epoch [1483], Train Loss: 0.1023, Validation Loss: 0.1008
2025-12-31 19:22:21 - INFO - PRINT: Epoch [1484], Train Loss: 0.1021, Validation Loss: 0.1008
2025-12-31 19:22:48 - INFO - PRINT: Epoch [1485], Train Loss: 0.1012, Validation Loss: 0.1008
2025-12-31 19:23:15 - INFO - PRINT: Epoch [1486], Train Loss: 0.1027, Validation Loss: 0.1008
2025-12-31 19:23:41 - INFO - PRINT: Epoch [1487], Train Loss: 0.1017, Validation Loss: 0.1008
2025-12-31 19:24:08 - INFO - PRINT: Epoch [1488], Train Loss: 0.1011, Validation Loss: 0.1008
2025-12-31 19:24:35 - INFO - PRINT: Epoch [1489], Train Loss: 0.1012, Validation Loss: 0.1008
2025-12-31 19:25:02 - INFO - PRINT: Epoch [1490], Train Loss: 0.1010, Validation Loss: 0.1008
2025-12-31 19:25:28 - INFO - PRINT: Epoch [1491], Train Loss: 0.1019, Validation Loss: 0.1008
2025-12-31 19:25:55 - INFO - PRINT: Epoch [1492], Train Loss: 0.1023, Validation Loss: 0.1008
2025-12-31 19:26:22 - INFO - PRINT: Epoch [1493], Train Loss: 0.1019, Validation Loss: 0.1008
2025-12-31 19:26:48 - INFO - PRINT: Epoch [1494], Train Loss: 0.1013, Validation Loss: 0.1008
2025-12-31 19:27:15 - INFO - PRINT: Epoch [1495], Train Loss: 0.1027, Validation Loss: 0.1008
2025-12-31 19:27:42 - INFO - PRINT: Epoch [1496], Train Loss: 0.1007, Validation Loss: 0.1008
2025-12-31 19:28:09 - INFO - PRINT: Epoch [1497], Train Loss: 0.1027, Validation Loss: 0.1008
2025-12-31 19:28:35 - INFO - PRINT: Epoch [1498], Train Loss: 0.1014, Validation Loss: 0.1008
2025-12-31 19:29:02 - INFO - PRINT: Epoch [1499], Train Loss: 0.1010, Validation Loss: 0.1008
2025-12-31 19:29:33 - INFO - PRINT: Epoch [1500], Train Loss: 0.1014, Validation Loss: 0.0996
2025-12-31 19:29:33 - INFO - PRINT: ----> Saving model from epoch 1440 (val loss: 0.09901672303676605). Explosive!
2025-12-31 19:29:59 - INFO - PRINT: Epoch [1501], Train Loss: 0.1018, Validation Loss: 0.0996
2025-12-31 19:30:26 - INFO - PRINT: Epoch [1502], Train Loss: 0.1014, Validation Loss: 0.0996
2025-12-31 19:30:53 - INFO - PRINT: Epoch [1503], Train Loss: 0.1009, Validation Loss: 0.0996
2025-12-31 19:31:20 - INFO - PRINT: Epoch [1504], Train Loss: 0.1008, Validation Loss: 0.0996
2025-12-31 19:31:46 - INFO - PRINT: Epoch [1505], Train Loss: 0.1020, Validation Loss: 0.0996
2025-12-31 19:32:13 - INFO - PRINT: Epoch [1506], Train Loss: 0.1018, Validation Loss: 0.0996
2025-12-31 19:32:40 - INFO - PRINT: Epoch [1507], Train Loss: 0.1015, Validation Loss: 0.0996
2025-12-31 19:33:06 - INFO - PRINT: Epoch [1508], Train Loss: 0.1014, Validation Loss: 0.0996
2025-12-31 19:33:33 - INFO - PRINT: Epoch [1509], Train Loss: 0.1014, Validation Loss: 0.0996
2025-12-31 19:34:00 - INFO - PRINT: Epoch [1510], Train Loss: 0.1028, Validation Loss: 0.0996
2025-12-31 19:34:27 - INFO - PRINT: Epoch [1511], Train Loss: 0.1023, Validation Loss: 0.0996
2025-12-31 19:34:53 - INFO - PRINT: Epoch [1512], Train Loss: 0.1013, Validation Loss: 0.0996
2025-12-31 19:35:20 - INFO - PRINT: Epoch [1513], Train Loss: 0.1022, Validation Loss: 0.0996
2025-12-31 19:35:47 - INFO - PRINT: Epoch [1514], Train Loss: 0.1017, Validation Loss: 0.0996
2025-12-31 19:36:13 - INFO - PRINT: Epoch [1515], Train Loss: 0.1021, Validation Loss: 0.0996
2025-12-31 19:36:40 - INFO - PRINT: Epoch [1516], Train Loss: 0.1016, Validation Loss: 0.0996
2025-12-31 19:37:07 - INFO - PRINT: Epoch [1517], Train Loss: 0.1012, Validation Loss: 0.0996
2025-12-31 19:37:34 - INFO - PRINT: Epoch [1518], Train Loss: 0.1020, Validation Loss: 0.0996
2025-12-31 19:38:00 - INFO - PRINT: Epoch [1519], Train Loss: 0.1008, Validation Loss: 0.0996
2025-12-31 19:38:31 - INFO - PRINT: Epoch [1520], Train Loss: 0.1011, Validation Loss: 0.0991
2025-12-31 19:38:58 - INFO - PRINT: Epoch [1521], Train Loss: 0.1008, Validation Loss: 0.0991
2025-12-31 19:39:24 - INFO - PRINT: Epoch [1522], Train Loss: 0.1011, Validation Loss: 0.0991
2025-12-31 19:39:51 - INFO - PRINT: Epoch [1523], Train Loss: 0.1012, Validation Loss: 0.0991
2025-12-31 19:40:18 - INFO - PRINT: Epoch [1524], Train Loss: 0.1014, Validation Loss: 0.0991
2025-12-31 19:40:45 - INFO - PRINT: Epoch [1525], Train Loss: 0.1019, Validation Loss: 0.0991
2025-12-31 19:41:11 - INFO - PRINT: Epoch [1526], Train Loss: 0.1025, Validation Loss: 0.0991
2025-12-31 19:41:38 - INFO - PRINT: Epoch [1527], Train Loss: 0.1022, Validation Loss: 0.0991
2025-12-31 19:42:05 - INFO - PRINT: Epoch [1528], Train Loss: 0.1020, Validation Loss: 0.0991
2025-12-31 19:42:31 - INFO - PRINT: Epoch [1529], Train Loss: 0.1013, Validation Loss: 0.0991
2025-12-31 19:42:58 - INFO - PRINT: Epoch [1530], Train Loss: 0.1014, Validation Loss: 0.0991
2025-12-31 19:43:25 - INFO - PRINT: Epoch [1531], Train Loss: 0.1033, Validation Loss: 0.0991
2025-12-31 19:43:51 - INFO - PRINT: Epoch [1532], Train Loss: 0.1015, Validation Loss: 0.0991
2025-12-31 19:44:18 - INFO - PRINT: Epoch [1533], Train Loss: 0.1008, Validation Loss: 0.0991
2025-12-31 19:44:45 - INFO - PRINT: Epoch [1534], Train Loss: 0.1013, Validation Loss: 0.0991
2025-12-31 19:45:12 - INFO - PRINT: Epoch [1535], Train Loss: 0.1013, Validation Loss: 0.0991
2025-12-31 19:45:38 - INFO - PRINT: Epoch [1536], Train Loss: 0.1014, Validation Loss: 0.0991
2025-12-31 19:46:05 - INFO - PRINT: Epoch [1537], Train Loss: 0.1019, Validation Loss: 0.0991
2025-12-31 19:46:32 - INFO - PRINT: Epoch [1538], Train Loss: 0.1018, Validation Loss: 0.0991
2025-12-31 19:46:58 - INFO - PRINT: Epoch [1539], Train Loss: 0.1011, Validation Loss: 0.0991
2025-12-31 19:47:30 - INFO - PRINT: Epoch [1540], Train Loss: 0.1006, Validation Loss: 0.0983
2025-12-31 19:47:56 - INFO - PRINT: Epoch [1541], Train Loss: 0.1008, Validation Loss: 0.0983
2025-12-31 19:48:23 - INFO - PRINT: Epoch [1542], Train Loss: 0.1010, Validation Loss: 0.0983
2025-12-31 19:48:49 - INFO - PRINT: Epoch [1543], Train Loss: 0.1028, Validation Loss: 0.0983
2025-12-31 19:49:16 - INFO - PRINT: Epoch [1544], Train Loss: 0.1016, Validation Loss: 0.0983
2025-12-31 19:49:43 - INFO - PRINT: Epoch [1545], Train Loss: 0.1011, Validation Loss: 0.0983
2025-12-31 19:50:10 - INFO - PRINT: Epoch [1546], Train Loss: 0.1013, Validation Loss: 0.0983
2025-12-31 19:50:36 - INFO - PRINT: Epoch [1547], Train Loss: 0.1018, Validation Loss: 0.0983
2025-12-31 19:51:03 - INFO - PRINT: Epoch [1548], Train Loss: 0.1018, Validation Loss: 0.0983
2025-12-31 19:51:30 - INFO - PRINT: Epoch [1549], Train Loss: 0.1018, Validation Loss: 0.0983
2025-12-31 19:51:56 - INFO - PRINT: Epoch [1550], Train Loss: 0.1009, Validation Loss: 0.0983
2025-12-31 19:52:23 - INFO - PRINT: Epoch [1551], Train Loss: 0.1021, Validation Loss: 0.0983
2025-12-31 19:52:50 - INFO - PRINT: Epoch [1552], Train Loss: 0.1021, Validation Loss: 0.0983
2025-12-31 19:53:16 - INFO - PRINT: Epoch [1553], Train Loss: 0.1023, Validation Loss: 0.0983
2025-12-31 19:53:43 - INFO - PRINT: Epoch [1554], Train Loss: 0.1012, Validation Loss: 0.0983
2025-12-31 19:54:10 - INFO - PRINT: Epoch [1555], Train Loss: 0.1018, Validation Loss: 0.0983
2025-12-31 19:54:37 - INFO - PRINT: Epoch [1556], Train Loss: 0.1010, Validation Loss: 0.0983
2025-12-31 19:55:03 - INFO - PRINT: Epoch [1557], Train Loss: 0.1011, Validation Loss: 0.0983
2025-12-31 19:55:30 - INFO - PRINT: Epoch [1558], Train Loss: 0.1019, Validation Loss: 0.0983
2025-12-31 19:55:57 - INFO - PRINT: Epoch [1559], Train Loss: 0.1013, Validation Loss: 0.0983
2025-12-31 19:56:28 - INFO - PRINT: Epoch [1560], Train Loss: 0.1011, Validation Loss: 0.0997
2025-12-31 19:56:54 - INFO - PRINT: Epoch [1561], Train Loss: 0.1010, Validation Loss: 0.0997
2025-12-31 19:57:21 - INFO - PRINT: Epoch [1562], Train Loss: 0.1012, Validation Loss: 0.0997
2025-12-31 19:57:48 - INFO - PRINT: Epoch [1563], Train Loss: 0.1030, Validation Loss: 0.0997
2025-12-31 19:58:14 - INFO - PRINT: Epoch [1564], Train Loss: 0.1014, Validation Loss: 0.0997
2025-12-31 19:58:41 - INFO - PRINT: Epoch [1565], Train Loss: 0.1017, Validation Loss: 0.0997
2025-12-31 19:59:08 - INFO - PRINT: Epoch [1566], Train Loss: 0.1016, Validation Loss: 0.0997
2025-12-31 19:59:35 - INFO - PRINT: Epoch [1567], Train Loss: 0.1018, Validation Loss: 0.0997
2025-12-31 20:00:01 - INFO - PRINT: Epoch [1568], Train Loss: 0.1014, Validation Loss: 0.0997
2025-12-31 20:00:28 - INFO - PRINT: Epoch [1569], Train Loss: 0.1007, Validation Loss: 0.0997
2025-12-31 20:00:55 - INFO - PRINT: Epoch [1570], Train Loss: 0.1007, Validation Loss: 0.0997
2025-12-31 20:01:21 - INFO - PRINT: Epoch [1571], Train Loss: 0.1023, Validation Loss: 0.0997
2025-12-31 20:01:48 - INFO - PRINT: Epoch [1572], Train Loss: 0.1013, Validation Loss: 0.0997
2025-12-31 20:02:15 - INFO - PRINT: Epoch [1573], Train Loss: 0.1014, Validation Loss: 0.0997
2025-12-31 20:02:42 - INFO - PRINT: Epoch [1574], Train Loss: 0.1011, Validation Loss: 0.0997
2025-12-31 20:03:08 - INFO - PRINT: Epoch [1575], Train Loss: 0.1013, Validation Loss: 0.0997
2025-12-31 20:03:35 - INFO - PRINT: Epoch [1576], Train Loss: 0.1015, Validation Loss: 0.0997
2025-12-31 20:04:02 - INFO - PRINT: Epoch [1577], Train Loss: 0.1016, Validation Loss: 0.0997
2025-12-31 20:04:28 - INFO - PRINT: Epoch [1578], Train Loss: 0.1012, Validation Loss: 0.0997
2025-12-31 20:04:55 - INFO - PRINT: Epoch [1579], Train Loss: 0.1019, Validation Loss: 0.0997
2025-12-31 20:05:26 - INFO - PRINT: Epoch [1580], Train Loss: 0.1011, Validation Loss: 0.1000
2025-12-31 20:05:53 - INFO - PRINT: Epoch [1581], Train Loss: 0.1016, Validation Loss: 0.1000
2025-12-31 20:06:19 - INFO - PRINT: Epoch [1582], Train Loss: 0.1023, Validation Loss: 0.1000
2025-12-31 20:06:46 - INFO - PRINT: Epoch [1583], Train Loss: 0.1012, Validation Loss: 0.1000
2025-12-31 20:07:13 - INFO - PRINT: Epoch [1584], Train Loss: 0.1008, Validation Loss: 0.1000
2025-12-31 20:07:40 - INFO - PRINT: Epoch [1585], Train Loss: 0.1011, Validation Loss: 0.1000
2025-12-31 20:08:06 - INFO - PRINT: Epoch [1586], Train Loss: 0.1016, Validation Loss: 0.1000
2025-12-31 20:08:33 - INFO - PRINT: Epoch [1587], Train Loss: 0.1011, Validation Loss: 0.1000
2025-12-31 20:09:00 - INFO - PRINT: Epoch [1588], Train Loss: 0.1016, Validation Loss: 0.1000
2025-12-31 20:09:26 - INFO - PRINT: Epoch [1589], Train Loss: 0.1007, Validation Loss: 0.1000
2025-12-31 20:09:53 - INFO - PRINT: Epoch [1590], Train Loss: 0.1014, Validation Loss: 0.1000
2025-12-31 20:10:20 - INFO - PRINT: Epoch [1591], Train Loss: 0.1016, Validation Loss: 0.1000
2025-12-31 20:10:47 - INFO - PRINT: Epoch [1592], Train Loss: 0.1007, Validation Loss: 0.1000
2025-12-31 20:11:13 - INFO - PRINT: Epoch [1593], Train Loss: 0.1022, Validation Loss: 0.1000
2025-12-31 20:11:40 - INFO - PRINT: Epoch [1594], Train Loss: 0.1014, Validation Loss: 0.1000
2025-12-31 20:12:07 - INFO - PRINT: Epoch [1595], Train Loss: 0.1016, Validation Loss: 0.1000
2025-12-31 20:12:33 - INFO - PRINT: Epoch [1596], Train Loss: 0.1020, Validation Loss: 0.1000
2025-12-31 20:13:00 - INFO - PRINT: Epoch [1597], Train Loss: 0.1012, Validation Loss: 0.1000
2025-12-31 20:13:27 - INFO - PRINT: Epoch [1598], Train Loss: 0.1008, Validation Loss: 0.1000
2025-12-31 20:13:54 - INFO - PRINT: Epoch [1599], Train Loss: 0.1016, Validation Loss: 0.1000
2025-12-31 20:14:25 - INFO - PRINT: Epoch [1600], Train Loss: 0.1018, Validation Loss: 0.0995
2025-12-31 20:14:25 - INFO - PRINT: ----> Saving model from epoch 1540 (val loss: 0.09834834948182106). Tangy!
2025-12-31 20:14:51 - INFO - PRINT: Epoch [1601], Train Loss: 0.1008, Validation Loss: 0.0995
2025-12-31 20:15:18 - INFO - PRINT: Epoch [1602], Train Loss: 0.1006, Validation Loss: 0.0995
2025-12-31 20:15:44 - INFO - PRINT: Epoch [1603], Train Loss: 0.1014, Validation Loss: 0.0995
2025-12-31 20:16:11 - INFO - PRINT: Epoch [1604], Train Loss: 0.1015, Validation Loss: 0.0995
2025-12-31 20:16:38 - INFO - PRINT: Epoch [1605], Train Loss: 0.1034, Validation Loss: 0.0995
2025-12-31 20:17:05 - INFO - PRINT: Epoch [1606], Train Loss: 0.1018, Validation Loss: 0.0995
2025-12-31 20:17:31 - INFO - PRINT: Epoch [1607], Train Loss: 0.1016, Validation Loss: 0.0995
2025-12-31 20:17:58 - INFO - PRINT: Epoch [1608], Train Loss: 0.1014, Validation Loss: 0.0995
2025-12-31 20:18:25 - INFO - PRINT: Epoch [1609], Train Loss: 0.1018, Validation Loss: 0.0995
2025-12-31 20:18:51 - INFO - PRINT: Epoch [1610], Train Loss: 0.1016, Validation Loss: 0.0995
2025-12-31 20:19:18 - INFO - PRINT: Epoch [1611], Train Loss: 0.1018, Validation Loss: 0.0995
2025-12-31 20:19:45 - INFO - PRINT: Epoch [1612], Train Loss: 0.1014, Validation Loss: 0.0995
2025-12-31 20:20:12 - INFO - PRINT: Epoch [1613], Train Loss: 0.1013, Validation Loss: 0.0995
2025-12-31 20:20:38 - INFO - PRINT: Epoch [1614], Train Loss: 0.1015, Validation Loss: 0.0995
2025-12-31 20:21:05 - INFO - PRINT: Epoch [1615], Train Loss: 0.1013, Validation Loss: 0.0995
2025-12-31 20:21:32 - INFO - PRINT: Epoch [1616], Train Loss: 0.1007, Validation Loss: 0.0995
2025-12-31 20:21:58 - INFO - PRINT: Epoch [1617], Train Loss: 0.1009, Validation Loss: 0.0995
2025-12-31 20:22:25 - INFO - PRINT: Epoch [1618], Train Loss: 0.1013, Validation Loss: 0.0995
2025-12-31 20:22:52 - INFO - PRINT: Epoch [1619], Train Loss: 0.1012, Validation Loss: 0.0995
2025-12-31 20:23:23 - INFO - PRINT: Epoch [1620], Train Loss: 0.1014, Validation Loss: 0.0989
2025-12-31 20:23:49 - INFO - PRINT: Epoch [1621], Train Loss: 0.1013, Validation Loss: 0.0989
2025-12-31 20:24:16 - INFO - PRINT: Epoch [1622], Train Loss: 0.1021, Validation Loss: 0.0989
2025-12-31 20:24:43 - INFO - PRINT: Epoch [1623], Train Loss: 0.1019, Validation Loss: 0.0989
2025-12-31 20:25:10 - INFO - PRINT: Epoch [1624], Train Loss: 0.1022, Validation Loss: 0.0989
2025-12-31 20:25:36 - INFO - PRINT: Epoch [1625], Train Loss: 0.1010, Validation Loss: 0.0989
2025-12-31 20:26:03 - INFO - PRINT: Epoch [1626], Train Loss: 0.1015, Validation Loss: 0.0989
2025-12-31 20:26:30 - INFO - PRINT: Epoch [1627], Train Loss: 0.1008, Validation Loss: 0.0989
2025-12-31 20:26:56 - INFO - PRINT: Epoch [1628], Train Loss: 0.1028, Validation Loss: 0.0989
2025-12-31 20:27:23 - INFO - PRINT: Epoch [1629], Train Loss: 0.1021, Validation Loss: 0.0989
2025-12-31 20:27:50 - INFO - PRINT: Epoch [1630], Train Loss: 0.1008, Validation Loss: 0.0989
2025-12-31 20:28:17 - INFO - PRINT: Epoch [1631], Train Loss: 0.1009, Validation Loss: 0.0989
2025-12-31 20:28:43 - INFO - PRINT: Epoch [1632], Train Loss: 0.1010, Validation Loss: 0.0989
2025-12-31 20:29:10 - INFO - PRINT: Epoch [1633], Train Loss: 0.1010, Validation Loss: 0.0989
2025-12-31 20:29:37 - INFO - PRINT: Epoch [1634], Train Loss: 0.1023, Validation Loss: 0.0989
2025-12-31 20:30:03 - INFO - PRINT: Epoch [1635], Train Loss: 0.1008, Validation Loss: 0.0989
2025-12-31 20:30:30 - INFO - PRINT: Epoch [1636], Train Loss: 0.1017, Validation Loss: 0.0989
2025-12-31 20:30:57 - INFO - PRINT: Epoch [1637], Train Loss: 0.1011, Validation Loss: 0.0989
2025-12-31 20:31:24 - INFO - PRINT: Epoch [1638], Train Loss: 0.1012, Validation Loss: 0.0989
2025-12-31 20:31:50 - INFO - PRINT: Epoch [1639], Train Loss: 0.1012, Validation Loss: 0.0989
2025-12-31 20:32:21 - INFO - PRINT: Epoch [1640], Train Loss: 0.1011, Validation Loss: 0.0999
2025-12-31 20:32:48 - INFO - PRINT: Epoch [1641], Train Loss: 0.1014, Validation Loss: 0.0999
2025-12-31 20:33:15 - INFO - PRINT: Epoch [1642], Train Loss: 0.1009, Validation Loss: 0.0999
2025-12-31 20:33:41 - INFO - PRINT: Epoch [1643], Train Loss: 0.1011, Validation Loss: 0.0999
2025-12-31 20:34:08 - INFO - PRINT: Epoch [1644], Train Loss: 0.1019, Validation Loss: 0.0999
2025-12-31 20:34:35 - INFO - PRINT: Epoch [1645], Train Loss: 0.1020, Validation Loss: 0.0999
2025-12-31 20:35:01 - INFO - PRINT: Epoch [1646], Train Loss: 0.1012, Validation Loss: 0.0999
2025-12-31 20:35:28 - INFO - PRINT: Epoch [1647], Train Loss: 0.1015, Validation Loss: 0.0999
2025-12-31 20:35:55 - INFO - PRINT: Epoch [1648], Train Loss: 0.1012, Validation Loss: 0.0999
2025-12-31 20:36:22 - INFO - PRINT: Epoch [1649], Train Loss: 0.1026, Validation Loss: 0.0999
2025-12-31 20:36:48 - INFO - PRINT: Epoch [1650], Train Loss: 0.1011, Validation Loss: 0.0999
2025-12-31 20:37:15 - INFO - PRINT: Epoch [1651], Train Loss: 0.1007, Validation Loss: 0.0999
2025-12-31 20:37:42 - INFO - PRINT: Epoch [1652], Train Loss: 0.1013, Validation Loss: 0.0999
2025-12-31 20:38:08 - INFO - PRINT: Epoch [1653], Train Loss: 0.1015, Validation Loss: 0.0999
2025-12-31 20:38:35 - INFO - PRINT: Epoch [1654], Train Loss: 0.1007, Validation Loss: 0.0999
2025-12-31 20:39:02 - INFO - PRINT: Epoch [1655], Train Loss: 0.1028, Validation Loss: 0.0999
2025-12-31 20:39:28 - INFO - PRINT: Epoch [1656], Train Loss: 0.1012, Validation Loss: 0.0999
2025-12-31 20:39:55 - INFO - PRINT: Epoch [1657], Train Loss: 0.1013, Validation Loss: 0.0999
2025-12-31 20:40:22 - INFO - PRINT: Epoch [1658], Train Loss: 0.1012, Validation Loss: 0.0999
2025-12-31 20:40:49 - INFO - PRINT: Epoch [1659], Train Loss: 0.1012, Validation Loss: 0.0999
2025-12-31 20:41:20 - INFO - PRINT: Epoch [1660], Train Loss: 0.1010, Validation Loss: 0.1005
2025-12-31 20:41:46 - INFO - PRINT: Epoch [1661], Train Loss: 0.1021, Validation Loss: 0.1005
2025-12-31 20:42:13 - INFO - PRINT: Epoch [1662], Train Loss: 0.1014, Validation Loss: 0.1005
2025-12-31 20:42:39 - INFO - PRINT: Epoch [1663], Train Loss: 0.1011, Validation Loss: 0.1005
2025-12-31 20:43:06 - INFO - PRINT: Epoch [1664], Train Loss: 0.1017, Validation Loss: 0.1005
2025-12-31 20:43:33 - INFO - PRINT: Epoch [1665], Train Loss: 0.1007, Validation Loss: 0.1005
2025-12-31 20:44:00 - INFO - PRINT: Epoch [1666], Train Loss: 0.1012, Validation Loss: 0.1005
2025-12-31 20:44:26 - INFO - PRINT: Epoch [1667], Train Loss: 0.1012, Validation Loss: 0.1005
2025-12-31 20:44:53 - INFO - PRINT: Epoch [1668], Train Loss: 0.1012, Validation Loss: 0.1005
2025-12-31 20:45:20 - INFO - PRINT: Epoch [1669], Train Loss: 0.1028, Validation Loss: 0.1005
2025-12-31 20:45:46 - INFO - PRINT: Epoch [1670], Train Loss: 0.1013, Validation Loss: 0.1005
2025-12-31 20:46:13 - INFO - PRINT: Epoch [1671], Train Loss: 0.1020, Validation Loss: 0.1005
2025-12-31 20:46:40 - INFO - PRINT: Epoch [1672], Train Loss: 0.1019, Validation Loss: 0.1005
2025-12-31 20:47:07 - INFO - PRINT: Epoch [1673], Train Loss: 0.1008, Validation Loss: 0.1005
2025-12-31 20:47:33 - INFO - PRINT: Epoch [1674], Train Loss: 0.1006, Validation Loss: 0.1005
2025-12-31 20:48:00 - INFO - PRINT: Epoch [1675], Train Loss: 0.1019, Validation Loss: 0.1005
2025-12-31 20:48:27 - INFO - PRINT: Epoch [1676], Train Loss: 0.1013, Validation Loss: 0.1005
2025-12-31 20:48:53 - INFO - PRINT: Epoch [1677], Train Loss: 0.1004, Validation Loss: 0.1005
2025-12-31 20:49:20 - INFO - PRINT: Epoch [1678], Train Loss: 0.1017, Validation Loss: 0.1005
2025-12-31 20:49:47 - INFO - PRINT: Epoch [1679], Train Loss: 0.1014, Validation Loss: 0.1005
2025-12-31 20:50:18 - INFO - PRINT: Epoch [1680], Train Loss: 0.1007, Validation Loss: 0.0988
2025-12-31 20:50:44 - INFO - PRINT: Epoch [1681], Train Loss: 0.1010, Validation Loss: 0.0988
2025-12-31 20:51:11 - INFO - PRINT: Epoch [1682], Train Loss: 0.1011, Validation Loss: 0.0988
2025-12-31 20:51:38 - INFO - PRINT: Epoch [1683], Train Loss: 0.1018, Validation Loss: 0.0988
2025-12-31 20:52:04 - INFO - PRINT: Epoch [1684], Train Loss: 0.1012, Validation Loss: 0.0988
2025-12-31 20:52:31 - INFO - PRINT: Epoch [1685], Train Loss: 0.1020, Validation Loss: 0.0988
2025-12-31 20:52:58 - INFO - PRINT: Epoch [1686], Train Loss: 0.1012, Validation Loss: 0.0988
2025-12-31 20:53:25 - INFO - PRINT: Epoch [1687], Train Loss: 0.1013, Validation Loss: 0.0988
2025-12-31 20:53:51 - INFO - PRINT: Epoch [1688], Train Loss: 0.1007, Validation Loss: 0.0988
2025-12-31 20:54:18 - INFO - PRINT: Epoch [1689], Train Loss: 0.1007, Validation Loss: 0.0988
2025-12-31 20:54:45 - INFO - PRINT: Epoch [1690], Train Loss: 0.1022, Validation Loss: 0.0988
2025-12-31 20:55:11 - INFO - PRINT: Epoch [1691], Train Loss: 0.1028, Validation Loss: 0.0988
2025-12-31 20:55:38 - INFO - PRINT: Epoch [1692], Train Loss: 0.1005, Validation Loss: 0.0988
2025-12-31 20:56:05 - INFO - PRINT: Epoch [1693], Train Loss: 0.1025, Validation Loss: 0.0988
2025-12-31 20:56:31 - INFO - PRINT: Epoch [1694], Train Loss: 0.1046, Validation Loss: 0.0988
2025-12-31 20:56:58 - INFO - PRINT: Epoch [1695], Train Loss: 0.1014, Validation Loss: 0.0988
2025-12-31 20:57:25 - INFO - PRINT: Epoch [1696], Train Loss: 0.1016, Validation Loss: 0.0988
2025-12-31 20:57:52 - INFO - PRINT: Epoch [1697], Train Loss: 0.1004, Validation Loss: 0.0988
2025-12-31 20:58:18 - INFO - PRINT: Epoch [1698], Train Loss: 0.1006, Validation Loss: 0.0988
2025-12-31 20:58:45 - INFO - PRINT: Epoch [1699], Train Loss: 0.1005, Validation Loss: 0.0988
2025-12-31 20:59:16 - INFO - PRINT: Epoch [1700], Train Loss: 0.1010, Validation Loss: 0.0987
2025-12-31 20:59:16 - INFO - PRINT: ----> Saving model from epoch 1540 (val loss: 0.09834834948182106). Pungent!
2025-12-31 20:59:43 - INFO - PRINT: Epoch [1701], Train Loss: 0.1006, Validation Loss: 0.0987
2025-12-31 21:00:09 - INFO - PRINT: Epoch [1702], Train Loss: 0.1018, Validation Loss: 0.0987
2025-12-31 21:00:36 - INFO - PRINT: Epoch [1703], Train Loss: 0.1016, Validation Loss: 0.0987
2025-12-31 21:01:03 - INFO - PRINT: Epoch [1704], Train Loss: 0.1017, Validation Loss: 0.0987
2025-12-31 21:01:29 - INFO - PRINT: Epoch [1705], Train Loss: 0.1014, Validation Loss: 0.0987
2025-12-31 21:01:56 - INFO - PRINT: Epoch [1706], Train Loss: 0.1015, Validation Loss: 0.0987
2025-12-31 21:02:23 - INFO - PRINT: Epoch [1707], Train Loss: 0.1011, Validation Loss: 0.0987
2025-12-31 21:02:50 - INFO - PRINT: Epoch [1708], Train Loss: 0.1015, Validation Loss: 0.0987
2025-12-31 21:03:16 - INFO - PRINT: Epoch [1709], Train Loss: 0.1018, Validation Loss: 0.0987
2025-12-31 21:03:43 - INFO - PRINT: Epoch [1710], Train Loss: 0.1023, Validation Loss: 0.0987
2025-12-31 21:04:10 - INFO - PRINT: Epoch [1711], Train Loss: 0.1017, Validation Loss: 0.0987
2025-12-31 21:04:36 - INFO - PRINT: Epoch [1712], Train Loss: 0.1006, Validation Loss: 0.0987
2025-12-31 21:05:05 - INFO - PRINT: Epoch [1713], Train Loss: 0.1009, Validation Loss: 0.0987
2025-12-31 21:05:31 - INFO - PRINT: Epoch [1714], Train Loss: 0.1009, Validation Loss: 0.0987
2025-12-31 21:05:58 - INFO - PRINT: Epoch [1715], Train Loss: 0.1010, Validation Loss: 0.0987
2025-12-31 21:06:25 - INFO - PRINT: Epoch [1716], Train Loss: 0.1006, Validation Loss: 0.0987
2025-12-31 21:06:51 - INFO - PRINT: Epoch [1717], Train Loss: 0.1010, Validation Loss: 0.0987
2025-12-31 21:07:18 - INFO - PRINT: Epoch [1718], Train Loss: 0.1014, Validation Loss: 0.0987
2025-12-31 21:07:45 - INFO - PRINT: Epoch [1719], Train Loss: 0.1015, Validation Loss: 0.0987
2025-12-31 21:08:16 - INFO - PRINT: Epoch [1720], Train Loss: 0.1008, Validation Loss: 0.0989
2025-12-31 21:08:42 - INFO - PRINT: Epoch [1721], Train Loss: 0.1012, Validation Loss: 0.0989
2025-12-31 21:09:09 - INFO - PRINT: Epoch [1722], Train Loss: 0.1006, Validation Loss: 0.0989
2025-12-31 21:09:36 - INFO - PRINT: Epoch [1723], Train Loss: 0.1015, Validation Loss: 0.0989
2025-12-31 21:10:02 - INFO - PRINT: Epoch [1724], Train Loss: 0.1019, Validation Loss: 0.0989
2025-12-31 21:10:29 - INFO - PRINT: Epoch [1725], Train Loss: 0.1013, Validation Loss: 0.0989
2025-12-31 21:10:56 - INFO - PRINT: Epoch [1726], Train Loss: 0.1006, Validation Loss: 0.0989
2025-12-31 21:11:23 - INFO - PRINT: Epoch [1727], Train Loss: 0.1011, Validation Loss: 0.0989
2025-12-31 21:11:49 - INFO - PRINT: Epoch [1728], Train Loss: 0.1012, Validation Loss: 0.0989
2025-12-31 21:12:16 - INFO - PRINT: Epoch [1729], Train Loss: 0.1012, Validation Loss: 0.0989
2025-12-31 21:12:43 - INFO - PRINT: Epoch [1730], Train Loss: 0.1015, Validation Loss: 0.0989
2025-12-31 21:13:09 - INFO - PRINT: Epoch [1731], Train Loss: 0.1014, Validation Loss: 0.0989
2025-12-31 21:13:36 - INFO - PRINT: Epoch [1732], Train Loss: 0.1009, Validation Loss: 0.0989
2025-12-31 21:14:03 - INFO - PRINT: Epoch [1733], Train Loss: 0.1013, Validation Loss: 0.0989
2025-12-31 21:14:30 - INFO - PRINT: Epoch [1734], Train Loss: 0.1008, Validation Loss: 0.0989
2025-12-31 21:14:56 - INFO - PRINT: Epoch [1735], Train Loss: 0.1017, Validation Loss: 0.0989
2025-12-31 21:15:23 - INFO - PRINT: Epoch [1736], Train Loss: 0.1013, Validation Loss: 0.0989
2025-12-31 21:15:50 - INFO - PRINT: Epoch [1737], Train Loss: 0.1010, Validation Loss: 0.0989
2025-12-31 21:16:16 - INFO - PRINT: Epoch [1738], Train Loss: 0.1012, Validation Loss: 0.0989
2025-12-31 21:16:43 - INFO - PRINT: Epoch [1739], Train Loss: 0.1008, Validation Loss: 0.0989
2025-12-31 21:17:14 - INFO - PRINT: Epoch [1740], Train Loss: 0.1010, Validation Loss: 0.1002
2025-12-31 21:17:41 - INFO - PRINT: Epoch [1741], Train Loss: 0.1012, Validation Loss: 0.1002
2025-12-31 21:18:07 - INFO - PRINT: Epoch [1742], Train Loss: 0.1007, Validation Loss: 0.1002
2025-12-31 21:18:34 - INFO - PRINT: Epoch [1743], Train Loss: 0.1011, Validation Loss: 0.1002
2025-12-31 21:19:01 - INFO - PRINT: Epoch [1744], Train Loss: 0.1013, Validation Loss: 0.1002
2025-12-31 21:19:27 - INFO - PRINT: Epoch [1745], Train Loss: 0.1014, Validation Loss: 0.1002
2025-12-31 21:19:54 - INFO - PRINT: Epoch [1746], Train Loss: 0.1022, Validation Loss: 0.1002
2025-12-31 21:20:21 - INFO - PRINT: Epoch [1747], Train Loss: 0.1013, Validation Loss: 0.1002
2025-12-31 21:20:47 - INFO - PRINT: Epoch [1748], Train Loss: 0.1009, Validation Loss: 0.1002
2025-12-31 21:21:14 - INFO - PRINT: Epoch [1749], Train Loss: 0.1010, Validation Loss: 0.1002
2025-12-31 21:21:41 - INFO - PRINT: Epoch [1750], Train Loss: 0.1017, Validation Loss: 0.1002
2025-12-31 21:22:08 - INFO - PRINT: Epoch [1751], Train Loss: 0.1009, Validation Loss: 0.1002
2025-12-31 21:22:34 - INFO - PRINT: Epoch [1752], Train Loss: 0.1009, Validation Loss: 0.1002
2025-12-31 21:23:01 - INFO - PRINT: Epoch [1753], Train Loss: 0.1010, Validation Loss: 0.1002
2025-12-31 21:23:28 - INFO - PRINT: Epoch [1754], Train Loss: 0.1014, Validation Loss: 0.1002
2025-12-31 21:23:54 - INFO - PRINT: Epoch [1755], Train Loss: 0.1013, Validation Loss: 0.1002
2025-12-31 21:24:21 - INFO - PRINT: Epoch [1756], Train Loss: 0.1014, Validation Loss: 0.1002
2025-12-31 21:24:48 - INFO - PRINT: Epoch [1757], Train Loss: 0.1013, Validation Loss: 0.1002
2025-12-31 21:25:15 - INFO - PRINT: Epoch [1758], Train Loss: 0.1022, Validation Loss: 0.1002
2025-12-31 21:25:41 - INFO - PRINT: Epoch [1759], Train Loss: 0.1018, Validation Loss: 0.1002
2025-12-31 21:26:12 - INFO - PRINT: Epoch [1760], Train Loss: 0.1012, Validation Loss: 0.0990
2025-12-31 21:26:39 - INFO - PRINT: Epoch [1761], Train Loss: 0.1016, Validation Loss: 0.0990
2025-12-31 21:27:05 - INFO - PRINT: Epoch [1762], Train Loss: 0.1012, Validation Loss: 0.0990
2025-12-31 21:27:32 - INFO - PRINT: Epoch [1763], Train Loss: 0.1004, Validation Loss: 0.0990
2025-12-31 21:27:59 - INFO - PRINT: Epoch [1764], Train Loss: 0.1003, Validation Loss: 0.0990
2025-12-31 21:28:26 - INFO - PRINT: Epoch [1765], Train Loss: 0.1017, Validation Loss: 0.0990
2025-12-31 21:28:52 - INFO - PRINT: Epoch [1766], Train Loss: 0.1013, Validation Loss: 0.0990
2025-12-31 21:29:19 - INFO - PRINT: Epoch [1767], Train Loss: 0.1007, Validation Loss: 0.0990
2025-12-31 21:29:46 - INFO - PRINT: Epoch [1768], Train Loss: 0.1012, Validation Loss: 0.0990
2025-12-31 21:30:12 - INFO - PRINT: Epoch [1769], Train Loss: 0.1023, Validation Loss: 0.0990
2025-12-31 21:30:39 - INFO - PRINT: Epoch [1770], Train Loss: 0.1008, Validation Loss: 0.0990
2025-12-31 21:31:06 - INFO - PRINT: Epoch [1771], Train Loss: 0.1014, Validation Loss: 0.0990
2025-12-31 21:31:33 - INFO - PRINT: Epoch [1772], Train Loss: 0.1019, Validation Loss: 0.0990
2025-12-31 21:31:59 - INFO - PRINT: Epoch [1773], Train Loss: 0.1018, Validation Loss: 0.0990
2025-12-31 21:32:26 - INFO - PRINT: Epoch [1774], Train Loss: 0.1007, Validation Loss: 0.0990
2025-12-31 21:32:53 - INFO - PRINT: Epoch [1775], Train Loss: 0.1011, Validation Loss: 0.0990
2025-12-31 21:33:19 - INFO - PRINT: Epoch [1776], Train Loss: 0.1015, Validation Loss: 0.0990
2025-12-31 21:33:46 - INFO - PRINT: Epoch [1777], Train Loss: 0.1026, Validation Loss: 0.0990
2025-12-31 21:34:13 - INFO - PRINT: Epoch [1778], Train Loss: 0.1019, Validation Loss: 0.0990
2025-12-31 21:34:39 - INFO - PRINT: Epoch [1779], Train Loss: 0.1014, Validation Loss: 0.0990
2025-12-31 21:35:11 - INFO - PRINT: Epoch [1780], Train Loss: 0.1013, Validation Loss: 0.0992
2025-12-31 21:35:37 - INFO - PRINT: Epoch [1781], Train Loss: 0.1006, Validation Loss: 0.0992
2025-12-31 21:36:04 - INFO - PRINT: Epoch [1782], Train Loss: 0.1013, Validation Loss: 0.0992
2025-12-31 21:36:30 - INFO - PRINT: Epoch [1783], Train Loss: 0.1014, Validation Loss: 0.0992
2025-12-31 21:36:57 - INFO - PRINT: Epoch [1784], Train Loss: 0.1009, Validation Loss: 0.0992
2025-12-31 21:37:24 - INFO - PRINT: Epoch [1785], Train Loss: 0.1006, Validation Loss: 0.0992
2025-12-31 21:37:50 - INFO - PRINT: Epoch [1786], Train Loss: 0.1008, Validation Loss: 0.0992
2025-12-31 21:38:17 - INFO - PRINT: Epoch [1787], Train Loss: 0.1018, Validation Loss: 0.0992
2025-12-31 21:38:44 - INFO - PRINT: Epoch [1788], Train Loss: 0.1012, Validation Loss: 0.0992
2025-12-31 21:39:11 - INFO - PRINT: Epoch [1789], Train Loss: 0.1014, Validation Loss: 0.0992
2025-12-31 21:39:37 - INFO - PRINT: Epoch [1790], Train Loss: 0.1010, Validation Loss: 0.0992
2025-12-31 21:40:04 - INFO - PRINT: Epoch [1791], Train Loss: 0.1012, Validation Loss: 0.0992
2025-12-31 21:40:31 - INFO - PRINT: Epoch [1792], Train Loss: 0.1005, Validation Loss: 0.0992
2025-12-31 21:40:58 - INFO - PRINT: Epoch [1793], Train Loss: 0.1010, Validation Loss: 0.0992
2025-12-31 21:41:24 - INFO - PRINT: Epoch [1794], Train Loss: 0.1011, Validation Loss: 0.0992
2025-12-31 21:41:51 - INFO - PRINT: Epoch [1795], Train Loss: 0.1015, Validation Loss: 0.0992
2025-12-31 21:42:18 - INFO - PRINT: Epoch [1796], Train Loss: 0.1007, Validation Loss: 0.0992
2025-12-31 21:42:44 - INFO - PRINT: Epoch [1797], Train Loss: 0.1017, Validation Loss: 0.0992
2025-12-31 21:43:11 - INFO - PRINT: Epoch [1798], Train Loss: 0.1014, Validation Loss: 0.0992
2025-12-31 21:43:38 - INFO - PRINT: Epoch [1799], Train Loss: 0.1006, Validation Loss: 0.0992
2025-12-31 21:44:09 - INFO - PRINT: Epoch [1800], Train Loss: 0.1015, Validation Loss: 0.0997
2025-12-31 21:44:09 - INFO - PRINT: ----> Saving model from epoch 1540 (val loss: 0.09834834948182106). Decadent!
2025-12-31 21:44:35 - INFO - PRINT: Epoch [1801], Train Loss: 0.1004, Validation Loss: 0.0997
2025-12-31 21:45:02 - INFO - PRINT: Epoch [1802], Train Loss: 0.1001, Validation Loss: 0.0997
2025-12-31 21:45:29 - INFO - PRINT: Epoch [1803], Train Loss: 0.1031, Validation Loss: 0.0997
2025-12-31 21:45:55 - INFO - PRINT: Epoch [1804], Train Loss: 0.1021, Validation Loss: 0.0997
2025-12-31 21:46:22 - INFO - PRINT: Epoch [1805], Train Loss: 0.1007, Validation Loss: 0.0997
2025-12-31 21:46:49 - INFO - PRINT: Epoch [1806], Train Loss: 0.1005, Validation Loss: 0.0997
2025-12-31 21:47:16 - INFO - PRINT: Epoch [1807], Train Loss: 0.1011, Validation Loss: 0.0997
2025-12-31 21:47:42 - INFO - PRINT: Epoch [1808], Train Loss: 0.1012, Validation Loss: 0.0997
2025-12-31 21:48:09 - INFO - PRINT: Epoch [1809], Train Loss: 0.1012, Validation Loss: 0.0997
2025-12-31 21:48:36 - INFO - PRINT: Epoch [1810], Train Loss: 0.1013, Validation Loss: 0.0997
2025-12-31 21:49:02 - INFO - PRINT: Epoch [1811], Train Loss: 0.1004, Validation Loss: 0.0997
2025-12-31 21:49:29 - INFO - PRINT: Epoch [1812], Train Loss: 0.1017, Validation Loss: 0.0997
2025-12-31 21:49:56 - INFO - PRINT: Epoch [1813], Train Loss: 0.1020, Validation Loss: 0.0997
2025-12-31 21:50:22 - INFO - PRINT: Epoch [1814], Train Loss: 0.1009, Validation Loss: 0.0997
2025-12-31 21:50:49 - INFO - PRINT: Epoch [1815], Train Loss: 0.1008, Validation Loss: 0.0997
2025-12-31 21:51:16 - INFO - PRINT: Epoch [1816], Train Loss: 0.1005, Validation Loss: 0.0997
2025-12-31 21:51:43 - INFO - PRINT: Epoch [1817], Train Loss: 0.1003, Validation Loss: 0.0997
2025-12-31 21:52:09 - INFO - PRINT: Epoch [1818], Train Loss: 0.1007, Validation Loss: 0.0997
2025-12-31 21:52:36 - INFO - PRINT: Epoch [1819], Train Loss: 0.1011, Validation Loss: 0.0997
2025-12-31 21:53:07 - INFO - PRINT: Epoch [1820], Train Loss: 0.1008, Validation Loss: 0.0992
2025-12-31 21:53:34 - INFO - PRINT: Epoch [1821], Train Loss: 0.1019, Validation Loss: 0.0992
2025-12-31 21:54:00 - INFO - PRINT: Epoch [1822], Train Loss: 0.1010, Validation Loss: 0.0992
2025-12-31 21:54:27 - INFO - PRINT: Epoch [1823], Train Loss: 0.1007, Validation Loss: 0.0992
2025-12-31 21:54:54 - INFO - PRINT: Epoch [1824], Train Loss: 0.1010, Validation Loss: 0.0992
2025-12-31 21:55:20 - INFO - PRINT: Epoch [1825], Train Loss: 0.1012, Validation Loss: 0.0992
2025-12-31 21:55:47 - INFO - PRINT: Epoch [1826], Train Loss: 0.1007, Validation Loss: 0.0992
2025-12-31 21:56:14 - INFO - PRINT: Epoch [1827], Train Loss: 0.1013, Validation Loss: 0.0992
2025-12-31 21:56:41 - INFO - PRINT: Epoch [1828], Train Loss: 0.1022, Validation Loss: 0.0992
2025-12-31 21:57:07 - INFO - PRINT: Epoch [1829], Train Loss: 0.1009, Validation Loss: 0.0992
2025-12-31 21:57:34 - INFO - PRINT: Epoch [1830], Train Loss: 0.1006, Validation Loss: 0.0992
2025-12-31 21:58:01 - INFO - PRINT: Epoch [1831], Train Loss: 0.1004, Validation Loss: 0.0992
2025-12-31 21:58:27 - INFO - PRINT: Epoch [1832], Train Loss: 0.1015, Validation Loss: 0.0992
2025-12-31 21:58:54 - INFO - PRINT: Epoch [1833], Train Loss: 0.1010, Validation Loss: 0.0992
2025-12-31 21:59:21 - INFO - PRINT: Epoch [1834], Train Loss: 0.1014, Validation Loss: 0.0992
2025-12-31 21:59:48 - INFO - PRINT: Epoch [1835], Train Loss: 0.1012, Validation Loss: 0.0992
2025-12-31 22:00:14 - INFO - PRINT: Epoch [1836], Train Loss: 0.1013, Validation Loss: 0.0992
2025-12-31 22:00:41 - INFO - PRINT: Epoch [1837], Train Loss: 0.1016, Validation Loss: 0.0992
2025-12-31 22:01:08 - INFO - PRINT: Epoch [1838], Train Loss: 0.1010, Validation Loss: 0.0992
2025-12-31 22:01:34 - INFO - PRINT: Epoch [1839], Train Loss: 0.1010, Validation Loss: 0.0992
2025-12-31 22:02:05 - INFO - PRINT: Epoch [1840], Train Loss: 0.1009, Validation Loss: 0.0993
2025-12-31 22:02:32 - INFO - PRINT: Epoch [1841], Train Loss: 0.1013, Validation Loss: 0.0993
2025-12-31 22:02:59 - INFO - PRINT: Epoch [1842], Train Loss: 0.1006, Validation Loss: 0.0993
2025-12-31 22:03:25 - INFO - PRINT: Epoch [1843], Train Loss: 0.1011, Validation Loss: 0.0993
2025-12-31 22:03:52 - INFO - PRINT: Epoch [1844], Train Loss: 0.1025, Validation Loss: 0.0993
2025-12-31 22:04:19 - INFO - PRINT: Epoch [1845], Train Loss: 0.1019, Validation Loss: 0.0993
2025-12-31 22:04:45 - INFO - PRINT: Epoch [1846], Train Loss: 0.1012, Validation Loss: 0.0993
2025-12-31 22:05:12 - INFO - PRINT: Epoch [1847], Train Loss: 0.1006, Validation Loss: 0.0993
2025-12-31 22:05:39 - INFO - PRINT: Epoch [1848], Train Loss: 0.1010, Validation Loss: 0.0993
2025-12-31 22:06:06 - INFO - PRINT: Epoch [1849], Train Loss: 0.1009, Validation Loss: 0.0993
2025-12-31 22:06:32 - INFO - PRINT: Epoch [1850], Train Loss: 0.1009, Validation Loss: 0.0993
2025-12-31 22:06:59 - INFO - PRINT: Epoch [1851], Train Loss: 0.1013, Validation Loss: 0.0993
2025-12-31 22:07:26 - INFO - PRINT: Epoch [1852], Train Loss: 0.1015, Validation Loss: 0.0993
2025-12-31 22:07:52 - INFO - PRINT: Epoch [1853], Train Loss: 0.1016, Validation Loss: 0.0993
2025-12-31 22:08:19 - INFO - PRINT: Epoch [1854], Train Loss: 0.1013, Validation Loss: 0.0993
2025-12-31 22:08:46 - INFO - PRINT: Epoch [1855], Train Loss: 0.1007, Validation Loss: 0.0993
2025-12-31 22:09:13 - INFO - PRINT: Epoch [1856], Train Loss: 0.1011, Validation Loss: 0.0993
2025-12-31 22:09:39 - INFO - PRINT: Epoch [1857], Train Loss: 0.1007, Validation Loss: 0.0993
2025-12-31 22:10:06 - INFO - PRINT: Epoch [1858], Train Loss: 0.1005, Validation Loss: 0.0993
2025-12-31 22:10:33 - INFO - PRINT: Epoch [1859], Train Loss: 0.1006, Validation Loss: 0.0993
2025-12-31 22:11:04 - INFO - PRINT: Epoch [1860], Train Loss: 0.1015, Validation Loss: 0.1007
2025-12-31 22:11:30 - INFO - PRINT: Epoch [1861], Train Loss: 0.1010, Validation Loss: 0.1007
2025-12-31 22:11:57 - INFO - PRINT: Epoch [1862], Train Loss: 0.1012, Validation Loss: 0.1007
2025-12-31 22:12:24 - INFO - PRINT: Epoch [1863], Train Loss: 0.1017, Validation Loss: 0.1007
2025-12-31 22:12:50 - INFO - PRINT: Epoch [1864], Train Loss: 0.1020, Validation Loss: 0.1007
2025-12-31 22:13:17 - INFO - PRINT: Epoch [1865], Train Loss: 0.1012, Validation Loss: 0.1007
2025-12-31 22:13:44 - INFO - PRINT: Epoch [1866], Train Loss: 0.1009, Validation Loss: 0.1007
2025-12-31 22:14:10 - INFO - PRINT: Epoch [1867], Train Loss: 0.1012, Validation Loss: 0.1007
2025-12-31 22:14:37 - INFO - PRINT: Epoch [1868], Train Loss: 0.1009, Validation Loss: 0.1007
2025-12-31 22:15:04 - INFO - PRINT: Epoch [1869], Train Loss: 0.1011, Validation Loss: 0.1007
2025-12-31 22:15:31 - INFO - PRINT: Epoch [1870], Train Loss: 0.1008, Validation Loss: 0.1007
2025-12-31 22:15:57 - INFO - PRINT: Epoch [1871], Train Loss: 0.1016, Validation Loss: 0.1007
2025-12-31 22:16:24 - INFO - PRINT: Epoch [1872], Train Loss: 0.1013, Validation Loss: 0.1007
2025-12-31 22:16:51 - INFO - PRINT: Epoch [1873], Train Loss: 0.1013, Validation Loss: 0.1007
2025-12-31 22:17:17 - INFO - PRINT: Epoch [1874], Train Loss: 0.1009, Validation Loss: 0.1007
2025-12-31 22:17:44 - INFO - PRINT: Epoch [1875], Train Loss: 0.1011, Validation Loss: 0.1007
2025-12-31 22:18:11 - INFO - PRINT: Epoch [1876], Train Loss: 0.1009, Validation Loss: 0.1007
2025-12-31 22:18:38 - INFO - PRINT: Epoch [1877], Train Loss: 0.1007, Validation Loss: 0.1007
2025-12-31 22:19:04 - INFO - PRINT: Epoch [1878], Train Loss: 0.1023, Validation Loss: 0.1007
2025-12-31 22:19:31 - INFO - PRINT: Epoch [1879], Train Loss: 0.1004, Validation Loss: 0.1007
2025-12-31 22:20:02 - INFO - PRINT: Epoch [1880], Train Loss: 0.1014, Validation Loss: 0.0997
2025-12-31 22:20:28 - INFO - PRINT: Epoch [1881], Train Loss: 0.1011, Validation Loss: 0.0997
2025-12-31 22:20:55 - INFO - PRINT: Epoch [1882], Train Loss: 0.1009, Validation Loss: 0.0997
2025-12-31 22:21:22 - INFO - PRINT: Epoch [1883], Train Loss: 0.1014, Validation Loss: 0.0997
2025-12-31 22:21:49 - INFO - PRINT: Epoch [1884], Train Loss: 0.1009, Validation Loss: 0.0997
2025-12-31 22:22:15 - INFO - PRINT: Epoch [1885], Train Loss: 0.1003, Validation Loss: 0.0997
2025-12-31 22:22:42 - INFO - PRINT: Epoch [1886], Train Loss: 0.1020, Validation Loss: 0.0997
2025-12-31 22:23:09 - INFO - PRINT: Epoch [1887], Train Loss: 0.1006, Validation Loss: 0.0997
2025-12-31 22:23:35 - INFO - PRINT: Epoch [1888], Train Loss: 0.1011, Validation Loss: 0.0997
2025-12-31 22:24:02 - INFO - PRINT: Epoch [1889], Train Loss: 0.1007, Validation Loss: 0.0997
2025-12-31 22:24:29 - INFO - PRINT: Epoch [1890], Train Loss: 0.1002, Validation Loss: 0.0997
2025-12-31 22:24:56 - INFO - PRINT: Epoch [1891], Train Loss: 0.1012, Validation Loss: 0.0997
2025-12-31 22:25:22 - INFO - PRINT: Epoch [1892], Train Loss: 0.1012, Validation Loss: 0.0997
2025-12-31 22:25:49 - INFO - PRINT: Epoch [1893], Train Loss: 0.1015, Validation Loss: 0.0997
2025-12-31 22:26:16 - INFO - PRINT: Epoch [1894], Train Loss: 0.1016, Validation Loss: 0.0997
2025-12-31 22:26:42 - INFO - PRINT: Epoch [1895], Train Loss: 0.1002, Validation Loss: 0.0997
2025-12-31 22:27:09 - INFO - PRINT: Epoch [1896], Train Loss: 0.1009, Validation Loss: 0.0997
2025-12-31 22:27:36 - INFO - PRINT: Epoch [1897], Train Loss: 0.1010, Validation Loss: 0.0997
2025-12-31 22:28:03 - INFO - PRINT: Epoch [1898], Train Loss: 0.1010, Validation Loss: 0.0997
2025-12-31 22:28:29 - INFO - PRINT: Epoch [1899], Train Loss: 0.1006, Validation Loss: 0.0997
2025-12-31 22:29:00 - INFO - PRINT: Epoch [1900], Train Loss: 0.1012, Validation Loss: 0.0985
2025-12-31 22:29:00 - INFO - PRINT: ----> Saving model from epoch 1540 (val loss: 0.09834834948182106). Vibrant!
2025-12-31 22:29:27 - INFO - PRINT: Epoch [1901], Train Loss: 0.1008, Validation Loss: 0.0985
2025-12-31 22:29:53 - INFO - PRINT: Epoch [1902], Train Loss: 0.1008, Validation Loss: 0.0985
2025-12-31 22:30:20 - INFO - PRINT: Epoch [1903], Train Loss: 0.1018, Validation Loss: 0.0985
2025-12-31 22:30:47 - INFO - PRINT: Epoch [1904], Train Loss: 0.1010, Validation Loss: 0.0985
2025-12-31 22:31:14 - INFO - PRINT: Epoch [1905], Train Loss: 0.1008, Validation Loss: 0.0985
2025-12-31 22:31:40 - INFO - PRINT: Epoch [1906], Train Loss: 0.1011, Validation Loss: 0.0985
2025-12-31 22:32:07 - INFO - PRINT: Epoch [1907], Train Loss: 0.1012, Validation Loss: 0.0985
2025-12-31 22:32:34 - INFO - PRINT: Epoch [1908], Train Loss: 0.1008, Validation Loss: 0.0985
2025-12-31 22:33:01 - INFO - PRINT: Epoch [1909], Train Loss: 0.1006, Validation Loss: 0.0985
2025-12-31 22:33:27 - INFO - PRINT: Epoch [1910], Train Loss: 0.1020, Validation Loss: 0.0985
2025-12-31 22:33:54 - INFO - PRINT: Epoch [1911], Train Loss: 0.1013, Validation Loss: 0.0985
2025-12-31 22:34:21 - INFO - PRINT: Epoch [1912], Train Loss: 0.1006, Validation Loss: 0.0985
2025-12-31 22:34:47 - INFO - PRINT: Epoch [1913], Train Loss: 0.1006, Validation Loss: 0.0985
2025-12-31 22:35:14 - INFO - PRINT: Epoch [1914], Train Loss: 0.1009, Validation Loss: 0.0985
2025-12-31 22:35:41 - INFO - PRINT: Epoch [1915], Train Loss: 0.1016, Validation Loss: 0.0985
2025-12-31 22:36:08 - INFO - PRINT: Epoch [1916], Train Loss: 0.1013, Validation Loss: 0.0985
2025-12-31 22:36:34 - INFO - PRINT: Epoch [1917], Train Loss: 0.1005, Validation Loss: 0.0985
2025-12-31 22:37:01 - INFO - PRINT: Epoch [1918], Train Loss: 0.1012, Validation Loss: 0.0985
2025-12-31 22:37:28 - INFO - PRINT: Epoch [1919], Train Loss: 0.1011, Validation Loss: 0.0985
2025-12-31 22:37:59 - INFO - PRINT: Epoch [1920], Train Loss: 0.1018, Validation Loss: 0.0992
2025-12-31 22:38:25 - INFO - PRINT: Epoch [1921], Train Loss: 0.1012, Validation Loss: 0.0992
2025-12-31 22:38:52 - INFO - PRINT: Epoch [1922], Train Loss: 0.1010, Validation Loss: 0.0992
2025-12-31 22:39:19 - INFO - PRINT: Epoch [1923], Train Loss: 0.1025, Validation Loss: 0.0992
2025-12-31 22:39:45 - INFO - PRINT: Epoch [1924], Train Loss: 0.1010, Validation Loss: 0.0992
2025-12-31 22:40:12 - INFO - PRINT: Epoch [1925], Train Loss: 0.1003, Validation Loss: 0.0992
2025-12-31 22:40:39 - INFO - PRINT: Epoch [1926], Train Loss: 0.1007, Validation Loss: 0.0992
2025-12-31 22:41:05 - INFO - PRINT: Epoch [1927], Train Loss: 0.1009, Validation Loss: 0.0992
2025-12-31 22:41:32 - INFO - PRINT: Epoch [1928], Train Loss: 0.1005, Validation Loss: 0.0992
2025-12-31 22:41:59 - INFO - PRINT: Epoch [1929], Train Loss: 0.1002, Validation Loss: 0.0992
2025-12-31 22:42:26 - INFO - PRINT: Epoch [1930], Train Loss: 0.1013, Validation Loss: 0.0992
2025-12-31 22:42:52 - INFO - PRINT: Epoch [1931], Train Loss: 0.1003, Validation Loss: 0.0992
2025-12-31 22:43:19 - INFO - PRINT: Epoch [1932], Train Loss: 0.1010, Validation Loss: 0.0992
2025-12-31 22:43:46 - INFO - PRINT: Epoch [1933], Train Loss: 0.1015, Validation Loss: 0.0992
2025-12-31 22:44:12 - INFO - PRINT: Epoch [1934], Train Loss: 0.1009, Validation Loss: 0.0992
2025-12-31 22:44:39 - INFO - PRINT: Epoch [1935], Train Loss: 0.1008, Validation Loss: 0.0992
2025-12-31 22:45:06 - INFO - PRINT: Epoch [1936], Train Loss: 0.1022, Validation Loss: 0.0992
2025-12-31 22:45:33 - INFO - PRINT: Epoch [1937], Train Loss: 0.1010, Validation Loss: 0.0992
2025-12-31 22:45:59 - INFO - PRINT: Epoch [1938], Train Loss: 0.1005, Validation Loss: 0.0992
2025-12-31 22:46:26 - INFO - PRINT: Epoch [1939], Train Loss: 0.1005, Validation Loss: 0.0992
2025-12-31 22:46:57 - INFO - PRINT: Epoch [1940], Train Loss: 0.1016, Validation Loss: 0.1036
2025-12-31 22:47:23 - INFO - PRINT: Epoch [1941], Train Loss: 0.1004, Validation Loss: 0.1036
2025-12-31 22:47:50 - INFO - PRINT: Epoch [1942], Train Loss: 0.1003, Validation Loss: 0.1036
2025-12-31 22:48:17 - INFO - PRINT: Epoch [1943], Train Loss: 0.1008, Validation Loss: 0.1036
2025-12-31 22:48:44 - INFO - PRINT: Epoch [1944], Train Loss: 0.1011, Validation Loss: 0.1036
2025-12-31 22:49:10 - INFO - PRINT: Epoch [1945], Train Loss: 0.1009, Validation Loss: 0.1036
2025-12-31 22:49:37 - INFO - PRINT: Epoch [1946], Train Loss: 0.1014, Validation Loss: 0.1036
2025-12-31 22:50:04 - INFO - PRINT: Epoch [1947], Train Loss: 0.1007, Validation Loss: 0.1036
2025-12-31 22:50:30 - INFO - PRINT: Epoch [1948], Train Loss: 0.1009, Validation Loss: 0.1036
2025-12-31 22:50:57 - INFO - PRINT: Epoch [1949], Train Loss: 0.1015, Validation Loss: 0.1036
2025-12-31 22:51:24 - INFO - PRINT: Epoch [1950], Train Loss: 0.1023, Validation Loss: 0.1036
2025-12-31 22:51:51 - INFO - PRINT: Epoch [1951], Train Loss: 0.1009, Validation Loss: 0.1036
2025-12-31 22:52:17 - INFO - PRINT: Epoch [1952], Train Loss: 0.1012, Validation Loss: 0.1036
2025-12-31 22:52:44 - INFO - PRINT: Epoch [1953], Train Loss: 0.1015, Validation Loss: 0.1036
2025-12-31 22:53:11 - INFO - PRINT: Epoch [1954], Train Loss: 0.1007, Validation Loss: 0.1036
2025-12-31 22:53:37 - INFO - PRINT: Epoch [1955], Train Loss: 0.1015, Validation Loss: 0.1036
2025-12-31 22:54:04 - INFO - PRINT: Epoch [1956], Train Loss: 0.1010, Validation Loss: 0.1036
2025-12-31 22:54:31 - INFO - PRINT: Epoch [1957], Train Loss: 0.1001, Validation Loss: 0.1036
2025-12-31 22:54:58 - INFO - PRINT: Epoch [1958], Train Loss: 0.1005, Validation Loss: 0.1036
2025-12-31 22:55:24 - INFO - PRINT: Epoch [1959], Train Loss: 0.1007, Validation Loss: 0.1036
2025-12-31 22:55:55 - INFO - PRINT: Epoch [1960], Train Loss: 0.1011, Validation Loss: 0.0996
2025-12-31 22:56:22 - INFO - PRINT: Epoch [1961], Train Loss: 0.1007, Validation Loss: 0.0996
2025-12-31 22:56:49 - INFO - PRINT: Epoch [1962], Train Loss: 0.1005, Validation Loss: 0.0996
2025-12-31 22:57:15 - INFO - PRINT: Epoch [1963], Train Loss: 0.1015, Validation Loss: 0.0996
2025-12-31 22:57:42 - INFO - PRINT: Epoch [1964], Train Loss: 0.1007, Validation Loss: 0.0996
2025-12-31 22:58:09 - INFO - PRINT: Epoch [1965], Train Loss: 0.1004, Validation Loss: 0.0996
2025-12-31 22:58:35 - INFO - PRINT: Epoch [1966], Train Loss: 0.1007, Validation Loss: 0.0996
2025-12-31 22:59:02 - INFO - PRINT: Epoch [1967], Train Loss: 0.1012, Validation Loss: 0.0996
2025-12-31 22:59:29 - INFO - PRINT: Epoch [1968], Train Loss: 0.1017, Validation Loss: 0.0996
2025-12-31 22:59:55 - INFO - PRINT: Epoch [1969], Train Loss: 0.1015, Validation Loss: 0.0996
2025-12-31 23:00:22 - INFO - PRINT: Epoch [1970], Train Loss: 0.1005, Validation Loss: 0.0996
2025-12-31 23:00:49 - INFO - PRINT: Epoch [1971], Train Loss: 0.1010, Validation Loss: 0.0996
2025-12-31 23:01:16 - INFO - PRINT: Epoch [1972], Train Loss: 0.1003, Validation Loss: 0.0996
2025-12-31 23:01:42 - INFO - PRINT: Epoch [1973], Train Loss: 0.1007, Validation Loss: 0.0996
2025-12-31 23:02:09 - INFO - PRINT: Epoch [1974], Train Loss: 0.1012, Validation Loss: 0.0996
2025-12-31 23:02:36 - INFO - PRINT: Epoch [1975], Train Loss: 0.1011, Validation Loss: 0.0996
2025-12-31 23:03:02 - INFO - PRINT: Epoch [1976], Train Loss: 0.1009, Validation Loss: 0.0996
2025-12-31 23:03:29 - INFO - PRINT: Epoch [1977], Train Loss: 0.1016, Validation Loss: 0.0996
2025-12-31 23:03:56 - INFO - PRINT: Epoch [1978], Train Loss: 0.1010, Validation Loss: 0.0996
2025-12-31 23:04:23 - INFO - PRINT: Epoch [1979], Train Loss: 0.1014, Validation Loss: 0.0996
2025-12-31 23:04:54 - INFO - PRINT: Epoch [1980], Train Loss: 0.1008, Validation Loss: 0.0989
2025-12-31 23:05:20 - INFO - PRINT: Epoch [1981], Train Loss: 0.1005, Validation Loss: 0.0989
2025-12-31 23:05:47 - INFO - PRINT: Epoch [1982], Train Loss: 0.1007, Validation Loss: 0.0989
2025-12-31 23:06:14 - INFO - PRINT: Epoch [1983], Train Loss: 0.1012, Validation Loss: 0.0989
2025-12-31 23:06:40 - INFO - PRINT: Epoch [1984], Train Loss: 0.1007, Validation Loss: 0.0989
2025-12-31 23:07:07 - INFO - PRINT: Epoch [1985], Train Loss: 0.1007, Validation Loss: 0.0989
2025-12-31 23:07:34 - INFO - PRINT: Epoch [1986], Train Loss: 0.1005, Validation Loss: 0.0989
2025-12-31 23:08:00 - INFO - PRINT: Epoch [1987], Train Loss: 0.1006, Validation Loss: 0.0989
2025-12-31 23:08:27 - INFO - PRINT: Epoch [1988], Train Loss: 0.1009, Validation Loss: 0.0989
2025-12-31 23:08:54 - INFO - PRINT: Epoch [1989], Train Loss: 0.1021, Validation Loss: 0.0989
2025-12-31 23:09:21 - INFO - PRINT: Epoch [1990], Train Loss: 0.1014, Validation Loss: 0.0989
2025-12-31 23:09:47 - INFO - PRINT: Epoch [1991], Train Loss: 0.1001, Validation Loss: 0.0989
2025-12-31 23:10:14 - INFO - PRINT: Epoch [1992], Train Loss: 0.1011, Validation Loss: 0.0989
2025-12-31 23:10:41 - INFO - PRINT: Epoch [1993], Train Loss: 0.1007, Validation Loss: 0.0989
2025-12-31 23:11:07 - INFO - PRINT: Epoch [1994], Train Loss: 0.1015, Validation Loss: 0.0989
2025-12-31 23:11:34 - INFO - PRINT: Epoch [1995], Train Loss: 0.1027, Validation Loss: 0.0989
2025-12-31 23:12:01 - INFO - PRINT: Epoch [1996], Train Loss: 0.1011, Validation Loss: 0.0989
2025-12-31 23:12:27 - INFO - PRINT: Epoch [1997], Train Loss: 0.0998, Validation Loss: 0.0989
2025-12-31 23:12:54 - INFO - PRINT: Epoch [1998], Train Loss: 0.1003, Validation Loss: 0.0989
2025-12-31 23:13:21 - INFO - PRINT: Epoch [1999], Train Loss: 0.1001, Validation Loss: 0.0989
2025-12-31 23:13:52 - INFO - PRINT: Epoch [2000], Train Loss: 0.1008, Validation Loss: 0.0985
2025-12-31 23:13:52 - INFO - PRINT: ----> Saving model from epoch 1540 (val loss: 0.09834834948182106). Smooth!
2025-12-31 23:14:18 - INFO - PRINT: Epoch [2001], Train Loss: 0.1008, Validation Loss: 0.0985
2025-12-31 23:14:45 - INFO - PRINT: Epoch [2002], Train Loss: 0.1008, Validation Loss: 0.0985
2025-12-31 23:15:12 - INFO - PRINT: Epoch [2003], Train Loss: 0.1006, Validation Loss: 0.0985
2025-12-31 23:15:39 - INFO - PRINT: Epoch [2004], Train Loss: 0.1008, Validation Loss: 0.0985
2025-12-31 23:16:05 - INFO - PRINT: Epoch [2005], Train Loss: 0.1012, Validation Loss: 0.0985
2025-12-31 23:16:32 - INFO - PRINT: Epoch [2006], Train Loss: 0.1018, Validation Loss: 0.0985
2025-12-31 23:16:59 - INFO - PRINT: Epoch [2007], Train Loss: 0.1005, Validation Loss: 0.0985
2025-12-31 23:17:25 - INFO - PRINT: Epoch [2008], Train Loss: 0.1025, Validation Loss: 0.0985
2025-12-31 23:17:52 - INFO - PRINT: Epoch [2009], Train Loss: 0.1015, Validation Loss: 0.0985
2025-12-31 23:18:19 - INFO - PRINT: Epoch [2010], Train Loss: 0.1012, Validation Loss: 0.0985
2025-12-31 23:18:45 - INFO - PRINT: Epoch [2011], Train Loss: 0.1008, Validation Loss: 0.0985
2025-12-31 23:19:12 - INFO - PRINT: Epoch [2012], Train Loss: 0.1012, Validation Loss: 0.0985
2025-12-31 23:19:39 - INFO - PRINT: Epoch [2013], Train Loss: 0.1010, Validation Loss: 0.0985
2025-12-31 23:20:06 - INFO - PRINT: Epoch [2014], Train Loss: 0.1011, Validation Loss: 0.0985
2025-12-31 23:20:32 - INFO - PRINT: Epoch [2015], Train Loss: 0.1008, Validation Loss: 0.0985
2025-12-31 23:20:59 - INFO - PRINT: Epoch [2016], Train Loss: 0.1010, Validation Loss: 0.0985
2025-12-31 23:21:26 - INFO - PRINT: Epoch [2017], Train Loss: 0.1010, Validation Loss: 0.0985
2025-12-31 23:21:52 - INFO - PRINT: Epoch [2018], Train Loss: 0.1012, Validation Loss: 0.0985
2025-12-31 23:22:19 - INFO - PRINT: Epoch [2019], Train Loss: 0.1008, Validation Loss: 0.0985
2025-12-31 23:22:50 - INFO - PRINT: Epoch [2020], Train Loss: 0.1007, Validation Loss: 0.0981
2025-12-31 23:23:17 - INFO - PRINT: Epoch [2021], Train Loss: 0.1003, Validation Loss: 0.0981
2025-12-31 23:23:43 - INFO - PRINT: Epoch [2022], Train Loss: 0.1013, Validation Loss: 0.0981
2025-12-31 23:24:10 - INFO - PRINT: Epoch [2023], Train Loss: 0.1015, Validation Loss: 0.0981
2025-12-31 23:24:37 - INFO - PRINT: Epoch [2024], Train Loss: 0.1010, Validation Loss: 0.0981
2025-12-31 23:25:04 - INFO - PRINT: Epoch [2025], Train Loss: 0.1005, Validation Loss: 0.0981
2025-12-31 23:25:30 - INFO - PRINT: Epoch [2026], Train Loss: 0.1015, Validation Loss: 0.0981
2025-12-31 23:25:57 - INFO - PRINT: Epoch [2027], Train Loss: 0.1011, Validation Loss: 0.0981
2025-12-31 23:26:24 - INFO - PRINT: Epoch [2028], Train Loss: 0.1003, Validation Loss: 0.0981
2025-12-31 23:26:50 - INFO - PRINT: Epoch [2029], Train Loss: 0.1009, Validation Loss: 0.0981
2025-12-31 23:27:17 - INFO - PRINT: Epoch [2030], Train Loss: 0.1012, Validation Loss: 0.0981
2025-12-31 23:27:44 - INFO - PRINT: Epoch [2031], Train Loss: 0.1004, Validation Loss: 0.0981
2025-12-31 23:28:11 - INFO - PRINT: Epoch [2032], Train Loss: 0.1004, Validation Loss: 0.0981
2025-12-31 23:28:37 - INFO - PRINT: Epoch [2033], Train Loss: 0.1010, Validation Loss: 0.0981
2025-12-31 23:29:04 - INFO - PRINT: Epoch [2034], Train Loss: 0.1005, Validation Loss: 0.0981
2025-12-31 23:29:31 - INFO - PRINT: Epoch [2035], Train Loss: 0.1007, Validation Loss: 0.0981
2025-12-31 23:29:57 - INFO - PRINT: Epoch [2036], Train Loss: 0.1012, Validation Loss: 0.0981
2025-12-31 23:30:24 - INFO - PRINT: Epoch [2037], Train Loss: 0.1009, Validation Loss: 0.0981
2025-12-31 23:30:51 - INFO - PRINT: Epoch [2038], Train Loss: 0.1003, Validation Loss: 0.0981
2025-12-31 23:31:18 - INFO - PRINT: Epoch [2039], Train Loss: 0.1005, Validation Loss: 0.0981
2025-12-31 23:31:49 - INFO - PRINT: Epoch [2040], Train Loss: 0.1012, Validation Loss: 0.0994
2025-12-31 23:32:15 - INFO - PRINT: Epoch [2041], Train Loss: 0.1003, Validation Loss: 0.0994
2025-12-31 23:32:42 - INFO - PRINT: Epoch [2042], Train Loss: 0.1007, Validation Loss: 0.0994
2025-12-31 23:33:08 - INFO - PRINT: Epoch [2043], Train Loss: 0.1020, Validation Loss: 0.0994
2025-12-31 23:33:35 - INFO - PRINT: Epoch [2044], Train Loss: 0.1020, Validation Loss: 0.0994
2025-12-31 23:34:02 - INFO - PRINT: Epoch [2045], Train Loss: 0.1008, Validation Loss: 0.0994
2025-12-31 23:34:29 - INFO - PRINT: Epoch [2046], Train Loss: 0.1005, Validation Loss: 0.0994
2025-12-31 23:34:55 - INFO - PRINT: Epoch [2047], Train Loss: 0.0999, Validation Loss: 0.0994
2025-12-31 23:35:22 - INFO - PRINT: Epoch [2048], Train Loss: 0.1007, Validation Loss: 0.0994
2025-12-31 23:35:49 - INFO - PRINT: Epoch [2049], Train Loss: 0.1017, Validation Loss: 0.0994
2025-12-31 23:36:15 - INFO - PRINT: Epoch [2050], Train Loss: 0.1009, Validation Loss: 0.0994
2025-12-31 23:36:42 - INFO - PRINT: Epoch [2051], Train Loss: 0.1015, Validation Loss: 0.0994
2025-12-31 23:37:09 - INFO - PRINT: Epoch [2052], Train Loss: 0.1004, Validation Loss: 0.0994
2025-12-31 23:37:36 - INFO - PRINT: Epoch [2053], Train Loss: 0.1012, Validation Loss: 0.0994
2025-12-31 23:38:02 - INFO - PRINT: Epoch [2054], Train Loss: 0.1009, Validation Loss: 0.0994
2025-12-31 23:38:29 - INFO - PRINT: Epoch [2055], Train Loss: 0.0999, Validation Loss: 0.0994
2025-12-31 23:38:56 - INFO - PRINT: Epoch [2056], Train Loss: 0.1009, Validation Loss: 0.0994
2025-12-31 23:39:22 - INFO - PRINT: Epoch [2057], Train Loss: 0.1010, Validation Loss: 0.0994
2025-12-31 23:39:49 - INFO - PRINT: Epoch [2058], Train Loss: 0.1017, Validation Loss: 0.0994
2025-12-31 23:40:16 - INFO - PRINT: Epoch [2059], Train Loss: 0.1006, Validation Loss: 0.0994
2025-12-31 23:40:47 - INFO - PRINT: Epoch [2060], Train Loss: 0.1005, Validation Loss: 0.0981
2025-12-31 23:41:13 - INFO - PRINT: Epoch [2061], Train Loss: 0.1008, Validation Loss: 0.0981
2025-12-31 23:41:40 - INFO - PRINT: Epoch [2062], Train Loss: 0.1013, Validation Loss: 0.0981
2025-12-31 23:42:07 - INFO - PRINT: Epoch [2063], Train Loss: 0.1010, Validation Loss: 0.0981
2025-12-31 23:42:33 - INFO - PRINT: Epoch [2064], Train Loss: 0.1008, Validation Loss: 0.0981
2025-12-31 23:43:00 - INFO - PRINT: Epoch [2065], Train Loss: 0.1007, Validation Loss: 0.0981
2025-12-31 23:43:27 - INFO - PRINT: Epoch [2066], Train Loss: 0.1003, Validation Loss: 0.0981
2025-12-31 23:43:54 - INFO - PRINT: Epoch [2067], Train Loss: 0.1010, Validation Loss: 0.0981
2025-12-31 23:44:20 - INFO - PRINT: Epoch [2068], Train Loss: 0.1018, Validation Loss: 0.0981
2025-12-31 23:44:47 - INFO - PRINT: Epoch [2069], Train Loss: 0.1008, Validation Loss: 0.0981
2025-12-31 23:45:14 - INFO - PRINT: Epoch [2070], Train Loss: 0.1003, Validation Loss: 0.0981
2025-12-31 23:45:40 - INFO - PRINT: Epoch [2071], Train Loss: 0.1006, Validation Loss: 0.0981
2025-12-31 23:46:07 - INFO - PRINT: Epoch [2072], Train Loss: 0.1005, Validation Loss: 0.0981
2025-12-31 23:46:34 - INFO - PRINT: Epoch [2073], Train Loss: 0.1005, Validation Loss: 0.0981
2025-12-31 23:47:01 - INFO - PRINT: Epoch [2074], Train Loss: 0.1010, Validation Loss: 0.0981
2025-12-31 23:47:27 - INFO - PRINT: Epoch [2075], Train Loss: 0.1005, Validation Loss: 0.0981
2025-12-31 23:47:54 - INFO - PRINT: Epoch [2076], Train Loss: 0.1012, Validation Loss: 0.0981
2025-12-31 23:48:21 - INFO - PRINT: Epoch [2077], Train Loss: 0.1015, Validation Loss: 0.0981
2025-12-31 23:48:47 - INFO - PRINT: Epoch [2078], Train Loss: 0.1004, Validation Loss: 0.0981
2025-12-31 23:49:14 - INFO - PRINT: Epoch [2079], Train Loss: 0.1006, Validation Loss: 0.0981
2025-12-31 23:49:45 - INFO - PRINT: Epoch [2080], Train Loss: 0.1006, Validation Loss: 0.0988
2025-12-31 23:50:12 - INFO - PRINT: Epoch [2081], Train Loss: 0.1007, Validation Loss: 0.0988
2025-12-31 23:50:38 - INFO - PRINT: Epoch [2082], Train Loss: 0.1013, Validation Loss: 0.0988
2025-12-31 23:51:05 - INFO - PRINT: Epoch [2083], Train Loss: 0.1011, Validation Loss: 0.0988
2025-12-31 23:51:32 - INFO - PRINT: Epoch [2084], Train Loss: 0.1000, Validation Loss: 0.0988
2025-12-31 23:51:58 - INFO - PRINT: Epoch [2085], Train Loss: 0.1012, Validation Loss: 0.0988
2025-12-31 23:52:25 - INFO - PRINT: Epoch [2086], Train Loss: 0.1005, Validation Loss: 0.0988
2025-12-31 23:52:52 - INFO - PRINT: Epoch [2087], Train Loss: 0.1002, Validation Loss: 0.0988
2025-12-31 23:53:19 - INFO - PRINT: Epoch [2088], Train Loss: 0.1010, Validation Loss: 0.0988
2025-12-31 23:53:45 - INFO - PRINT: Epoch [2089], Train Loss: 0.1011, Validation Loss: 0.0988
2025-12-31 23:54:12 - INFO - PRINT: Epoch [2090], Train Loss: 0.1011, Validation Loss: 0.0988
2025-12-31 23:54:39 - INFO - PRINT: Epoch [2091], Train Loss: 0.1010, Validation Loss: 0.0988
2025-12-31 23:55:05 - INFO - PRINT: Epoch [2092], Train Loss: 0.1013, Validation Loss: 0.0988
2025-12-31 23:55:32 - INFO - PRINT: Epoch [2093], Train Loss: 0.1012, Validation Loss: 0.0988
2025-12-31 23:55:59 - INFO - PRINT: Epoch [2094], Train Loss: 0.1008, Validation Loss: 0.0988
2025-12-31 23:56:26 - INFO - PRINT: Epoch [2095], Train Loss: 0.1009, Validation Loss: 0.0988
2025-12-31 23:56:52 - INFO - PRINT: Epoch [2096], Train Loss: 0.1009, Validation Loss: 0.0988
2025-12-31 23:57:19 - INFO - PRINT: Epoch [2097], Train Loss: 0.1008, Validation Loss: 0.0988
2025-12-31 23:57:46 - INFO - PRINT: Epoch [2098], Train Loss: 0.1006, Validation Loss: 0.0988
2025-12-31 23:58:12 - INFO - PRINT: Epoch [2099], Train Loss: 0.0999, Validation Loss: 0.0988
2025-12-31 23:58:44 - INFO - PRINT: Epoch [2100], Train Loss: 0.1004, Validation Loss: 0.0988
2025-12-31 23:58:44 - INFO - PRINT: ----> Saving model from epoch 2060 (val loss: 0.09808043345808982). Umami-rich!
2025-12-31 23:59:10 - INFO - PRINT: Epoch [2101], Train Loss: 0.1009, Validation Loss: 0.0988
2025-12-31 23:59:37 - INFO - PRINT: Epoch [2102], Train Loss: 0.1005, Validation Loss: 0.0988
2026-01-01 00:00:03 - INFO - PRINT: Epoch [2103], Train Loss: 0.1011, Validation Loss: 0.0988
2026-01-01 00:00:30 - INFO - PRINT: Epoch [2104], Train Loss: 0.1011, Validation Loss: 0.0988
2026-01-01 00:00:57 - INFO - PRINT: Epoch [2105], Train Loss: 0.1011, Validation Loss: 0.0988
2026-01-01 00:01:24 - INFO - PRINT: Epoch [2106], Train Loss: 0.1015, Validation Loss: 0.0988
2026-01-01 00:01:50 - INFO - PRINT: Epoch [2107], Train Loss: 0.1008, Validation Loss: 0.0988
2026-01-01 00:02:17 - INFO - PRINT: Epoch [2108], Train Loss: 0.1006, Validation Loss: 0.0988
2026-01-01 00:02:44 - INFO - PRINT: Epoch [2109], Train Loss: 0.1003, Validation Loss: 0.0988
2026-01-01 00:03:10 - INFO - PRINT: Epoch [2110], Train Loss: 0.1005, Validation Loss: 0.0988
2026-01-01 00:03:37 - INFO - PRINT: Epoch [2111], Train Loss: 0.1004, Validation Loss: 0.0988
2026-01-01 00:04:04 - INFO - PRINT: Epoch [2112], Train Loss: 0.1008, Validation Loss: 0.0988
2026-01-01 00:04:31 - INFO - PRINT: Epoch [2113], Train Loss: 0.1018, Validation Loss: 0.0988
2026-01-01 00:04:57 - INFO - PRINT: Epoch [2114], Train Loss: 0.1009, Validation Loss: 0.0988
2026-01-01 00:05:24 - INFO - PRINT: Epoch [2115], Train Loss: 0.1002, Validation Loss: 0.0988
2026-01-01 00:05:51 - INFO - PRINT: Epoch [2116], Train Loss: 0.1001, Validation Loss: 0.0988
2026-01-01 00:06:17 - INFO - PRINT: Epoch [2117], Train Loss: 0.1006, Validation Loss: 0.0988
2026-01-01 00:06:44 - INFO - PRINT: Epoch [2118], Train Loss: 0.1008, Validation Loss: 0.0988
2026-01-01 00:07:11 - INFO - PRINT: Epoch [2119], Train Loss: 0.1017, Validation Loss: 0.0988
2026-01-01 00:07:42 - INFO - PRINT: Epoch [2120], Train Loss: 0.1004, Validation Loss: 0.0989
2026-01-01 00:08:08 - INFO - PRINT: Epoch [2121], Train Loss: 0.1011, Validation Loss: 0.0989
2026-01-01 00:08:35 - INFO - PRINT: Epoch [2122], Train Loss: 0.1002, Validation Loss: 0.0989
2026-01-01 00:09:02 - INFO - PRINT: Epoch [2123], Train Loss: 0.1005, Validation Loss: 0.0989
2026-01-01 00:09:28 - INFO - PRINT: Epoch [2124], Train Loss: 0.1012, Validation Loss: 0.0989
2026-01-01 00:09:55 - INFO - PRINT: Epoch [2125], Train Loss: 0.1006, Validation Loss: 0.0989
2026-01-01 00:10:22 - INFO - PRINT: Epoch [2126], Train Loss: 0.1005, Validation Loss: 0.0989
2026-01-01 00:10:49 - INFO - PRINT: Epoch [2127], Train Loss: 0.1007, Validation Loss: 0.0989
2026-01-01 00:11:15 - INFO - PRINT: Epoch [2128], Train Loss: 0.1008, Validation Loss: 0.0989
2026-01-01 00:11:42 - INFO - PRINT: Epoch [2129], Train Loss: 0.1008, Validation Loss: 0.0989
2026-01-01 00:12:09 - INFO - PRINT: Epoch [2130], Train Loss: 0.1008, Validation Loss: 0.0989
2026-01-01 00:12:35 - INFO - PRINT: Epoch [2131], Train Loss: 0.1021, Validation Loss: 0.0989
2026-01-01 00:13:02 - INFO - PRINT: Epoch [2132], Train Loss: 0.1010, Validation Loss: 0.0989
2026-01-01 00:13:29 - INFO - PRINT: Epoch [2133], Train Loss: 0.1007, Validation Loss: 0.0989
2026-01-01 00:13:56 - INFO - PRINT: Epoch [2134], Train Loss: 0.1014, Validation Loss: 0.0989
2026-01-01 00:14:22 - INFO - PRINT: Epoch [2135], Train Loss: 0.1006, Validation Loss: 0.0989
2026-01-01 00:14:49 - INFO - PRINT: Epoch [2136], Train Loss: 0.1004, Validation Loss: 0.0989
2026-01-01 00:15:16 - INFO - PRINT: Epoch [2137], Train Loss: 0.1003, Validation Loss: 0.0989
2026-01-01 00:15:42 - INFO - PRINT: Epoch [2138], Train Loss: 0.1003, Validation Loss: 0.0989
2026-01-01 00:16:09 - INFO - PRINT: Epoch [2139], Train Loss: 0.1015, Validation Loss: 0.0989
2026-01-01 00:16:40 - INFO - PRINT: Epoch [2140], Train Loss: 0.1015, Validation Loss: 0.1006
2026-01-01 00:17:07 - INFO - PRINT: Epoch [2141], Train Loss: 0.1007, Validation Loss: 0.1006
2026-01-01 00:17:33 - INFO - PRINT: Epoch [2142], Train Loss: 0.1006, Validation Loss: 0.1006
2026-01-01 00:18:00 - INFO - PRINT: Epoch [2143], Train Loss: 0.1017, Validation Loss: 0.1006
2026-01-01 00:18:27 - INFO - PRINT: Epoch [2144], Train Loss: 0.1007, Validation Loss: 0.1006
2026-01-01 00:18:53 - INFO - PRINT: Epoch [2145], Train Loss: 0.1013, Validation Loss: 0.1006
2026-01-01 00:19:20 - INFO - PRINT: Epoch [2146], Train Loss: 0.1001, Validation Loss: 0.1006
2026-01-01 00:19:47 - INFO - PRINT: Epoch [2147], Train Loss: 0.1009, Validation Loss: 0.1006
2026-01-01 00:20:14 - INFO - PRINT: Epoch [2148], Train Loss: 0.1015, Validation Loss: 0.1006
2026-01-01 00:20:40 - INFO - PRINT: Epoch [2149], Train Loss: 0.1006, Validation Loss: 0.1006
2026-01-01 00:21:07 - INFO - PRINT: Epoch [2150], Train Loss: 0.1008, Validation Loss: 0.1006
2026-01-01 00:21:34 - INFO - PRINT: Epoch [2151], Train Loss: 0.1013, Validation Loss: 0.1006
2026-01-01 00:22:00 - INFO - PRINT: Epoch [2152], Train Loss: 0.1005, Validation Loss: 0.1006
2026-01-01 00:22:27 - INFO - PRINT: Epoch [2153], Train Loss: 0.1017, Validation Loss: 0.1006
2026-01-01 00:22:54 - INFO - PRINT: Epoch [2154], Train Loss: 0.1012, Validation Loss: 0.1006
2026-01-01 00:23:21 - INFO - PRINT: Epoch [2155], Train Loss: 0.1004, Validation Loss: 0.1006
2026-01-01 00:23:47 - INFO - PRINT: Epoch [2156], Train Loss: 0.1003, Validation Loss: 0.1006
2026-01-01 00:24:14 - INFO - PRINT: Epoch [2157], Train Loss: 0.1005, Validation Loss: 0.1006
2026-01-01 00:24:41 - INFO - PRINT: Epoch [2158], Train Loss: 0.1003, Validation Loss: 0.1006
2026-01-01 00:25:07 - INFO - PRINT: Epoch [2159], Train Loss: 0.1008, Validation Loss: 0.1006
2026-01-01 00:25:38 - INFO - PRINT: Epoch [2160], Train Loss: 0.1004, Validation Loss: 0.0983
2026-01-01 00:26:05 - INFO - PRINT: Epoch [2161], Train Loss: 0.1001, Validation Loss: 0.0983
2026-01-01 00:26:32 - INFO - PRINT: Epoch [2162], Train Loss: 0.1015, Validation Loss: 0.0983
2026-01-01 00:26:58 - INFO - PRINT: Epoch [2163], Train Loss: 0.1014, Validation Loss: 0.0983
2026-01-01 00:27:25 - INFO - PRINT: Epoch [2164], Train Loss: 0.1019, Validation Loss: 0.0983
2026-01-01 00:27:52 - INFO - PRINT: Epoch [2165], Train Loss: 0.1008, Validation Loss: 0.0983
2026-01-01 00:28:18 - INFO - PRINT: Epoch [2166], Train Loss: 0.1002, Validation Loss: 0.0983
2026-01-01 00:28:45 - INFO - PRINT: Epoch [2167], Train Loss: 0.1021, Validation Loss: 0.0983
2026-01-01 00:29:12 - INFO - PRINT: Epoch [2168], Train Loss: 0.1013, Validation Loss: 0.0983
2026-01-01 00:29:39 - INFO - PRINT: Epoch [2169], Train Loss: 0.0996, Validation Loss: 0.0983
2026-01-01 00:30:05 - INFO - PRINT: Epoch [2170], Train Loss: 0.1003, Validation Loss: 0.0983
2026-01-01 00:30:32 - INFO - PRINT: Epoch [2171], Train Loss: 0.1007, Validation Loss: 0.0983
2026-01-01 00:30:59 - INFO - PRINT: Epoch [2172], Train Loss: 0.1011, Validation Loss: 0.0983
2026-01-01 00:31:25 - INFO - PRINT: Epoch [2173], Train Loss: 0.1019, Validation Loss: 0.0983
2026-01-01 00:31:52 - INFO - PRINT: Epoch [2174], Train Loss: 0.1010, Validation Loss: 0.0983
2026-01-01 00:32:19 - INFO - PRINT: Epoch [2175], Train Loss: 0.1007, Validation Loss: 0.0983
2026-01-01 00:32:45 - INFO - PRINT: Epoch [2176], Train Loss: 0.1012, Validation Loss: 0.0983
2026-01-01 00:33:12 - INFO - PRINT: Epoch [2177], Train Loss: 0.1004, Validation Loss: 0.0983
2026-01-01 00:33:39 - INFO - PRINT: Epoch [2178], Train Loss: 0.1007, Validation Loss: 0.0983
2026-01-01 00:34:06 - INFO - PRINT: Epoch [2179], Train Loss: 0.1008, Validation Loss: 0.0983
2026-01-01 00:34:37 - INFO - PRINT: Epoch [2180], Train Loss: 0.1007, Validation Loss: 0.1007
2026-01-01 00:35:03 - INFO - PRINT: Epoch [2181], Train Loss: 0.1006, Validation Loss: 0.1007
2026-01-01 00:35:30 - INFO - PRINT: Epoch [2182], Train Loss: 0.1006, Validation Loss: 0.1007
2026-01-01 00:35:57 - INFO - PRINT: Epoch [2183], Train Loss: 0.1009, Validation Loss: 0.1007
2026-01-01 00:36:23 - INFO - PRINT: Epoch [2184], Train Loss: 0.1017, Validation Loss: 0.1007
2026-01-01 00:36:50 - INFO - PRINT: Epoch [2185], Train Loss: 0.1004, Validation Loss: 0.1007
2026-01-01 00:37:17 - INFO - PRINT: Epoch [2186], Train Loss: 0.1010, Validation Loss: 0.1007
2026-01-01 00:37:43 - INFO - PRINT: Epoch [2187], Train Loss: 0.1004, Validation Loss: 0.1007
2026-01-01 00:38:10 - INFO - PRINT: Epoch [2188], Train Loss: 0.1012, Validation Loss: 0.1007
2026-01-01 00:38:37 - INFO - PRINT: Epoch [2189], Train Loss: 0.1001, Validation Loss: 0.1007
2026-01-01 00:39:04 - INFO - PRINT: Epoch [2190], Train Loss: 0.1002, Validation Loss: 0.1007
2026-01-01 00:39:30 - INFO - PRINT: Epoch [2191], Train Loss: 0.1006, Validation Loss: 0.1007
2026-01-01 00:39:57 - INFO - PRINT: Epoch [2192], Train Loss: 0.1012, Validation Loss: 0.1007
2026-01-01 00:40:24 - INFO - PRINT: Epoch [2193], Train Loss: 0.1004, Validation Loss: 0.1007
2026-01-01 00:40:50 - INFO - PRINT: Epoch [2194], Train Loss: 0.1005, Validation Loss: 0.1007
2026-01-01 00:41:17 - INFO - PRINT: Epoch [2195], Train Loss: 0.1013, Validation Loss: 0.1007
2026-01-01 00:41:44 - INFO - PRINT: Epoch [2196], Train Loss: 0.1003, Validation Loss: 0.1007
2026-01-01 00:42:11 - INFO - PRINT: Epoch [2197], Train Loss: 0.1008, Validation Loss: 0.1007
2026-01-01 00:42:37 - INFO - PRINT: Epoch [2198], Train Loss: 0.1005, Validation Loss: 0.1007
2026-01-01 00:43:04 - INFO - PRINT: Epoch [2199], Train Loss: 0.1015, Validation Loss: 0.1007
2026-01-01 00:43:35 - INFO - PRINT: Epoch [2200], Train Loss: 0.1009, Validation Loss: 0.0988
2026-01-01 00:43:35 - INFO - PRINT: ----> Saving model from epoch 2060 (val loss: 0.09808043345808982). Layered!
2026-01-01 00:44:02 - INFO - PRINT: Epoch [2201], Train Loss: 0.1016, Validation Loss: 0.0988
2026-01-01 00:44:28 - INFO - PRINT: Epoch [2202], Train Loss: 0.1014, Validation Loss: 0.0988
2026-01-01 00:44:55 - INFO - PRINT: Epoch [2203], Train Loss: 0.1003, Validation Loss: 0.0988
2026-01-01 00:45:22 - INFO - PRINT: Epoch [2204], Train Loss: 0.1003, Validation Loss: 0.0988
2026-01-01 00:45:48 - INFO - PRINT: Epoch [2205], Train Loss: 0.1000, Validation Loss: 0.0988
2026-01-01 00:46:15 - INFO - PRINT: Epoch [2206], Train Loss: 0.1002, Validation Loss: 0.0988
2026-01-01 00:46:42 - INFO - PRINT: Epoch [2207], Train Loss: 0.1013, Validation Loss: 0.0988
2026-01-01 00:47:09 - INFO - PRINT: Epoch [2208], Train Loss: 0.1016, Validation Loss: 0.0988
2026-01-01 00:47:35 - INFO - PRINT: Epoch [2209], Train Loss: 0.1006, Validation Loss: 0.0988
2026-01-01 00:48:02 - INFO - PRINT: Epoch [2210], Train Loss: 0.1000, Validation Loss: 0.0988
2026-01-01 00:48:29 - INFO - PRINT: Epoch [2211], Train Loss: 0.1003, Validation Loss: 0.0988
2026-01-01 00:48:55 - INFO - PRINT: Epoch [2212], Train Loss: 0.1009, Validation Loss: 0.0988
2026-01-01 00:49:22 - INFO - PRINT: Epoch [2213], Train Loss: 0.1011, Validation Loss: 0.0988
2026-01-01 00:49:49 - INFO - PRINT: Epoch [2214], Train Loss: 0.1008, Validation Loss: 0.0988
2026-01-01 00:50:16 - INFO - PRINT: Epoch [2215], Train Loss: 0.1011, Validation Loss: 0.0988
2026-01-01 00:50:42 - INFO - PRINT: Epoch [2216], Train Loss: 0.1003, Validation Loss: 0.0988
2026-01-01 00:51:09 - INFO - PRINT: Epoch [2217], Train Loss: 0.1010, Validation Loss: 0.0988
2026-01-01 00:51:36 - INFO - PRINT: Epoch [2218], Train Loss: 0.1009, Validation Loss: 0.0988
2026-01-01 00:52:02 - INFO - PRINT: Epoch [2219], Train Loss: 0.1002, Validation Loss: 0.0988
2026-01-01 00:52:33 - INFO - PRINT: Epoch [2220], Train Loss: 0.1009, Validation Loss: 0.1003
2026-01-01 00:53:00 - INFO - PRINT: Epoch [2221], Train Loss: 0.1018, Validation Loss: 0.1003
2026-01-01 00:53:27 - INFO - PRINT: Epoch [2222], Train Loss: 0.1007, Validation Loss: 0.1003
2026-01-01 00:53:53 - INFO - PRINT: Epoch [2223], Train Loss: 0.1002, Validation Loss: 0.1003
2026-01-01 00:54:20 - INFO - PRINT: Epoch [2224], Train Loss: 0.1007, Validation Loss: 0.1003
2026-01-01 00:54:47 - INFO - PRINT: Epoch [2225], Train Loss: 0.1000, Validation Loss: 0.1003
2026-01-01 00:55:13 - INFO - PRINT: Epoch [2226], Train Loss: 0.1005, Validation Loss: 0.1003
2026-01-01 00:55:40 - INFO - PRINT: Epoch [2227], Train Loss: 0.1015, Validation Loss: 0.1003
2026-01-01 00:56:07 - INFO - PRINT: Epoch [2228], Train Loss: 0.1014, Validation Loss: 0.1003
2026-01-01 00:56:34 - INFO - PRINT: Epoch [2229], Train Loss: 0.1002, Validation Loss: 0.1003
2026-01-01 00:57:00 - INFO - PRINT: Epoch [2230], Train Loss: 0.1003, Validation Loss: 0.1003
2026-01-01 00:57:27 - INFO - PRINT: Epoch [2231], Train Loss: 0.1015, Validation Loss: 0.1003
2026-01-01 00:57:54 - INFO - PRINT: Epoch [2232], Train Loss: 0.1014, Validation Loss: 0.1003
2026-01-01 00:58:20 - INFO - PRINT: Epoch [2233], Train Loss: 0.1005, Validation Loss: 0.1003
2026-01-01 00:58:47 - INFO - PRINT: Epoch [2234], Train Loss: 0.0999, Validation Loss: 0.1003
2026-01-01 00:59:14 - INFO - PRINT: Epoch [2235], Train Loss: 0.1004, Validation Loss: 0.1003
2026-01-01 00:59:41 - INFO - PRINT: Epoch [2236], Train Loss: 0.1009, Validation Loss: 0.1003
2026-01-01 01:00:07 - INFO - PRINT: Epoch [2237], Train Loss: 0.1005, Validation Loss: 0.1003
2026-01-01 01:00:34 - INFO - PRINT: Epoch [2238], Train Loss: 0.1006, Validation Loss: 0.1003
2026-01-01 01:01:01 - INFO - PRINT: Epoch [2239], Train Loss: 0.1009, Validation Loss: 0.1003
2026-01-01 01:01:32 - INFO - PRINT: Epoch [2240], Train Loss: 0.1005, Validation Loss: 0.1016
2026-01-01 01:01:58 - INFO - PRINT: Epoch [2241], Train Loss: 0.1024, Validation Loss: 0.1016
2026-01-01 01:02:25 - INFO - PRINT: Epoch [2242], Train Loss: 0.1007, Validation Loss: 0.1016
2026-01-01 01:02:52 - INFO - PRINT: Epoch [2243], Train Loss: 0.1013, Validation Loss: 0.1016
2026-01-01 01:03:18 - INFO - PRINT: Epoch [2244], Train Loss: 0.1012, Validation Loss: 0.1016
2026-01-01 01:03:45 - INFO - PRINT: Epoch [2245], Train Loss: 0.1004, Validation Loss: 0.1016
2026-01-01 01:04:12 - INFO - PRINT: Epoch [2246], Train Loss: 0.1001, Validation Loss: 0.1016
2026-01-01 01:04:38 - INFO - PRINT: Epoch [2247], Train Loss: 0.1001, Validation Loss: 0.1016
2026-01-01 01:05:05 - INFO - PRINT: Epoch [2248], Train Loss: 0.1005, Validation Loss: 0.1016
2026-01-01 01:05:32 - INFO - PRINT: Epoch [2249], Train Loss: 0.1002, Validation Loss: 0.1016
2026-01-01 01:05:59 - INFO - PRINT: Epoch [2250], Train Loss: 0.1003, Validation Loss: 0.1016
2026-01-01 01:06:25 - INFO - PRINT: Epoch [2251], Train Loss: 0.1005, Validation Loss: 0.1016
2026-01-01 01:06:52 - INFO - PRINT: Epoch [2252], Train Loss: 0.1010, Validation Loss: 0.1016
2026-01-01 01:07:19 - INFO - PRINT: Epoch [2253], Train Loss: 0.1008, Validation Loss: 0.1016
2026-01-01 01:07:45 - INFO - PRINT: Epoch [2254], Train Loss: 0.1014, Validation Loss: 0.1016
2026-01-01 01:08:12 - INFO - PRINT: Epoch [2255], Train Loss: 0.1006, Validation Loss: 0.1016
2026-01-01 01:08:39 - INFO - PRINT: Epoch [2256], Train Loss: 0.1004, Validation Loss: 0.1016
2026-01-01 01:09:06 - INFO - PRINT: Epoch [2257], Train Loss: 0.1012, Validation Loss: 0.1016
2026-01-01 01:09:32 - INFO - PRINT: Epoch [2258], Train Loss: 0.1007, Validation Loss: 0.1016
2026-01-01 01:09:59 - INFO - PRINT: Epoch [2259], Train Loss: 0.1001, Validation Loss: 0.1016
2026-01-01 01:10:30 - INFO - PRINT: Epoch [2260], Train Loss: 0.1007, Validation Loss: 0.0991
2026-01-01 01:10:56 - INFO - PRINT: Epoch [2261], Train Loss: 0.1013, Validation Loss: 0.0991
2026-01-01 01:11:23 - INFO - PRINT: Epoch [2262], Train Loss: 0.1004, Validation Loss: 0.0991
2026-01-01 01:11:50 - INFO - PRINT: Epoch [2263], Train Loss: 0.1000, Validation Loss: 0.0991
2026-01-01 01:12:17 - INFO - PRINT: Epoch [2264], Train Loss: 0.1005, Validation Loss: 0.0991
2026-01-01 01:12:43 - INFO - PRINT: Epoch [2265], Train Loss: 0.1005, Validation Loss: 0.0991
2026-01-01 01:13:10 - INFO - PRINT: Epoch [2266], Train Loss: 0.1009, Validation Loss: 0.0991
2026-01-01 01:13:37 - INFO - PRINT: Epoch [2267], Train Loss: 0.1001, Validation Loss: 0.0991
2026-01-01 01:14:03 - INFO - PRINT: Epoch [2268], Train Loss: 0.1022, Validation Loss: 0.0991
2026-01-01 01:14:30 - INFO - PRINT: Epoch [2269], Train Loss: 0.1021, Validation Loss: 0.0991
2026-01-01 01:14:57 - INFO - PRINT: Epoch [2270], Train Loss: 0.1004, Validation Loss: 0.0991
2026-01-01 01:15:24 - INFO - PRINT: Epoch [2271], Train Loss: 0.1008, Validation Loss: 0.0991
2026-01-01 01:15:50 - INFO - PRINT: Epoch [2272], Train Loss: 0.1005, Validation Loss: 0.0991
2026-01-01 01:16:17 - INFO - PRINT: Epoch [2273], Train Loss: 0.1005, Validation Loss: 0.0991
2026-01-01 01:16:44 - INFO - PRINT: Epoch [2274], Train Loss: 0.1003, Validation Loss: 0.0991
2026-01-01 01:17:10 - INFO - PRINT: Epoch [2275], Train Loss: 0.1001, Validation Loss: 0.0991
2026-01-01 01:17:37 - INFO - PRINT: Epoch [2276], Train Loss: 0.1018, Validation Loss: 0.0991
2026-01-01 01:18:04 - INFO - PRINT: Epoch [2277], Train Loss: 0.1002, Validation Loss: 0.0991
2026-01-01 01:18:31 - INFO - PRINT: Epoch [2278], Train Loss: 0.1003, Validation Loss: 0.0991
2026-01-01 01:18:57 - INFO - PRINT: Epoch [2279], Train Loss: 0.1006, Validation Loss: 0.0991
2026-01-01 01:19:28 - INFO - PRINT: Epoch [2280], Train Loss: 0.1007, Validation Loss: 0.0983
2026-01-01 01:19:55 - INFO - PRINT: Epoch [2281], Train Loss: 0.1001, Validation Loss: 0.0983
2026-01-01 01:20:22 - INFO - PRINT: Epoch [2282], Train Loss: 0.1000, Validation Loss: 0.0983
2026-01-01 01:20:48 - INFO - PRINT: Epoch [2283], Train Loss: 0.1011, Validation Loss: 0.0983
2026-01-01 01:21:15 - INFO - PRINT: Epoch [2284], Train Loss: 0.1013, Validation Loss: 0.0983
2026-01-01 01:21:42 - INFO - PRINT: Epoch [2285], Train Loss: 0.1007, Validation Loss: 0.0983
2026-01-01 01:22:08 - INFO - PRINT: Epoch [2286], Train Loss: 0.1008, Validation Loss: 0.0983
2026-01-01 01:22:35 - INFO - PRINT: Epoch [2287], Train Loss: 0.1007, Validation Loss: 0.0983
2026-01-01 01:23:02 - INFO - PRINT: Epoch [2288], Train Loss: 0.1006, Validation Loss: 0.0983
2026-01-01 01:23:28 - INFO - PRINT: Epoch [2289], Train Loss: 0.1009, Validation Loss: 0.0983
2026-01-01 01:23:55 - INFO - PRINT: Epoch [2290], Train Loss: 0.1000, Validation Loss: 0.0983
2026-01-01 01:24:22 - INFO - PRINT: Epoch [2291], Train Loss: 0.1013, Validation Loss: 0.0983
2026-01-01 01:24:49 - INFO - PRINT: Epoch [2292], Train Loss: 0.1008, Validation Loss: 0.0983
2026-01-01 01:25:15 - INFO - PRINT: Epoch [2293], Train Loss: 0.1008, Validation Loss: 0.0983
2026-01-01 01:25:42 - INFO - PRINT: Epoch [2294], Train Loss: 0.1005, Validation Loss: 0.0983
2026-01-01 01:26:09 - INFO - PRINT: Epoch [2295], Train Loss: 0.1012, Validation Loss: 0.0983
2026-01-01 01:26:36 - INFO - PRINT: Epoch [2296], Train Loss: 0.1002, Validation Loss: 0.0983
2026-01-01 01:27:02 - INFO - PRINT: Epoch [2297], Train Loss: 0.1008, Validation Loss: 0.0983
2026-01-01 01:27:29 - INFO - PRINT: Epoch [2298], Train Loss: 0.1008, Validation Loss: 0.0983
2026-01-01 01:27:56 - INFO - PRINT: Epoch [2299], Train Loss: 0.1002, Validation Loss: 0.0983
2026-01-01 01:28:27 - INFO - PRINT: Epoch [2300], Train Loss: 0.1011, Validation Loss: 0.0980
2026-01-01 01:28:27 - INFO - PRINT: ----> Saving model from epoch 2300 (val loss: 0.09803423255681992). Magnificent!
2026-01-01 01:28:53 - INFO - PRINT: Epoch [2301], Train Loss: 0.1000, Validation Loss: 0.0980
2026-01-01 01:29:20 - INFO - PRINT: Epoch [2302], Train Loss: 0.1021, Validation Loss: 0.0980
2026-01-01 01:29:47 - INFO - PRINT: Epoch [2303], Train Loss: 0.1010, Validation Loss: 0.0980
2026-01-01 01:30:13 - INFO - PRINT: Epoch [2304], Train Loss: 0.1000, Validation Loss: 0.0980
2026-01-01 01:30:40 - INFO - PRINT: Epoch [2305], Train Loss: 0.1011, Validation Loss: 0.0980
2026-01-01 01:31:07 - INFO - PRINT: Epoch [2306], Train Loss: 0.1003, Validation Loss: 0.0980
2026-01-01 01:31:33 - INFO - PRINT: Epoch [2307], Train Loss: 0.1005, Validation Loss: 0.0980
2026-01-01 01:32:00 - INFO - PRINT: Epoch [2308], Train Loss: 0.1009, Validation Loss: 0.0980
2026-01-01 01:32:27 - INFO - PRINT: Epoch [2309], Train Loss: 0.1022, Validation Loss: 0.0980
2026-01-01 01:32:54 - INFO - PRINT: Epoch [2310], Train Loss: 0.1007, Validation Loss: 0.0980
2026-01-01 01:33:20 - INFO - PRINT: Epoch [2311], Train Loss: 0.1019, Validation Loss: 0.0980
2026-01-01 01:33:47 - INFO - PRINT: Epoch [2312], Train Loss: 0.1013, Validation Loss: 0.0980
2026-01-01 01:34:14 - INFO - PRINT: Epoch [2313], Train Loss: 0.1003, Validation Loss: 0.0980
2026-01-01 01:34:40 - INFO - PRINT: Epoch [2314], Train Loss: 0.0997, Validation Loss: 0.0980
2026-01-01 01:35:07 - INFO - PRINT: Epoch [2315], Train Loss: 0.0999, Validation Loss: 0.0980
2026-01-01 01:35:34 - INFO - PRINT: Epoch [2316], Train Loss: 0.1000, Validation Loss: 0.0980
2026-01-01 01:36:01 - INFO - PRINT: Epoch [2317], Train Loss: 0.1004, Validation Loss: 0.0980
2026-01-01 01:36:27 - INFO - PRINT: Epoch [2318], Train Loss: 0.1008, Validation Loss: 0.0980
2026-01-01 01:36:54 - INFO - PRINT: Epoch [2319], Train Loss: 0.1004, Validation Loss: 0.0980
2026-01-01 01:37:25 - INFO - PRINT: Epoch [2320], Train Loss: 0.1018, Validation Loss: 0.0991
2026-01-01 01:37:51 - INFO - PRINT: Epoch [2321], Train Loss: 0.1006, Validation Loss: 0.0991
2026-01-01 01:38:18 - INFO - PRINT: Epoch [2322], Train Loss: 0.0999, Validation Loss: 0.0991
2026-01-01 01:38:45 - INFO - PRINT: Epoch [2323], Train Loss: 0.1004, Validation Loss: 0.0991
2026-01-01 01:39:12 - INFO - PRINT: Epoch [2324], Train Loss: 0.1014, Validation Loss: 0.0991
2026-01-01 01:39:38 - INFO - PRINT: Epoch [2325], Train Loss: 0.1002, Validation Loss: 0.0991
2026-01-01 01:40:05 - INFO - PRINT: Epoch [2326], Train Loss: 0.1008, Validation Loss: 0.0991
2026-01-01 01:40:32 - INFO - PRINT: Epoch [2327], Train Loss: 0.1009, Validation Loss: 0.0991
2026-01-01 01:40:58 - INFO - PRINT: Epoch [2328], Train Loss: 0.1010, Validation Loss: 0.0991
2026-01-01 01:41:25 - INFO - PRINT: Epoch [2329], Train Loss: 0.1013, Validation Loss: 0.0991
2026-01-01 01:41:52 - INFO - PRINT: Epoch [2330], Train Loss: 0.1007, Validation Loss: 0.0991
2026-01-01 01:42:19 - INFO - PRINT: Epoch [2331], Train Loss: 0.1007, Validation Loss: 0.0991
2026-01-01 01:42:45 - INFO - PRINT: Epoch [2332], Train Loss: 0.1001, Validation Loss: 0.0991
2026-01-01 01:43:12 - INFO - PRINT: Epoch [2333], Train Loss: 0.1002, Validation Loss: 0.0991
2026-01-01 01:43:39 - INFO - PRINT: Epoch [2334], Train Loss: 0.1002, Validation Loss: 0.0991
2026-01-01 01:44:05 - INFO - PRINT: Epoch [2335], Train Loss: 0.1002, Validation Loss: 0.0991
2026-01-01 01:44:32 - INFO - PRINT: Epoch [2336], Train Loss: 0.1004, Validation Loss: 0.0991
2026-01-01 01:44:59 - INFO - PRINT: Epoch [2337], Train Loss: 0.1001, Validation Loss: 0.0991
2026-01-01 01:45:26 - INFO - PRINT: Epoch [2338], Train Loss: 0.1009, Validation Loss: 0.0991
2026-01-01 01:45:52 - INFO - PRINT: Epoch [2339], Train Loss: 0.1001, Validation Loss: 0.0991
2026-01-01 01:46:23 - INFO - PRINT: Epoch [2340], Train Loss: 0.1007, Validation Loss: 0.0980
2026-01-01 01:46:50 - INFO - PRINT: Epoch [2341], Train Loss: 0.1007, Validation Loss: 0.0980
2026-01-01 01:47:16 - INFO - PRINT: Epoch [2342], Train Loss: 0.1004, Validation Loss: 0.0980
2026-01-01 01:47:43 - INFO - PRINT: Epoch [2343], Train Loss: 0.1007, Validation Loss: 0.0980
2026-01-01 01:48:10 - INFO - PRINT: Epoch [2344], Train Loss: 0.1004, Validation Loss: 0.0980
2026-01-01 01:48:37 - INFO - PRINT: Epoch [2345], Train Loss: 0.1008, Validation Loss: 0.0980
2026-01-01 01:49:03 - INFO - PRINT: Epoch [2346], Train Loss: 0.1005, Validation Loss: 0.0980
2026-01-01 01:49:30 - INFO - PRINT: Epoch [2347], Train Loss: 0.1014, Validation Loss: 0.0980
2026-01-01 01:49:57 - INFO - PRINT: Epoch [2348], Train Loss: 0.1005, Validation Loss: 0.0980
2026-01-01 01:50:23 - INFO - PRINT: Epoch [2349], Train Loss: 0.1011, Validation Loss: 0.0980
2026-01-01 01:50:50 - INFO - PRINT: Epoch [2350], Train Loss: 0.1010, Validation Loss: 0.0980
2026-01-01 01:51:17 - INFO - PRINT: Epoch [2351], Train Loss: 0.1005, Validation Loss: 0.0980
2026-01-01 01:51:44 - INFO - PRINT: Epoch [2352], Train Loss: 0.1003, Validation Loss: 0.0980
2026-01-01 01:52:10 - INFO - PRINT: Epoch [2353], Train Loss: 0.1005, Validation Loss: 0.0980
2026-01-01 01:52:37 - INFO - PRINT: Epoch [2354], Train Loss: 0.1005, Validation Loss: 0.0980
2026-01-01 01:53:04 - INFO - PRINT: Epoch [2355], Train Loss: 0.1004, Validation Loss: 0.0980
2026-01-01 01:53:30 - INFO - PRINT: Epoch [2356], Train Loss: 0.1014, Validation Loss: 0.0980
2026-01-01 01:53:57 - INFO - PRINT: Epoch [2357], Train Loss: 0.1004, Validation Loss: 0.0980
2026-01-01 01:54:24 - INFO - PRINT: Epoch [2358], Train Loss: 0.1011, Validation Loss: 0.0980
2026-01-01 01:54:51 - INFO - PRINT: Epoch [2359], Train Loss: 0.1009, Validation Loss: 0.0980
2026-01-01 01:55:22 - INFO - PRINT: Epoch [2360], Train Loss: 0.1008, Validation Loss: 0.0994
2026-01-01 01:55:48 - INFO - PRINT: Epoch [2361], Train Loss: 0.1000, Validation Loss: 0.0994
2026-01-01 01:56:15 - INFO - PRINT: Epoch [2362], Train Loss: 0.1006, Validation Loss: 0.0994
2026-01-01 01:56:42 - INFO - PRINT: Epoch [2363], Train Loss: 0.1007, Validation Loss: 0.0994
2026-01-01 01:57:08 - INFO - PRINT: Epoch [2364], Train Loss: 0.1009, Validation Loss: 0.0994
2026-01-01 01:57:35 - INFO - PRINT: Epoch [2365], Train Loss: 0.1006, Validation Loss: 0.0994
2026-01-01 01:58:02 - INFO - PRINT: Epoch [2366], Train Loss: 0.1015, Validation Loss: 0.0994
2026-01-01 01:58:28 - INFO - PRINT: Epoch [2367], Train Loss: 0.1007, Validation Loss: 0.0994
2026-01-01 01:58:55 - INFO - PRINT: Epoch [2368], Train Loss: 0.1003, Validation Loss: 0.0994
2026-01-01 01:59:22 - INFO - PRINT: Epoch [2369], Train Loss: 0.1004, Validation Loss: 0.0994
2026-01-01 01:59:48 - INFO - PRINT: Epoch [2370], Train Loss: 0.1007, Validation Loss: 0.0994
2026-01-01 02:00:15 - INFO - PRINT: Epoch [2371], Train Loss: 0.1004, Validation Loss: 0.0994
2026-01-01 02:00:42 - INFO - PRINT: Epoch [2372], Train Loss: 0.1010, Validation Loss: 0.0994
2026-01-01 02:01:09 - INFO - PRINT: Epoch [2373], Train Loss: 0.0999, Validation Loss: 0.0994
2026-01-01 02:01:35 - INFO - PRINT: Epoch [2374], Train Loss: 0.1002, Validation Loss: 0.0994
2026-01-01 02:02:02 - INFO - PRINT: Epoch [2375], Train Loss: 0.1001, Validation Loss: 0.0994
2026-01-01 02:02:29 - INFO - PRINT: Epoch [2376], Train Loss: 0.1003, Validation Loss: 0.0994
2026-01-01 02:02:55 - INFO - PRINT: Epoch [2377], Train Loss: 0.1007, Validation Loss: 0.0994
2026-01-01 02:03:22 - INFO - PRINT: Epoch [2378], Train Loss: 0.1006, Validation Loss: 0.0994
2026-01-01 02:03:49 - INFO - PRINT: Epoch [2379], Train Loss: 0.1001, Validation Loss: 0.0994
2026-01-01 02:04:20 - INFO - PRINT: Epoch [2380], Train Loss: 0.1004, Validation Loss: 0.0979
2026-01-01 02:04:46 - INFO - PRINT: Epoch [2381], Train Loss: 0.1005, Validation Loss: 0.0979
2026-01-01 02:05:13 - INFO - PRINT: Epoch [2382], Train Loss: 0.1010, Validation Loss: 0.0979
2026-01-01 02:05:40 - INFO - PRINT: Epoch [2383], Train Loss: 0.1004, Validation Loss: 0.0979
2026-01-01 02:06:07 - INFO - PRINT: Epoch [2384], Train Loss: 0.1001, Validation Loss: 0.0979
2026-01-01 02:06:33 - INFO - PRINT: Epoch [2385], Train Loss: 0.1005, Validation Loss: 0.0979
2026-01-01 02:07:00 - INFO - PRINT: Epoch [2386], Train Loss: 0.1009, Validation Loss: 0.0979
2026-01-01 02:07:27 - INFO - PRINT: Epoch [2387], Train Loss: 0.1008, Validation Loss: 0.0979
2026-01-01 02:07:53 - INFO - PRINT: Epoch [2388], Train Loss: 0.1003, Validation Loss: 0.0979
2026-01-01 02:08:20 - INFO - PRINT: Epoch [2389], Train Loss: 0.1005, Validation Loss: 0.0979
2026-01-01 02:08:47 - INFO - PRINT: Epoch [2390], Train Loss: 0.1011, Validation Loss: 0.0979
2026-01-01 02:09:14 - INFO - PRINT: Epoch [2391], Train Loss: 0.1004, Validation Loss: 0.0979
2026-01-01 02:09:40 - INFO - PRINT: Epoch [2392], Train Loss: 0.1005, Validation Loss: 0.0979
2026-01-01 02:10:07 - INFO - PRINT: Epoch [2393], Train Loss: 0.1007, Validation Loss: 0.0979
2026-01-01 02:10:34 - INFO - PRINT: Epoch [2394], Train Loss: 0.1010, Validation Loss: 0.0979
2026-01-01 02:11:00 - INFO - PRINT: Epoch [2395], Train Loss: 0.1007, Validation Loss: 0.0979
2026-01-01 02:11:27 - INFO - PRINT: Epoch [2396], Train Loss: 0.1000, Validation Loss: 0.0979
2026-01-01 02:11:54 - INFO - PRINT: Epoch [2397], Train Loss: 0.1005, Validation Loss: 0.0979
2026-01-01 02:12:21 - INFO - PRINT: Epoch [2398], Train Loss: 0.1005, Validation Loss: 0.0979
2026-01-01 02:12:47 - INFO - PRINT: Epoch [2399], Train Loss: 0.1010, Validation Loss: 0.0979
2026-01-01 02:13:18 - INFO - PRINT: Epoch [2400], Train Loss: 0.1009, Validation Loss: 0.0988
2026-01-01 02:13:18 - INFO - PRINT: ----> Saving model from epoch 2380 (val loss: 0.09791000068187714). Nom-worthy!
2026-01-01 02:13:45 - INFO - PRINT: Epoch [2401], Train Loss: 0.1002, Validation Loss: 0.0988
2026-01-01 02:14:11 - INFO - PRINT: Epoch [2402], Train Loss: 0.1006, Validation Loss: 0.0988
2026-01-01 02:14:38 - INFO - PRINT: Epoch [2403], Train Loss: 0.1021, Validation Loss: 0.0988
2026-01-01 02:15:05 - INFO - PRINT: Epoch [2404], Train Loss: 0.1004, Validation Loss: 0.0988
2026-01-01 02:15:32 - INFO - PRINT: Epoch [2405], Train Loss: 0.1007, Validation Loss: 0.0988
2026-01-01 02:15:58 - INFO - PRINT: Epoch [2406], Train Loss: 0.1014, Validation Loss: 0.0988
2026-01-01 02:16:25 - INFO - PRINT: Epoch [2407], Train Loss: 0.1010, Validation Loss: 0.0988
2026-01-01 02:16:52 - INFO - PRINT: Epoch [2408], Train Loss: 0.1015, Validation Loss: 0.0988
2026-01-01 02:17:18 - INFO - PRINT: Epoch [2409], Train Loss: 0.0997, Validation Loss: 0.0988
2026-01-01 02:17:45 - INFO - PRINT: Epoch [2410], Train Loss: 0.1001, Validation Loss: 0.0988
2026-01-01 02:18:12 - INFO - PRINT: Epoch [2411], Train Loss: 0.1005, Validation Loss: 0.0988
2026-01-01 02:18:39 - INFO - PRINT: Epoch [2412], Train Loss: 0.1009, Validation Loss: 0.0988
2026-01-01 02:19:05 - INFO - PRINT: Epoch [2413], Train Loss: 0.1004, Validation Loss: 0.0988
2026-01-01 02:19:32 - INFO - PRINT: Epoch [2414], Train Loss: 0.1027, Validation Loss: 0.0988
2026-01-01 02:19:59 - INFO - PRINT: Epoch [2415], Train Loss: 0.1013, Validation Loss: 0.0988
2026-01-01 02:20:25 - INFO - PRINT: Epoch [2416], Train Loss: 0.1007, Validation Loss: 0.0988
2026-01-01 02:20:52 - INFO - PRINT: Epoch [2417], Train Loss: 0.1005, Validation Loss: 0.0988
2026-01-01 02:21:19 - INFO - PRINT: Epoch [2418], Train Loss: 0.0999, Validation Loss: 0.0988
2026-01-01 02:21:45 - INFO - PRINT: Epoch [2419], Train Loss: 0.0999, Validation Loss: 0.0988
2026-01-01 02:22:17 - INFO - PRINT: Epoch [2420], Train Loss: 0.1011, Validation Loss: 0.0993
2026-01-01 02:22:43 - INFO - PRINT: Epoch [2421], Train Loss: 0.1007, Validation Loss: 0.0993
2026-01-01 02:23:10 - INFO - PRINT: Epoch [2422], Train Loss: 0.1004, Validation Loss: 0.0993
2026-01-01 02:23:36 - INFO - PRINT: Epoch [2423], Train Loss: 0.1000, Validation Loss: 0.0993
2026-01-01 02:24:03 - INFO - PRINT: Epoch [2424], Train Loss: 0.1002, Validation Loss: 0.0993
2026-01-01 02:24:30 - INFO - PRINT: Epoch [2425], Train Loss: 0.1007, Validation Loss: 0.0993
2026-01-01 02:24:57 - INFO - PRINT: Epoch [2426], Train Loss: 0.1007, Validation Loss: 0.0993
2026-01-01 02:25:23 - INFO - PRINT: Epoch [2427], Train Loss: 0.1003, Validation Loss: 0.0993
2026-01-01 02:25:50 - INFO - PRINT: Epoch [2428], Train Loss: 0.1005, Validation Loss: 0.0993
2026-01-01 02:26:17 - INFO - PRINT: Epoch [2429], Train Loss: 0.1015, Validation Loss: 0.0993
2026-01-01 02:26:43 - INFO - PRINT: Epoch [2430], Train Loss: 0.1002, Validation Loss: 0.0993
2026-01-01 02:27:10 - INFO - PRINT: Epoch [2431], Train Loss: 0.0996, Validation Loss: 0.0993
2026-01-01 02:27:37 - INFO - PRINT: Epoch [2432], Train Loss: 0.1001, Validation Loss: 0.0993
2026-01-01 02:28:04 - INFO - PRINT: Epoch [2433], Train Loss: 0.1005, Validation Loss: 0.0993
2026-01-01 02:28:30 - INFO - PRINT: Epoch [2434], Train Loss: 0.1002, Validation Loss: 0.0993
2026-01-01 02:28:57 - INFO - PRINT: Epoch [2435], Train Loss: 0.1002, Validation Loss: 0.0993
2026-01-01 02:29:24 - INFO - PRINT: Epoch [2436], Train Loss: 0.1007, Validation Loss: 0.0993
2026-01-01 02:29:50 - INFO - PRINT: Epoch [2437], Train Loss: 0.1010, Validation Loss: 0.0993
2026-01-01 02:30:17 - INFO - PRINT: Epoch [2438], Train Loss: 0.1007, Validation Loss: 0.0993
2026-01-01 02:30:44 - INFO - PRINT: Epoch [2439], Train Loss: 0.1008, Validation Loss: 0.0993
2026-01-01 02:31:15 - INFO - PRINT: Epoch [2440], Train Loss: 0.1015, Validation Loss: 0.0990
2026-01-01 02:31:41 - INFO - PRINT: Epoch [2441], Train Loss: 0.1016, Validation Loss: 0.0990
2026-01-01 02:32:08 - INFO - PRINT: Epoch [2442], Train Loss: 0.0998, Validation Loss: 0.0990
2026-01-01 02:32:35 - INFO - PRINT: Epoch [2443], Train Loss: 0.1003, Validation Loss: 0.0990
2026-01-01 02:33:01 - INFO - PRINT: Epoch [2444], Train Loss: 0.1002, Validation Loss: 0.0990
2026-01-01 02:33:28 - INFO - PRINT: Epoch [2445], Train Loss: 0.1005, Validation Loss: 0.0990
2026-01-01 02:33:55 - INFO - PRINT: Epoch [2446], Train Loss: 0.1002, Validation Loss: 0.0990
2026-01-01 02:34:22 - INFO - PRINT: Epoch [2447], Train Loss: 0.1002, Validation Loss: 0.0990
2026-01-01 02:34:48 - INFO - PRINT: Epoch [2448], Train Loss: 0.1000, Validation Loss: 0.0990
2026-01-01 02:35:15 - INFO - PRINT: Epoch [2449], Train Loss: 0.1005, Validation Loss: 0.0990
2026-01-01 02:35:42 - INFO - PRINT: Epoch [2450], Train Loss: 0.1001, Validation Loss: 0.0990
2026-01-01 02:36:08 - INFO - PRINT: Epoch [2451], Train Loss: 0.1006, Validation Loss: 0.0990
2026-01-01 02:36:35 - INFO - PRINT: Epoch [2452], Train Loss: 0.1005, Validation Loss: 0.0990
2026-01-01 02:37:02 - INFO - PRINT: Epoch [2453], Train Loss: 0.1004, Validation Loss: 0.0990
2026-01-01 02:37:29 - INFO - PRINT: Epoch [2454], Train Loss: 0.1006, Validation Loss: 0.0990
2026-01-01 02:37:55 - INFO - PRINT: Epoch [2455], Train Loss: 0.1007, Validation Loss: 0.0990
2026-01-01 02:38:22 - INFO - PRINT: Epoch [2456], Train Loss: 0.1019, Validation Loss: 0.0990
2026-01-01 02:38:49 - INFO - PRINT: Epoch [2457], Train Loss: 0.1004, Validation Loss: 0.0990
2026-01-01 02:39:15 - INFO - PRINT: Epoch [2458], Train Loss: 0.1005, Validation Loss: 0.0990
2026-01-01 02:39:42 - INFO - PRINT: Epoch [2459], Train Loss: 0.1011, Validation Loss: 0.0990
2026-01-01 02:40:13 - INFO - PRINT: Epoch [2460], Train Loss: 0.1011, Validation Loss: 0.0984
2026-01-01 02:40:40 - INFO - PRINT: Epoch [2461], Train Loss: 0.1003, Validation Loss: 0.0984
2026-01-01 02:41:06 - INFO - PRINT: Epoch [2462], Train Loss: 0.1001, Validation Loss: 0.0984
2026-01-01 02:41:33 - INFO - PRINT: Epoch [2463], Train Loss: 0.1002, Validation Loss: 0.0984
2026-01-01 02:42:00 - INFO - PRINT: Epoch [2464], Train Loss: 0.1002, Validation Loss: 0.0984
2026-01-01 02:42:27 - INFO - PRINT: Epoch [2465], Train Loss: 0.1003, Validation Loss: 0.0984
2026-01-01 02:42:53 - INFO - PRINT: Epoch [2466], Train Loss: 0.1011, Validation Loss: 0.0984
2026-01-01 02:43:20 - INFO - PRINT: Epoch [2467], Train Loss: 0.1001, Validation Loss: 0.0984
2026-01-01 02:43:47 - INFO - PRINT: Epoch [2468], Train Loss: 0.1009, Validation Loss: 0.0984
2026-01-01 02:44:13 - INFO - PRINT: Epoch [2469], Train Loss: 0.1001, Validation Loss: 0.0984
2026-01-01 02:44:40 - INFO - PRINT: Epoch [2470], Train Loss: 0.1001, Validation Loss: 0.0984
2026-01-01 02:45:07 - INFO - PRINT: Epoch [2471], Train Loss: 0.1000, Validation Loss: 0.0984
2026-01-01 02:45:34 - INFO - PRINT: Epoch [2472], Train Loss: 0.1004, Validation Loss: 0.0984
2026-01-01 02:46:00 - INFO - PRINT: Epoch [2473], Train Loss: 0.1012, Validation Loss: 0.0984
2026-01-01 02:46:27 - INFO - PRINT: Epoch [2474], Train Loss: 0.1019, Validation Loss: 0.0984
2026-01-01 02:46:54 - INFO - PRINT: Epoch [2475], Train Loss: 0.1012, Validation Loss: 0.0984
2026-01-01 02:47:20 - INFO - PRINT: Epoch [2476], Train Loss: 0.1008, Validation Loss: 0.0984
2026-01-01 02:47:47 - INFO - PRINT: Epoch [2477], Train Loss: 0.1007, Validation Loss: 0.0984
2026-01-01 02:48:14 - INFO - PRINT: Epoch [2478], Train Loss: 0.1001, Validation Loss: 0.0984
2026-01-01 02:48:40 - INFO - PRINT: Epoch [2479], Train Loss: 0.1007, Validation Loss: 0.0984
2026-01-01 02:49:12 - INFO - PRINT: Epoch [2480], Train Loss: 0.1007, Validation Loss: 0.0980
2026-01-01 02:49:38 - INFO - PRINT: Epoch [2481], Train Loss: 0.1009, Validation Loss: 0.0980
2026-01-01 02:50:05 - INFO - PRINT: Epoch [2482], Train Loss: 0.1002, Validation Loss: 0.0980
2026-01-01 02:50:31 - INFO - PRINT: Epoch [2483], Train Loss: 0.0997, Validation Loss: 0.0980
2026-01-01 02:50:58 - INFO - PRINT: Epoch [2484], Train Loss: 0.1004, Validation Loss: 0.0980
2026-01-01 02:51:25 - INFO - PRINT: Epoch [2485], Train Loss: 0.1007, Validation Loss: 0.0980
2026-01-01 02:51:51 - INFO - PRINT: Epoch [2486], Train Loss: 0.1012, Validation Loss: 0.0980
2026-01-01 02:52:18 - INFO - PRINT: Epoch [2487], Train Loss: 0.1011, Validation Loss: 0.0980
2026-01-01 02:52:45 - INFO - PRINT: Epoch [2488], Train Loss: 0.1003, Validation Loss: 0.0980
2026-01-01 02:53:12 - INFO - PRINT: Epoch [2489], Train Loss: 0.1000, Validation Loss: 0.0980
2026-01-01 02:53:38 - INFO - PRINT: Epoch [2490], Train Loss: 0.0999, Validation Loss: 0.0980
2026-01-01 02:54:05 - INFO - PRINT: Epoch [2491], Train Loss: 0.0996, Validation Loss: 0.0980
2026-01-01 02:54:32 - INFO - PRINT: Epoch [2492], Train Loss: 0.1005, Validation Loss: 0.0980
2026-01-01 02:54:58 - INFO - PRINT: Epoch [2493], Train Loss: 0.1004, Validation Loss: 0.0980
2026-01-01 02:55:25 - INFO - PRINT: Epoch [2494], Train Loss: 0.1001, Validation Loss: 0.0980
2026-01-01 02:55:52 - INFO - PRINT: Epoch [2495], Train Loss: 0.1003, Validation Loss: 0.0980
2026-01-01 02:56:19 - INFO - PRINT: Epoch [2496], Train Loss: 0.1009, Validation Loss: 0.0980
2026-01-01 02:56:45 - INFO - PRINT: Epoch [2497], Train Loss: 0.1019, Validation Loss: 0.0980
2026-01-01 02:57:12 - INFO - PRINT: Epoch [2498], Train Loss: 0.1003, Validation Loss: 0.0980
2026-01-01 02:57:39 - INFO - PRINT: Epoch [2499], Train Loss: 0.1007, Validation Loss: 0.0980
2026-01-01 02:58:10 - INFO - PRINT: Epoch [2500], Train Loss: 0.1006, Validation Loss: 0.1003
2026-01-01 02:58:10 - INFO - PRINT: ----> Saving model from epoch 2380 (val loss: 0.09791000068187714). Heavenly!
2026-01-01 02:58:36 - INFO - PRINT: Epoch [2501], Train Loss: 0.1017, Validation Loss: 0.1003
2026-01-01 02:59:03 - INFO - PRINT: Epoch [2502], Train Loss: 0.1009, Validation Loss: 0.1003
2026-01-01 02:59:30 - INFO - PRINT: Epoch [2503], Train Loss: 0.1002, Validation Loss: 0.1003
2026-01-01 02:59:56 - INFO - PRINT: Epoch [2504], Train Loss: 0.0998, Validation Loss: 0.1003
2026-01-01 03:00:23 - INFO - PRINT: Epoch [2505], Train Loss: 0.1006, Validation Loss: 0.1003
2026-01-01 03:00:50 - INFO - PRINT: Epoch [2506], Train Loss: 0.0999, Validation Loss: 0.1003
2026-01-01 03:01:16 - INFO - PRINT: Epoch [2507], Train Loss: 0.1002, Validation Loss: 0.1003
2026-01-01 03:01:43 - INFO - PRINT: Epoch [2508], Train Loss: 0.1015, Validation Loss: 0.1003
2026-01-01 03:02:10 - INFO - PRINT: Epoch [2509], Train Loss: 0.1002, Validation Loss: 0.1003
2026-01-01 03:02:37 - INFO - PRINT: Epoch [2510], Train Loss: 0.1002, Validation Loss: 0.1003
2026-01-01 03:03:03 - INFO - PRINT: Epoch [2511], Train Loss: 0.1014, Validation Loss: 0.1003
2026-01-01 03:03:30 - INFO - PRINT: Epoch [2512], Train Loss: 0.1006, Validation Loss: 0.1003
2026-01-01 03:03:57 - INFO - PRINT: Epoch [2513], Train Loss: 0.1002, Validation Loss: 0.1003
2026-01-01 03:04:23 - INFO - PRINT: Epoch [2514], Train Loss: 0.1007, Validation Loss: 0.1003
2026-01-01 03:04:50 - INFO - PRINT: Epoch [2515], Train Loss: 0.1014, Validation Loss: 0.1003
2026-01-01 03:05:17 - INFO - PRINT: Epoch [2516], Train Loss: 0.0997, Validation Loss: 0.1003
2026-01-01 03:05:44 - INFO - PRINT: Epoch [2517], Train Loss: 0.1003, Validation Loss: 0.1003
2026-01-01 03:06:10 - INFO - PRINT: Epoch [2518], Train Loss: 0.1009, Validation Loss: 0.1003
2026-01-01 03:06:37 - INFO - PRINT: Epoch [2519], Train Loss: 0.1003, Validation Loss: 0.1003
2026-01-01 03:07:08 - INFO - PRINT: Epoch [2520], Train Loss: 0.1005, Validation Loss: 0.0995
2026-01-01 03:07:34 - INFO - PRINT: Epoch [2521], Train Loss: 0.1006, Validation Loss: 0.0995
2026-01-01 03:08:01 - INFO - PRINT: Epoch [2522], Train Loss: 0.1006, Validation Loss: 0.0995
2026-01-01 03:08:28 - INFO - PRINT: Epoch [2523], Train Loss: 0.1035, Validation Loss: 0.0995
2026-01-01 03:08:55 - INFO - PRINT: Epoch [2524], Train Loss: 0.1011, Validation Loss: 0.0995
2026-01-01 03:09:21 - INFO - PRINT: Epoch [2525], Train Loss: 0.1012, Validation Loss: 0.0995
2026-01-01 03:09:48 - INFO - PRINT: Epoch [2526], Train Loss: 0.1011, Validation Loss: 0.0995
2026-01-01 03:10:15 - INFO - PRINT: Epoch [2527], Train Loss: 0.0998, Validation Loss: 0.0995
2026-01-01 03:10:41 - INFO - PRINT: Epoch [2528], Train Loss: 0.1002, Validation Loss: 0.0995
2026-01-01 03:11:08 - INFO - PRINT: Epoch [2529], Train Loss: 0.0992, Validation Loss: 0.0995
2026-01-01 03:11:35 - INFO - PRINT: Epoch [2530], Train Loss: 0.1001, Validation Loss: 0.0995
2026-01-01 03:12:02 - INFO - PRINT: Epoch [2531], Train Loss: 0.1006, Validation Loss: 0.0995
2026-01-01 03:12:28 - INFO - PRINT: Epoch [2532], Train Loss: 0.1002, Validation Loss: 0.0995
2026-01-01 03:12:55 - INFO - PRINT: Epoch [2533], Train Loss: 0.0998, Validation Loss: 0.0995
2026-01-01 03:13:22 - INFO - PRINT: Epoch [2534], Train Loss: 0.1002, Validation Loss: 0.0995
2026-01-01 03:13:48 - INFO - PRINT: Epoch [2535], Train Loss: 0.1004, Validation Loss: 0.0995
2026-01-01 03:14:15 - INFO - PRINT: Epoch [2536], Train Loss: 0.0998, Validation Loss: 0.0995
2026-01-01 03:14:42 - INFO - PRINT: Epoch [2537], Train Loss: 0.1017, Validation Loss: 0.0995
2026-01-01 03:15:09 - INFO - PRINT: Epoch [2538], Train Loss: 0.1025, Validation Loss: 0.0995
2026-01-01 03:15:35 - INFO - PRINT: Epoch [2539], Train Loss: 0.1002, Validation Loss: 0.0995
2026-01-01 03:16:06 - INFO - PRINT: Epoch [2540], Train Loss: 0.0998, Validation Loss: 0.0989
2026-01-01 03:16:33 - INFO - PRINT: Epoch [2541], Train Loss: 0.1000, Validation Loss: 0.0989
2026-01-01 03:16:59 - INFO - PRINT: Epoch [2542], Train Loss: 0.1006, Validation Loss: 0.0989
2026-01-01 03:17:26 - INFO - PRINT: Epoch [2543], Train Loss: 0.1001, Validation Loss: 0.0989
2026-01-01 03:17:53 - INFO - PRINT: Epoch [2544], Train Loss: 0.1005, Validation Loss: 0.0989
2026-01-01 03:18:20 - INFO - PRINT: Epoch [2545], Train Loss: 0.1015, Validation Loss: 0.0989
2026-01-01 03:18:46 - INFO - PRINT: Epoch [2546], Train Loss: 0.1003, Validation Loss: 0.0989
2026-01-01 03:19:13 - INFO - PRINT: Epoch [2547], Train Loss: 0.1005, Validation Loss: 0.0989
2026-01-01 03:19:40 - INFO - PRINT: Epoch [2548], Train Loss: 0.1000, Validation Loss: 0.0989
2026-01-01 03:20:06 - INFO - PRINT: Epoch [2549], Train Loss: 0.1003, Validation Loss: 0.0989
2026-01-01 03:20:33 - INFO - PRINT: Epoch [2550], Train Loss: 0.1015, Validation Loss: 0.0989
2026-01-01 03:21:00 - INFO - PRINT: Epoch [2551], Train Loss: 0.1004, Validation Loss: 0.0989
2026-01-01 03:21:27 - INFO - PRINT: Epoch [2552], Train Loss: 0.1007, Validation Loss: 0.0989
2026-01-01 03:21:53 - INFO - PRINT: Epoch [2553], Train Loss: 0.0998, Validation Loss: 0.0989
2026-01-01 03:22:20 - INFO - PRINT: Epoch [2554], Train Loss: 0.1008, Validation Loss: 0.0989
2026-01-01 03:22:47 - INFO - PRINT: Epoch [2555], Train Loss: 0.1007, Validation Loss: 0.0989
2026-01-01 03:23:13 - INFO - PRINT: Epoch [2556], Train Loss: 0.1002, Validation Loss: 0.0989
2026-01-01 03:23:40 - INFO - PRINT: Epoch [2557], Train Loss: 0.0998, Validation Loss: 0.0989
2026-01-01 03:24:07 - INFO - PRINT: Epoch [2558], Train Loss: 0.1002, Validation Loss: 0.0989
2026-01-01 03:24:34 - INFO - PRINT: Epoch [2559], Train Loss: 0.1000, Validation Loss: 0.0989
2026-01-01 03:25:05 - INFO - PRINT: Epoch [2560], Train Loss: 0.1010, Validation Loss: 0.1024
2026-01-01 03:25:31 - INFO - PRINT: Epoch [2561], Train Loss: 0.1005, Validation Loss: 0.1024
2026-01-01 03:25:58 - INFO - PRINT: Epoch [2562], Train Loss: 0.0999, Validation Loss: 0.1024
2026-01-01 03:26:24 - INFO - PRINT: Epoch [2563], Train Loss: 0.0997, Validation Loss: 0.1024
2026-01-01 03:26:51 - INFO - PRINT: Epoch [2564], Train Loss: 0.1011, Validation Loss: 0.1024
2026-01-01 03:27:18 - INFO - PRINT: Epoch [2565], Train Loss: 0.1007, Validation Loss: 0.1024
2026-01-01 03:27:45 - INFO - PRINT: Epoch [2566], Train Loss: 0.0999, Validation Loss: 0.1024
2026-01-01 03:28:11 - INFO - PRINT: Epoch [2567], Train Loss: 0.1010, Validation Loss: 0.1024
2026-01-01 03:28:38 - INFO - PRINT: Epoch [2568], Train Loss: 0.1000, Validation Loss: 0.1024
2026-01-01 03:29:05 - INFO - PRINT: Epoch [2569], Train Loss: 0.0998, Validation Loss: 0.1024
2026-01-01 03:29:32 - INFO - PRINT: Epoch [2570], Train Loss: 0.1008, Validation Loss: 0.1024
2026-01-01 03:29:58 - INFO - PRINT: Epoch [2571], Train Loss: 0.1007, Validation Loss: 0.1024
2026-01-01 03:30:25 - INFO - PRINT: Epoch [2572], Train Loss: 0.1003, Validation Loss: 0.1024
2026-01-01 03:30:52 - INFO - PRINT: Epoch [2573], Train Loss: 0.1000, Validation Loss: 0.1024
2026-01-01 03:31:18 - INFO - PRINT: Epoch [2574], Train Loss: 0.1001, Validation Loss: 0.1024
2026-01-01 03:31:45 - INFO - PRINT: Epoch [2575], Train Loss: 0.1001, Validation Loss: 0.1024
2026-01-01 03:32:12 - INFO - PRINT: Epoch [2576], Train Loss: 0.1001, Validation Loss: 0.1024
2026-01-01 03:32:39 - INFO - PRINT: Epoch [2577], Train Loss: 0.1001, Validation Loss: 0.1024
2026-01-01 03:33:05 - INFO - PRINT: Epoch [2578], Train Loss: 0.1003, Validation Loss: 0.1024
2026-01-01 03:33:32 - INFO - PRINT: Epoch [2579], Train Loss: 0.1003, Validation Loss: 0.1024
2026-01-01 03:34:03 - INFO - PRINT: Epoch [2580], Train Loss: 0.1005, Validation Loss: 0.0987
2026-01-01 03:34:29 - INFO - PRINT: Epoch [2581], Train Loss: 0.1013, Validation Loss: 0.0987
2026-01-01 03:34:56 - INFO - PRINT: Epoch [2582], Train Loss: 0.1011, Validation Loss: 0.0987
2026-01-01 03:35:23 - INFO - PRINT: Epoch [2583], Train Loss: 0.1003, Validation Loss: 0.0987
2026-01-01 03:35:50 - INFO - PRINT: Epoch [2584], Train Loss: 0.1001, Validation Loss: 0.0987
2026-01-01 03:36:16 - INFO - PRINT: Epoch [2585], Train Loss: 0.1002, Validation Loss: 0.0987
2026-01-01 03:36:43 - INFO - PRINT: Epoch [2586], Train Loss: 0.1008, Validation Loss: 0.0987
2026-01-01 03:37:10 - INFO - PRINT: Epoch [2587], Train Loss: 0.0997, Validation Loss: 0.0987
2026-01-01 03:37:36 - INFO - PRINT: Epoch [2588], Train Loss: 0.1005, Validation Loss: 0.0987
2026-01-01 03:38:03 - INFO - PRINT: Epoch [2589], Train Loss: 0.1013, Validation Loss: 0.0987
2026-01-01 03:38:30 - INFO - PRINT: Epoch [2590], Train Loss: 0.0999, Validation Loss: 0.0987
2026-01-01 03:38:57 - INFO - PRINT: Epoch [2591], Train Loss: 0.1022, Validation Loss: 0.0987
2026-01-01 03:39:23 - INFO - PRINT: Epoch [2592], Train Loss: 0.1015, Validation Loss: 0.0987
2026-01-01 03:39:50 - INFO - PRINT: Epoch [2593], Train Loss: 0.1002, Validation Loss: 0.0987
2026-01-01 03:40:17 - INFO - PRINT: Epoch [2594], Train Loss: 0.1001, Validation Loss: 0.0987
2026-01-01 03:40:43 - INFO - PRINT: Epoch [2595], Train Loss: 0.1000, Validation Loss: 0.0987
2026-01-01 03:41:10 - INFO - PRINT: Epoch [2596], Train Loss: 0.1002, Validation Loss: 0.0987
2026-01-01 03:41:37 - INFO - PRINT: Epoch [2597], Train Loss: 0.1004, Validation Loss: 0.0987
2026-01-01 03:42:04 - INFO - PRINT: Epoch [2598], Train Loss: 0.1009, Validation Loss: 0.0987
2026-01-01 03:42:30 - INFO - PRINT: Epoch [2599], Train Loss: 0.1016, Validation Loss: 0.0987
2026-01-01 03:43:01 - INFO - PRINT: Epoch [2600], Train Loss: 0.0995, Validation Loss: 0.0975
2026-01-01 03:43:01 - INFO - PRINT: ----> Saving model from epoch 2600 (val loss: 0.0974807034432888). Binge-worthy!
2026-01-01 03:43:28 - INFO - PRINT: Epoch [2601], Train Loss: 0.0998, Validation Loss: 0.0975
2026-01-01 03:43:55 - INFO - PRINT: Epoch [2602], Train Loss: 0.1003, Validation Loss: 0.0975
2026-01-01 03:44:21 - INFO - PRINT: Epoch [2603], Train Loss: 0.0999, Validation Loss: 0.0975
2026-01-01 03:44:48 - INFO - PRINT: Epoch [2604], Train Loss: 0.1005, Validation Loss: 0.0975
2026-01-01 03:45:15 - INFO - PRINT: Epoch [2605], Train Loss: 0.1004, Validation Loss: 0.0975
2026-01-01 03:45:41 - INFO - PRINT: Epoch [2606], Train Loss: 0.1016, Validation Loss: 0.0975
2026-01-01 03:46:08 - INFO - PRINT: Epoch [2607], Train Loss: 0.1004, Validation Loss: 0.0975
2026-01-01 03:46:35 - INFO - PRINT: Epoch [2608], Train Loss: 0.1000, Validation Loss: 0.0975
2026-01-01 03:47:01 - INFO - PRINT: Epoch [2609], Train Loss: 0.0999, Validation Loss: 0.0975
2026-01-01 03:47:28 - INFO - PRINT: Epoch [2610], Train Loss: 0.1005, Validation Loss: 0.0975
2026-01-01 03:47:55 - INFO - PRINT: Epoch [2611], Train Loss: 0.1000, Validation Loss: 0.0975
2026-01-01 03:48:22 - INFO - PRINT: Epoch [2612], Train Loss: 0.1006, Validation Loss: 0.0975
2026-01-01 03:48:48 - INFO - PRINT: Epoch [2613], Train Loss: 0.1008, Validation Loss: 0.0975
2026-01-01 03:49:15 - INFO - PRINT: Epoch [2614], Train Loss: 0.1006, Validation Loss: 0.0975
2026-01-01 03:49:42 - INFO - PRINT: Epoch [2615], Train Loss: 0.1010, Validation Loss: 0.0975
2026-01-01 03:50:09 - INFO - PRINT: Epoch [2616], Train Loss: 0.0998, Validation Loss: 0.0975
2026-01-01 03:50:35 - INFO - PRINT: Epoch [2617], Train Loss: 0.1005, Validation Loss: 0.0975
2026-01-01 03:51:02 - INFO - PRINT: Epoch [2618], Train Loss: 0.1005, Validation Loss: 0.0975
2026-01-01 03:51:29 - INFO - PRINT: Epoch [2619], Train Loss: 0.1002, Validation Loss: 0.0975
2026-01-01 03:52:00 - INFO - PRINT: Epoch [2620], Train Loss: 0.1002, Validation Loss: 0.1003
2026-01-01 03:52:26 - INFO - PRINT: Epoch [2621], Train Loss: 0.1004, Validation Loss: 0.1003
2026-01-01 03:52:53 - INFO - PRINT: Epoch [2622], Train Loss: 0.1018, Validation Loss: 0.1003
2026-01-01 03:53:20 - INFO - PRINT: Epoch [2623], Train Loss: 0.1012, Validation Loss: 0.1003
2026-01-01 03:53:46 - INFO - PRINT: Epoch [2624], Train Loss: 0.1005, Validation Loss: 0.1003
2026-01-01 03:54:13 - INFO - PRINT: Epoch [2625], Train Loss: 0.1000, Validation Loss: 0.1003
2026-01-01 03:54:40 - INFO - PRINT: Epoch [2626], Train Loss: 0.1016, Validation Loss: 0.1003
2026-01-01 03:55:06 - INFO - PRINT: Epoch [2627], Train Loss: 0.1004, Validation Loss: 0.1003
2026-01-01 03:55:33 - INFO - PRINT: Epoch [2628], Train Loss: 0.1000, Validation Loss: 0.1003
2026-01-01 03:56:00 - INFO - PRINT: Epoch [2629], Train Loss: 0.0999, Validation Loss: 0.1003
2026-01-01 03:56:27 - INFO - PRINT: Epoch [2630], Train Loss: 0.1002, Validation Loss: 0.1003
2026-01-01 03:56:53 - INFO - PRINT: Epoch [2631], Train Loss: 0.0997, Validation Loss: 0.1003
2026-01-01 03:57:20 - INFO - PRINT: Epoch [2632], Train Loss: 0.1000, Validation Loss: 0.1003
2026-01-01 03:57:47 - INFO - PRINT: Epoch [2633], Train Loss: 0.0997, Validation Loss: 0.1003
2026-01-01 03:58:13 - INFO - PRINT: Epoch [2634], Train Loss: 0.1001, Validation Loss: 0.1003
2026-01-01 03:58:40 - INFO - PRINT: Epoch [2635], Train Loss: 0.1002, Validation Loss: 0.1003
2026-01-01 03:59:07 - INFO - PRINT: Epoch [2636], Train Loss: 0.1005, Validation Loss: 0.1003
2026-01-01 03:59:34 - INFO - PRINT: Epoch [2637], Train Loss: 0.1003, Validation Loss: 0.1003
2026-01-01 04:00:00 - INFO - PRINT: Epoch [2638], Train Loss: 0.1004, Validation Loss: 0.1003
2026-01-01 04:00:27 - INFO - PRINT: Epoch [2639], Train Loss: 0.1028, Validation Loss: 0.1003
2026-01-01 04:00:58 - INFO - PRINT: Epoch [2640], Train Loss: 0.1018, Validation Loss: 0.0986
2026-01-01 04:01:25 - INFO - PRINT: Epoch [2641], Train Loss: 0.1002, Validation Loss: 0.0986
2026-01-01 04:01:51 - INFO - PRINT: Epoch [2642], Train Loss: 0.0998, Validation Loss: 0.0986
2026-01-01 04:02:18 - INFO - PRINT: Epoch [2643], Train Loss: 0.1000, Validation Loss: 0.0986
2026-01-01 04:02:45 - INFO - PRINT: Epoch [2644], Train Loss: 0.0999, Validation Loss: 0.0986
2026-01-01 04:03:11 - INFO - PRINT: Epoch [2645], Train Loss: 0.1001, Validation Loss: 0.0986
2026-01-01 04:03:38 - INFO - PRINT: Epoch [2646], Train Loss: 0.1004, Validation Loss: 0.0986
2026-01-01 04:04:05 - INFO - PRINT: Epoch [2647], Train Loss: 0.0999, Validation Loss: 0.0986
2026-01-01 04:04:32 - INFO - PRINT: Epoch [2648], Train Loss: 0.1002, Validation Loss: 0.0986
2026-01-01 04:04:58 - INFO - PRINT: Epoch [2649], Train Loss: 0.1004, Validation Loss: 0.0986
2026-01-01 04:05:25 - INFO - PRINT: Epoch [2650], Train Loss: 0.1008, Validation Loss: 0.0986
2026-01-01 04:05:52 - INFO - PRINT: Epoch [2651], Train Loss: 0.1001, Validation Loss: 0.0986
2026-01-01 04:06:18 - INFO - PRINT: Epoch [2652], Train Loss: 0.0999, Validation Loss: 0.0986
2026-01-01 04:06:45 - INFO - PRINT: Epoch [2653], Train Loss: 0.0999, Validation Loss: 0.0986
2026-01-01 04:07:12 - INFO - PRINT: Epoch [2654], Train Loss: 0.1002, Validation Loss: 0.0986
2026-01-01 04:07:39 - INFO - PRINT: Epoch [2655], Train Loss: 0.1006, Validation Loss: 0.0986
2026-01-01 04:08:05 - INFO - PRINT: Epoch [2656], Train Loss: 0.1007, Validation Loss: 0.0986
2026-01-01 04:08:32 - INFO - PRINT: Epoch [2657], Train Loss: 0.1003, Validation Loss: 0.0986
2026-01-01 04:08:59 - INFO - PRINT: Epoch [2658], Train Loss: 0.1010, Validation Loss: 0.0986
2026-01-01 04:09:25 - INFO - PRINT: Epoch [2659], Train Loss: 0.1017, Validation Loss: 0.0986
2026-01-01 04:09:56 - INFO - PRINT: Epoch [2660], Train Loss: 0.1006, Validation Loss: 0.0981
2026-01-01 04:10:23 - INFO - PRINT: Epoch [2661], Train Loss: 0.1000, Validation Loss: 0.0981
2026-01-01 04:10:50 - INFO - PRINT: Epoch [2662], Train Loss: 0.1007, Validation Loss: 0.0981
2026-01-01 04:11:16 - INFO - PRINT: Epoch [2663], Train Loss: 0.1006, Validation Loss: 0.0981
2026-01-01 04:11:43 - INFO - PRINT: Epoch [2664], Train Loss: 0.1000, Validation Loss: 0.0981
2026-01-01 04:12:10 - INFO - PRINT: Epoch [2665], Train Loss: 0.0996, Validation Loss: 0.0981
2026-01-01 04:12:36 - INFO - PRINT: Epoch [2666], Train Loss: 0.1001, Validation Loss: 0.0981
2026-01-01 04:13:03 - INFO - PRINT: Epoch [2667], Train Loss: 0.0999, Validation Loss: 0.0981
2026-01-01 04:13:30 - INFO - PRINT: Epoch [2668], Train Loss: 0.0999, Validation Loss: 0.0981
2026-01-01 04:13:57 - INFO - PRINT: Epoch [2669], Train Loss: 0.1001, Validation Loss: 0.0981
2026-01-01 04:14:23 - INFO - PRINT: Epoch [2670], Train Loss: 0.1002, Validation Loss: 0.0981
2026-01-01 04:14:50 - INFO - PRINT: Epoch [2671], Train Loss: 0.1013, Validation Loss: 0.0981
2026-01-01 04:15:17 - INFO - PRINT: Epoch [2672], Train Loss: 0.1011, Validation Loss: 0.0981
2026-01-01 04:15:43 - INFO - PRINT: Epoch [2673], Train Loss: 0.1001, Validation Loss: 0.0981
2026-01-01 04:16:10 - INFO - PRINT: Epoch [2674], Train Loss: 0.1003, Validation Loss: 0.0981
2026-01-01 04:16:37 - INFO - PRINT: Epoch [2675], Train Loss: 0.1014, Validation Loss: 0.0981
2026-01-01 04:17:04 - INFO - PRINT: Epoch [2676], Train Loss: 0.1003, Validation Loss: 0.0981
2026-01-01 04:17:30 - INFO - PRINT: Epoch [2677], Train Loss: 0.1011, Validation Loss: 0.0981
2026-01-01 04:17:57 - INFO - PRINT: Epoch [2678], Train Loss: 0.1009, Validation Loss: 0.0981
2026-01-01 04:18:24 - INFO - PRINT: Epoch [2679], Train Loss: 0.0998, Validation Loss: 0.0981
2026-01-01 04:18:55 - INFO - PRINT: Epoch [2680], Train Loss: 0.1006, Validation Loss: 0.0988
2026-01-01 04:19:21 - INFO - PRINT: Epoch [2681], Train Loss: 0.1003, Validation Loss: 0.0988
2026-01-01 04:19:48 - INFO - PRINT: Epoch [2682], Train Loss: 0.1005, Validation Loss: 0.0988
2026-01-01 04:20:15 - INFO - PRINT: Epoch [2683], Train Loss: 0.0997, Validation Loss: 0.0988
2026-01-01 04:20:41 - INFO - PRINT: Epoch [2684], Train Loss: 0.0998, Validation Loss: 0.0988
2026-01-01 04:21:08 - INFO - PRINT: Epoch [2685], Train Loss: 0.1007, Validation Loss: 0.0988
2026-01-01 04:21:35 - INFO - PRINT: Epoch [2686], Train Loss: 0.1012, Validation Loss: 0.0988
2026-01-01 04:22:02 - INFO - PRINT: Epoch [2687], Train Loss: 0.1000, Validation Loss: 0.0988
2026-01-01 04:22:28 - INFO - PRINT: Epoch [2688], Train Loss: 0.1007, Validation Loss: 0.0988
2026-01-01 04:22:55 - INFO - PRINT: Epoch [2689], Train Loss: 0.1000, Validation Loss: 0.0988
2026-01-01 04:23:22 - INFO - PRINT: Epoch [2690], Train Loss: 0.1001, Validation Loss: 0.0988
2026-01-01 04:23:48 - INFO - PRINT: Epoch [2691], Train Loss: 0.0999, Validation Loss: 0.0988
2026-01-01 04:24:15 - INFO - PRINT: Epoch [2692], Train Loss: 0.1007, Validation Loss: 0.0988
2026-01-01 04:24:42 - INFO - PRINT: Epoch [2693], Train Loss: 0.1010, Validation Loss: 0.0988
2026-01-01 04:25:09 - INFO - PRINT: Epoch [2694], Train Loss: 0.1003, Validation Loss: 0.0988
2026-01-01 04:25:35 - INFO - PRINT: Epoch [2695], Train Loss: 0.0999, Validation Loss: 0.0988
2026-01-01 04:26:02 - INFO - PRINT: Epoch [2696], Train Loss: 0.1003, Validation Loss: 0.0988
2026-01-01 04:26:29 - INFO - PRINT: Epoch [2697], Train Loss: 0.1004, Validation Loss: 0.0988
2026-01-01 04:26:56 - INFO - PRINT: Epoch [2698], Train Loss: 0.0996, Validation Loss: 0.0988
2026-01-01 04:27:22 - INFO - PRINT: Epoch [2699], Train Loss: 0.1012, Validation Loss: 0.0988
2026-01-01 04:27:53 - INFO - PRINT: Epoch [2700], Train Loss: 0.1007, Validation Loss: 0.1001
2026-01-01 04:27:53 - INFO - PRINT: ----> Saving model from epoch 2600 (val loss: 0.0974807034432888). Sublime!
2026-01-01 04:28:20 - INFO - PRINT: Epoch [2701], Train Loss: 0.1005, Validation Loss: 0.1001
2026-01-01 04:28:46 - INFO - PRINT: Epoch [2702], Train Loss: 0.1036, Validation Loss: 0.1001
2026-01-01 04:29:13 - INFO - PRINT: Epoch [2703], Train Loss: 0.1020, Validation Loss: 0.1001
2026-01-01 04:29:40 - INFO - PRINT: Epoch [2704], Train Loss: 0.1003, Validation Loss: 0.1001
2026-01-01 04:30:07 - INFO - PRINT: Epoch [2705], Train Loss: 0.1009, Validation Loss: 0.1001
2026-01-01 04:30:33 - INFO - PRINT: Epoch [2706], Train Loss: 0.0998, Validation Loss: 0.1001
2026-01-01 04:31:00 - INFO - PRINT: Epoch [2707], Train Loss: 0.0999, Validation Loss: 0.1001
2026-01-01 04:31:27 - INFO - PRINT: Epoch [2708], Train Loss: 0.1000, Validation Loss: 0.1001
2026-01-01 04:31:53 - INFO - PRINT: Epoch [2709], Train Loss: 0.1002, Validation Loss: 0.1001
2026-01-01 04:32:20 - INFO - PRINT: Epoch [2710], Train Loss: 0.1006, Validation Loss: 0.1001
2026-01-01 04:32:47 - INFO - PRINT: Epoch [2711], Train Loss: 0.1003, Validation Loss: 0.1001
2026-01-01 04:33:14 - INFO - PRINT: Epoch [2712], Train Loss: 0.0999, Validation Loss: 0.1001
2026-01-01 04:33:40 - INFO - PRINT: Epoch [2713], Train Loss: 0.1012, Validation Loss: 0.1001
2026-01-01 04:34:07 - INFO - PRINT: Epoch [2714], Train Loss: 0.1002, Validation Loss: 0.1001
2026-01-01 04:34:34 - INFO - PRINT: Epoch [2715], Train Loss: 0.1005, Validation Loss: 0.1001
2026-01-01 04:35:00 - INFO - PRINT: Epoch [2716], Train Loss: 0.1008, Validation Loss: 0.1001
2026-01-01 04:35:27 - INFO - PRINT: Epoch [2717], Train Loss: 0.1005, Validation Loss: 0.1001
2026-01-01 04:35:54 - INFO - PRINT: Epoch [2718], Train Loss: 0.1004, Validation Loss: 0.1001
2026-01-01 04:36:21 - INFO - PRINT: Epoch [2719], Train Loss: 0.0999, Validation Loss: 0.1001
2026-01-01 04:36:52 - INFO - PRINT: Epoch [2720], Train Loss: 0.1003, Validation Loss: 0.0986
2026-01-01 04:37:18 - INFO - PRINT: Epoch [2721], Train Loss: 0.1001, Validation Loss: 0.0986
2026-01-01 04:37:45 - INFO - PRINT: Epoch [2722], Train Loss: 0.1007, Validation Loss: 0.0986
2026-01-01 04:38:11 - INFO - PRINT: Epoch [2723], Train Loss: 0.1002, Validation Loss: 0.0986
2026-01-01 04:38:38 - INFO - PRINT: Epoch [2724], Train Loss: 0.1020, Validation Loss: 0.0986
2026-01-01 04:39:05 - INFO - PRINT: Epoch [2725], Train Loss: 0.1006, Validation Loss: 0.0986
2026-01-01 04:39:32 - INFO - PRINT: Epoch [2726], Train Loss: 0.0998, Validation Loss: 0.0986
2026-01-01 04:39:58 - INFO - PRINT: Epoch [2727], Train Loss: 0.1007, Validation Loss: 0.0986
2026-01-01 04:40:25 - INFO - PRINT: Epoch [2728], Train Loss: 0.1015, Validation Loss: 0.0986
2026-01-01 04:40:52 - INFO - PRINT: Epoch [2729], Train Loss: 0.1005, Validation Loss: 0.0986
2026-01-01 04:41:18 - INFO - PRINT: Epoch [2730], Train Loss: 0.1004, Validation Loss: 0.0986
2026-01-01 04:41:45 - INFO - PRINT: Epoch [2731], Train Loss: 0.1007, Validation Loss: 0.0986
2026-01-01 04:42:12 - INFO - PRINT: Epoch [2732], Train Loss: 0.1009, Validation Loss: 0.0986
2026-01-01 04:42:39 - INFO - PRINT: Epoch [2733], Train Loss: 0.1002, Validation Loss: 0.0986
2026-01-01 04:43:05 - INFO - PRINT: Epoch [2734], Train Loss: 0.0996, Validation Loss: 0.0986
2026-01-01 04:43:32 - INFO - PRINT: Epoch [2735], Train Loss: 0.1009, Validation Loss: 0.0986
2026-01-01 04:43:59 - INFO - PRINT: Epoch [2736], Train Loss: 0.1009, Validation Loss: 0.0986
2026-01-01 04:44:25 - INFO - PRINT: Epoch [2737], Train Loss: 0.0996, Validation Loss: 0.0986
2026-01-01 04:44:52 - INFO - PRINT: Epoch [2738], Train Loss: 0.1009, Validation Loss: 0.0986
2026-01-01 04:45:19 - INFO - PRINT: Epoch [2739], Train Loss: 0.1010, Validation Loss: 0.0986
2026-01-01 04:45:50 - INFO - PRINT: Epoch [2740], Train Loss: 0.1013, Validation Loss: 0.1002
2026-01-01 04:46:16 - INFO - PRINT: Epoch [2741], Train Loss: 0.1005, Validation Loss: 0.1002
2026-01-01 04:46:43 - INFO - PRINT: Epoch [2742], Train Loss: 0.1002, Validation Loss: 0.1002
2026-01-01 04:47:10 - INFO - PRINT: Epoch [2743], Train Loss: 0.1004, Validation Loss: 0.1002
2026-01-01 04:47:36 - INFO - PRINT: Epoch [2744], Train Loss: 0.0999, Validation Loss: 0.1002
2026-01-01 04:48:03 - INFO - PRINT: Epoch [2745], Train Loss: 0.1002, Validation Loss: 0.1002
2026-01-01 04:48:30 - INFO - PRINT: Epoch [2746], Train Loss: 0.0996, Validation Loss: 0.1002
2026-01-01 04:48:57 - INFO - PRINT: Epoch [2747], Train Loss: 0.1011, Validation Loss: 0.1002
2026-01-01 04:49:23 - INFO - PRINT: Epoch [2748], Train Loss: 0.1001, Validation Loss: 0.1002
2026-01-01 04:49:50 - INFO - PRINT: Epoch [2749], Train Loss: 0.1000, Validation Loss: 0.1002
2026-01-01 04:50:17 - INFO - PRINT: Epoch [2750], Train Loss: 0.1020, Validation Loss: 0.1002
2026-01-01 04:50:43 - INFO - PRINT: Epoch [2751], Train Loss: 0.1003, Validation Loss: 0.1002
2026-01-01 04:51:10 - INFO - PRINT: Epoch [2752], Train Loss: 0.1002, Validation Loss: 0.1002
2026-01-01 04:51:37 - INFO - PRINT: Epoch [2753], Train Loss: 0.1007, Validation Loss: 0.1002
2026-01-01 04:52:04 - INFO - PRINT: Epoch [2754], Train Loss: 0.1010, Validation Loss: 0.1002
2026-01-01 04:52:30 - INFO - PRINT: Epoch [2755], Train Loss: 0.1000, Validation Loss: 0.1002
2026-01-01 04:52:57 - INFO - PRINT: Epoch [2756], Train Loss: 0.1002, Validation Loss: 0.1002
2026-01-01 04:53:24 - INFO - PRINT: Epoch [2757], Train Loss: 0.1000, Validation Loss: 0.1002
2026-01-01 04:53:50 - INFO - PRINT: Epoch [2758], Train Loss: 0.0998, Validation Loss: 0.1002
2026-01-01 04:54:17 - INFO - PRINT: Epoch [2759], Train Loss: 0.1002, Validation Loss: 0.1002
2026-01-01 04:54:48 - INFO - PRINT: Epoch [2760], Train Loss: 0.1004, Validation Loss: 0.0987
2026-01-01 04:55:15 - INFO - PRINT: Epoch [2761], Train Loss: 0.1008, Validation Loss: 0.0987
2026-01-01 04:55:41 - INFO - PRINT: Epoch [2762], Train Loss: 0.1006, Validation Loss: 0.0987
2026-01-01 04:56:08 - INFO - PRINT: Epoch [2763], Train Loss: 0.1012, Validation Loss: 0.0987
2026-01-01 04:56:35 - INFO - PRINT: Epoch [2764], Train Loss: 0.1001, Validation Loss: 0.0987
2026-01-01 04:57:01 - INFO - PRINT: Epoch [2765], Train Loss: 0.1000, Validation Loss: 0.0987
2026-01-01 04:57:28 - INFO - PRINT: Epoch [2766], Train Loss: 0.1001, Validation Loss: 0.0987
2026-01-01 04:57:55 - INFO - PRINT: Epoch [2767], Train Loss: 0.1009, Validation Loss: 0.0987
2026-01-01 04:58:22 - INFO - PRINT: Epoch [2768], Train Loss: 0.1007, Validation Loss: 0.0987
2026-01-01 04:58:48 - INFO - PRINT: Epoch [2769], Train Loss: 0.1007, Validation Loss: 0.0987
2026-01-01 04:59:15 - INFO - PRINT: Epoch [2770], Train Loss: 0.0998, Validation Loss: 0.0987
2026-01-01 04:59:42 - INFO - PRINT: Epoch [2771], Train Loss: 0.0999, Validation Loss: 0.0987
2026-01-01 05:00:08 - INFO - PRINT: Epoch [2772], Train Loss: 0.1003, Validation Loss: 0.0987
2026-01-01 05:00:35 - INFO - PRINT: Epoch [2773], Train Loss: 0.1003, Validation Loss: 0.0987
2026-01-01 05:01:02 - INFO - PRINT: Epoch [2774], Train Loss: 0.0999, Validation Loss: 0.0987
2026-01-01 05:01:29 - INFO - PRINT: Epoch [2775], Train Loss: 0.1003, Validation Loss: 0.0987
2026-01-01 05:01:55 - INFO - PRINT: Epoch [2776], Train Loss: 0.1013, Validation Loss: 0.0987
2026-01-01 05:02:22 - INFO - PRINT: Epoch [2777], Train Loss: 0.1004, Validation Loss: 0.0987
2026-01-01 05:02:49 - INFO - PRINT: Epoch [2778], Train Loss: 0.1005, Validation Loss: 0.0987
2026-01-01 05:03:15 - INFO - PRINT: Epoch [2779], Train Loss: 0.1013, Validation Loss: 0.0987
2026-01-01 05:03:47 - INFO - PRINT: Epoch [2780], Train Loss: 0.1007, Validation Loss: 0.0986
2026-01-01 05:04:13 - INFO - PRINT: Epoch [2781], Train Loss: 0.1000, Validation Loss: 0.0986
2026-01-01 05:04:40 - INFO - PRINT: Epoch [2782], Train Loss: 0.1005, Validation Loss: 0.0986
2026-01-01 05:05:07 - INFO - PRINT: Epoch [2783], Train Loss: 0.1002, Validation Loss: 0.0986
2026-01-01 05:05:34 - INFO - PRINT: Epoch [2784], Train Loss: 0.0996, Validation Loss: 0.0986
2026-01-01 05:06:01 - INFO - PRINT: Epoch [2785], Train Loss: 0.0996, Validation Loss: 0.0986
2026-01-01 05:06:28 - INFO - PRINT: Epoch [2786], Train Loss: 0.1001, Validation Loss: 0.0986
2026-01-01 05:06:54 - INFO - PRINT: Epoch [2787], Train Loss: 0.1006, Validation Loss: 0.0986
2026-01-01 05:07:21 - INFO - PRINT: Epoch [2788], Train Loss: 0.1005, Validation Loss: 0.0986
2026-01-01 05:07:48 - INFO - PRINT: Epoch [2789], Train Loss: 0.1005, Validation Loss: 0.0986
2026-01-01 05:08:14 - INFO - PRINT: Epoch [2790], Train Loss: 0.1014, Validation Loss: 0.0986
2026-01-01 05:08:41 - INFO - PRINT: Epoch [2791], Train Loss: 0.1006, Validation Loss: 0.0986
2026-01-01 05:09:08 - INFO - PRINT: Epoch [2792], Train Loss: 0.1001, Validation Loss: 0.0986
2026-01-01 05:09:35 - INFO - PRINT: Epoch [2793], Train Loss: 0.1006, Validation Loss: 0.0986
2026-01-01 05:10:01 - INFO - PRINT: Epoch [2794], Train Loss: 0.0998, Validation Loss: 0.0986
2026-01-01 05:10:28 - INFO - PRINT: Epoch [2795], Train Loss: 0.1000, Validation Loss: 0.0986
2026-01-01 05:10:55 - INFO - PRINT: Epoch [2796], Train Loss: 0.1012, Validation Loss: 0.0986
2026-01-01 05:11:22 - INFO - PRINT: Epoch [2797], Train Loss: 0.1000, Validation Loss: 0.0986
2026-01-01 05:11:48 - INFO - PRINT: Epoch [2798], Train Loss: 0.1006, Validation Loss: 0.0986
2026-01-01 05:12:15 - INFO - PRINT: Epoch [2799], Train Loss: 0.1009, Validation Loss: 0.0986
2026-01-01 05:12:46 - INFO - PRINT: Epoch [2800], Train Loss: 0.0997, Validation Loss: 0.0977
2026-01-01 05:12:46 - INFO - PRINT: ----> Saving model from epoch 2600 (val loss: 0.0974807034432888). Epicurean!
2026-01-01 05:13:13 - INFO - PRINT: Epoch [2801], Train Loss: 0.1005, Validation Loss: 0.0977
2026-01-01 05:13:39 - INFO - PRINT: Epoch [2802], Train Loss: 0.1002, Validation Loss: 0.0977
2026-01-01 05:14:06 - INFO - PRINT: Epoch [2803], Train Loss: 0.1005, Validation Loss: 0.0977
2026-01-01 05:14:33 - INFO - PRINT: Epoch [2804], Train Loss: 0.1001, Validation Loss: 0.0977
2026-01-01 05:15:00 - INFO - PRINT: Epoch [2805], Train Loss: 0.0996, Validation Loss: 0.0977
2026-01-01 05:15:26 - INFO - PRINT: Epoch [2806], Train Loss: 0.1005, Validation Loss: 0.0977
2026-01-01 05:15:53 - INFO - PRINT: Epoch [2807], Train Loss: 0.1010, Validation Loss: 0.0977
2026-01-01 05:16:20 - INFO - PRINT: Epoch [2808], Train Loss: 0.0998, Validation Loss: 0.0977
2026-01-01 05:16:46 - INFO - PRINT: Epoch [2809], Train Loss: 0.1001, Validation Loss: 0.0977
2026-01-01 05:17:13 - INFO - PRINT: Epoch [2810], Train Loss: 0.1003, Validation Loss: 0.0977
2026-01-01 05:17:40 - INFO - PRINT: Epoch [2811], Train Loss: 0.0999, Validation Loss: 0.0977
2026-01-01 05:18:07 - INFO - PRINT: Epoch [2812], Train Loss: 0.1016, Validation Loss: 0.0977
2026-01-01 05:18:33 - INFO - PRINT: Epoch [2813], Train Loss: 0.1010, Validation Loss: 0.0977
2026-01-01 05:19:00 - INFO - PRINT: Epoch [2814], Train Loss: 0.0998, Validation Loss: 0.0977
2026-01-01 05:19:27 - INFO - PRINT: Epoch [2815], Train Loss: 0.1000, Validation Loss: 0.0977
2026-01-01 05:19:53 - INFO - PRINT: Epoch [2816], Train Loss: 0.1001, Validation Loss: 0.0977
2026-01-01 05:20:20 - INFO - PRINT: Epoch [2817], Train Loss: 0.1005, Validation Loss: 0.0977
2026-01-01 05:20:47 - INFO - PRINT: Epoch [2818], Train Loss: 0.1004, Validation Loss: 0.0977
2026-01-01 05:21:14 - INFO - PRINT: Epoch [2819], Train Loss: 0.0995, Validation Loss: 0.0977
2026-01-01 05:21:45 - INFO - PRINT: Epoch [2820], Train Loss: 0.1000, Validation Loss: 0.0988
2026-01-01 05:22:11 - INFO - PRINT: Epoch [2821], Train Loss: 0.1004, Validation Loss: 0.0988
2026-01-01 05:22:38 - INFO - PRINT: Epoch [2822], Train Loss: 0.1008, Validation Loss: 0.0988
2026-01-01 05:23:04 - INFO - PRINT: Epoch [2823], Train Loss: 0.0997, Validation Loss: 0.0988
2026-01-01 05:23:31 - INFO - PRINT: Epoch [2824], Train Loss: 0.1009, Validation Loss: 0.0988
2026-01-01 05:23:58 - INFO - PRINT: Epoch [2825], Train Loss: 0.1001, Validation Loss: 0.0988
2026-01-01 05:24:25 - INFO - PRINT: Epoch [2826], Train Loss: 0.1006, Validation Loss: 0.0988
2026-01-01 05:24:51 - INFO - PRINT: Epoch [2827], Train Loss: 0.1005, Validation Loss: 0.0988
2026-01-01 05:25:18 - INFO - PRINT: Epoch [2828], Train Loss: 0.1007, Validation Loss: 0.0988
2026-01-01 05:25:45 - INFO - PRINT: Epoch [2829], Train Loss: 0.1008, Validation Loss: 0.0988
2026-01-01 05:26:11 - INFO - PRINT: Epoch [2830], Train Loss: 0.0999, Validation Loss: 0.0988
2026-01-01 05:26:38 - INFO - PRINT: Epoch [2831], Train Loss: 0.1006, Validation Loss: 0.0988
2026-01-01 05:27:05 - INFO - PRINT: Epoch [2832], Train Loss: 0.0996, Validation Loss: 0.0988
2026-01-01 05:27:32 - INFO - PRINT: Epoch [2833], Train Loss: 0.1002, Validation Loss: 0.0988
2026-01-01 05:27:58 - INFO - PRINT: Epoch [2834], Train Loss: 0.0999, Validation Loss: 0.0988
2026-01-01 05:28:25 - INFO - PRINT: Epoch [2835], Train Loss: 0.1001, Validation Loss: 0.0988
2026-01-01 05:28:52 - INFO - PRINT: Epoch [2836], Train Loss: 0.0996, Validation Loss: 0.0988
2026-01-01 05:29:18 - INFO - PRINT: Epoch [2837], Train Loss: 0.0998, Validation Loss: 0.0988
2026-01-01 05:29:45 - INFO - PRINT: Epoch [2838], Train Loss: 0.0998, Validation Loss: 0.0988
2026-01-01 05:30:12 - INFO - PRINT: Epoch [2839], Train Loss: 0.1006, Validation Loss: 0.0988
2026-01-01 05:30:43 - INFO - PRINT: Epoch [2840], Train Loss: 0.1009, Validation Loss: 0.0980
2026-01-01 05:31:09 - INFO - PRINT: Epoch [2841], Train Loss: 0.1003, Validation Loss: 0.0980
2026-01-01 05:31:36 - INFO - PRINT: Epoch [2842], Train Loss: 0.1010, Validation Loss: 0.0980
2026-01-01 05:32:03 - INFO - PRINT: Epoch [2843], Train Loss: 0.0995, Validation Loss: 0.0980
2026-01-01 05:32:29 - INFO - PRINT: Epoch [2844], Train Loss: 0.1001, Validation Loss: 0.0980
2026-01-01 05:32:56 - INFO - PRINT: Epoch [2845], Train Loss: 0.1002, Validation Loss: 0.0980
2026-01-01 05:33:23 - INFO - PRINT: Epoch [2846], Train Loss: 0.1011, Validation Loss: 0.0980
2026-01-01 05:33:50 - INFO - PRINT: Epoch [2847], Train Loss: 0.1001, Validation Loss: 0.0980
2026-01-01 05:34:16 - INFO - PRINT: Epoch [2848], Train Loss: 0.1022, Validation Loss: 0.0980
2026-01-01 05:34:43 - INFO - PRINT: Epoch [2849], Train Loss: 0.1000, Validation Loss: 0.0980
2026-01-01 05:35:10 - INFO - PRINT: Epoch [2850], Train Loss: 0.1001, Validation Loss: 0.0980
2026-01-01 05:35:36 - INFO - PRINT: Epoch [2851], Train Loss: 0.0996, Validation Loss: 0.0980
2026-01-01 05:36:03 - INFO - PRINT: Epoch [2852], Train Loss: 0.0997, Validation Loss: 0.0980
2026-01-01 05:36:30 - INFO - PRINT: Epoch [2853], Train Loss: 0.1003, Validation Loss: 0.0980
2026-01-01 05:36:57 - INFO - PRINT: Epoch [2854], Train Loss: 0.0995, Validation Loss: 0.0980
2026-01-01 05:37:23 - INFO - PRINT: Epoch [2855], Train Loss: 0.0998, Validation Loss: 0.0980
2026-01-01 05:37:50 - INFO - PRINT: Epoch [2856], Train Loss: 0.1001, Validation Loss: 0.0980
2026-01-01 05:38:17 - INFO - PRINT: Epoch [2857], Train Loss: 0.1003, Validation Loss: 0.0980
2026-01-01 05:38:43 - INFO - PRINT: Epoch [2858], Train Loss: 0.0999, Validation Loss: 0.0980
2026-01-01 05:39:10 - INFO - PRINT: Epoch [2859], Train Loss: 0.1016, Validation Loss: 0.0980
2026-01-01 05:39:41 - INFO - PRINT: Epoch [2860], Train Loss: 0.0999, Validation Loss: 0.0987
2026-01-01 05:40:08 - INFO - PRINT: Epoch [2861], Train Loss: 0.0996, Validation Loss: 0.0987
2026-01-01 05:40:34 - INFO - PRINT: Epoch [2862], Train Loss: 0.0997, Validation Loss: 0.0987
2026-01-01 05:41:01 - INFO - PRINT: Epoch [2863], Train Loss: 0.1008, Validation Loss: 0.0987
2026-01-01 05:41:28 - INFO - PRINT: Epoch [2864], Train Loss: 0.1002, Validation Loss: 0.0987
2026-01-01 05:41:54 - INFO - PRINT: Epoch [2865], Train Loss: 0.1000, Validation Loss: 0.0987
2026-01-01 05:42:21 - INFO - PRINT: Epoch [2866], Train Loss: 0.1002, Validation Loss: 0.0987
2026-01-01 05:42:48 - INFO - PRINT: Epoch [2867], Train Loss: 0.1008, Validation Loss: 0.0987
2026-01-01 05:43:15 - INFO - PRINT: Epoch [2868], Train Loss: 0.1014, Validation Loss: 0.0987
2026-01-01 05:43:41 - INFO - PRINT: Epoch [2869], Train Loss: 0.1000, Validation Loss: 0.0987
2026-01-01 05:44:08 - INFO - PRINT: Epoch [2870], Train Loss: 0.1002, Validation Loss: 0.0987
2026-01-01 05:44:35 - INFO - PRINT: Epoch [2871], Train Loss: 0.1000, Validation Loss: 0.0987
2026-01-01 05:45:01 - INFO - PRINT: Epoch [2872], Train Loss: 0.0999, Validation Loss: 0.0987
2026-01-01 05:45:28 - INFO - PRINT: Epoch [2873], Train Loss: 0.1005, Validation Loss: 0.0987
2026-01-01 05:45:55 - INFO - PRINT: Epoch [2874], Train Loss: 0.1002, Validation Loss: 0.0987
2026-01-01 05:46:22 - INFO - PRINT: Epoch [2875], Train Loss: 0.1004, Validation Loss: 0.0987
2026-01-01 05:46:48 - INFO - PRINT: Epoch [2876], Train Loss: 0.1003, Validation Loss: 0.0987
2026-01-01 05:47:15 - INFO - PRINT: Epoch [2877], Train Loss: 0.1000, Validation Loss: 0.0987
2026-01-01 05:47:42 - INFO - PRINT: Epoch [2878], Train Loss: 0.0999, Validation Loss: 0.0987
2026-01-01 05:48:08 - INFO - PRINT: Epoch [2879], Train Loss: 0.1000, Validation Loss: 0.0987
2026-01-01 05:48:39 - INFO - PRINT: Epoch [2880], Train Loss: 0.1013, Validation Loss: 0.0994
2026-01-01 05:49:06 - INFO - PRINT: Epoch [2881], Train Loss: 0.1011, Validation Loss: 0.0994
2026-01-01 05:49:33 - INFO - PRINT: Epoch [2882], Train Loss: 0.1002, Validation Loss: 0.0994
2026-01-01 05:49:59 - INFO - PRINT: Epoch [2883], Train Loss: 0.1017, Validation Loss: 0.0994
2026-01-01 05:50:26 - INFO - PRINT: Epoch [2884], Train Loss: 0.1003, Validation Loss: 0.0994
2026-01-01 05:50:53 - INFO - PRINT: Epoch [2885], Train Loss: 0.1000, Validation Loss: 0.0994
2026-01-01 05:51:19 - INFO - PRINT: Epoch [2886], Train Loss: 0.0995, Validation Loss: 0.0994
2026-01-01 05:51:46 - INFO - PRINT: Epoch [2887], Train Loss: 0.1003, Validation Loss: 0.0994
2026-01-01 05:52:13 - INFO - PRINT: Epoch [2888], Train Loss: 0.0996, Validation Loss: 0.0994
2026-01-01 05:52:40 - INFO - PRINT: Epoch [2889], Train Loss: 0.1003, Validation Loss: 0.0994
2026-01-01 05:53:06 - INFO - PRINT: Epoch [2890], Train Loss: 0.0993, Validation Loss: 0.0994
2026-01-01 05:53:33 - INFO - PRINT: Epoch [2891], Train Loss: 0.1001, Validation Loss: 0.0994
2026-01-01 05:54:00 - INFO - PRINT: Epoch [2892], Train Loss: 0.1002, Validation Loss: 0.0994
2026-01-01 05:54:26 - INFO - PRINT: Epoch [2893], Train Loss: 0.1003, Validation Loss: 0.0994
2026-01-01 05:54:53 - INFO - PRINT: Epoch [2894], Train Loss: 0.0996, Validation Loss: 0.0994
2026-01-01 05:55:20 - INFO - PRINT: Epoch [2895], Train Loss: 0.1001, Validation Loss: 0.0994
2026-01-01 05:55:46 - INFO - PRINT: Epoch [2896], Train Loss: 0.1003, Validation Loss: 0.0994
2026-01-01 05:56:13 - INFO - PRINT: Epoch [2897], Train Loss: 0.0999, Validation Loss: 0.0994
2026-01-01 05:56:40 - INFO - PRINT: Epoch [2898], Train Loss: 0.1006, Validation Loss: 0.0994
2026-01-01 05:57:07 - INFO - PRINT: Epoch [2899], Train Loss: 0.1004, Validation Loss: 0.0994
2026-01-01 05:57:38 - INFO - PRINT: Epoch [2900], Train Loss: 0.0999, Validation Loss: 0.0976
2026-01-01 05:57:38 - INFO - PRINT: ----> Saving model from epoch 2600 (val loss: 0.0974807034432888). Zingy!
2026-01-01 05:58:04 - INFO - PRINT: Epoch [2901], Train Loss: 0.0996, Validation Loss: 0.0976
2026-01-01 05:58:31 - INFO - PRINT: Epoch [2902], Train Loss: 0.1006, Validation Loss: 0.0976
2026-01-01 05:58:58 - INFO - PRINT: Epoch [2903], Train Loss: 0.0997, Validation Loss: 0.0976
2026-01-01 05:59:24 - INFO - PRINT: Epoch [2904], Train Loss: 0.1001, Validation Loss: 0.0976
2026-01-01 05:59:51 - INFO - PRINT: Epoch [2905], Train Loss: 0.1004, Validation Loss: 0.0976
2026-01-01 06:00:18 - INFO - PRINT: Epoch [2906], Train Loss: 0.1004, Validation Loss: 0.0976
2026-01-01 06:00:44 - INFO - PRINT: Epoch [2907], Train Loss: 0.1009, Validation Loss: 0.0976
2026-01-01 06:01:11 - INFO - PRINT: Epoch [2908], Train Loss: 0.0997, Validation Loss: 0.0976
2026-01-01 06:01:38 - INFO - PRINT: Epoch [2909], Train Loss: 0.0995, Validation Loss: 0.0976
2026-01-01 06:02:05 - INFO - PRINT: Epoch [2910], Train Loss: 0.1004, Validation Loss: 0.0976
2026-01-01 06:02:31 - INFO - PRINT: Epoch [2911], Train Loss: 0.0998, Validation Loss: 0.0976
2026-01-01 06:02:58 - INFO - PRINT: Epoch [2912], Train Loss: 0.1010, Validation Loss: 0.0976
2026-01-01 06:03:25 - INFO - PRINT: Epoch [2913], Train Loss: 0.1001, Validation Loss: 0.0976
2026-01-01 06:03:51 - INFO - PRINT: Epoch [2914], Train Loss: 0.1003, Validation Loss: 0.0976
2026-01-01 06:04:18 - INFO - PRINT: Epoch [2915], Train Loss: 0.1000, Validation Loss: 0.0976
2026-01-01 06:04:45 - INFO - PRINT: Epoch [2916], Train Loss: 0.1003, Validation Loss: 0.0976
2026-01-01 06:05:12 - INFO - PRINT: Epoch [2917], Train Loss: 0.0998, Validation Loss: 0.0976
2026-01-01 06:05:38 - INFO - PRINT: Epoch [2918], Train Loss: 0.0998, Validation Loss: 0.0976
2026-01-01 06:06:05 - INFO - PRINT: Epoch [2919], Train Loss: 0.1006, Validation Loss: 0.0976
2026-01-01 06:06:36 - INFO - PRINT: Epoch [2920], Train Loss: 0.1000, Validation Loss: 0.0985
2026-01-01 06:07:02 - INFO - PRINT: Epoch [2921], Train Loss: 0.0999, Validation Loss: 0.0985
2026-01-01 06:07:29 - INFO - PRINT: Epoch [2922], Train Loss: 0.1000, Validation Loss: 0.0985
2026-01-01 06:07:56 - INFO - PRINT: Epoch [2923], Train Loss: 0.0995, Validation Loss: 0.0985
2026-01-01 06:08:23 - INFO - PRINT: Epoch [2924], Train Loss: 0.1009, Validation Loss: 0.0985
2026-01-01 06:08:49 - INFO - PRINT: Epoch [2925], Train Loss: 0.1003, Validation Loss: 0.0985
2026-01-01 06:09:16 - INFO - PRINT: Epoch [2926], Train Loss: 0.0997, Validation Loss: 0.0985
2026-01-01 06:09:43 - INFO - PRINT: Epoch [2927], Train Loss: 0.1008, Validation Loss: 0.0985
2026-01-01 06:10:10 - INFO - PRINT: Epoch [2928], Train Loss: 0.1006, Validation Loss: 0.0985
2026-01-01 06:10:36 - INFO - PRINT: Epoch [2929], Train Loss: 0.1009, Validation Loss: 0.0985
2026-01-01 06:11:03 - INFO - PRINT: Epoch [2930], Train Loss: 0.1010, Validation Loss: 0.0985
2026-01-01 06:11:30 - INFO - PRINT: Epoch [2931], Train Loss: 0.0999, Validation Loss: 0.0985
2026-01-01 06:11:56 - INFO - PRINT: Epoch [2932], Train Loss: 0.0999, Validation Loss: 0.0985
2026-01-01 06:12:23 - INFO - PRINT: Epoch [2933], Train Loss: 0.1000, Validation Loss: 0.0985
2026-01-01 06:12:50 - INFO - PRINT: Epoch [2934], Train Loss: 0.1003, Validation Loss: 0.0985
2026-01-01 06:13:17 - INFO - PRINT: Epoch [2935], Train Loss: 0.0999, Validation Loss: 0.0985
2026-01-01 06:13:43 - INFO - PRINT: Epoch [2936], Train Loss: 0.1000, Validation Loss: 0.0985
2026-01-01 06:14:10 - INFO - PRINT: Epoch [2937], Train Loss: 0.1017, Validation Loss: 0.0985
2026-01-01 06:14:37 - INFO - PRINT: Epoch [2938], Train Loss: 0.1000, Validation Loss: 0.0985
2026-01-01 06:15:03 - INFO - PRINT: Epoch [2939], Train Loss: 0.1011, Validation Loss: 0.0985
2026-01-01 06:15:35 - INFO - PRINT: Epoch [2940], Train Loss: 0.1009, Validation Loss: 0.0998
2026-01-01 06:16:01 - INFO - PRINT: Epoch [2941], Train Loss: 0.0995, Validation Loss: 0.0998
2026-01-01 06:16:28 - INFO - PRINT: Epoch [2942], Train Loss: 0.0997, Validation Loss: 0.0998
2026-01-01 06:16:54 - INFO - PRINT: Epoch [2943], Train Loss: 0.1005, Validation Loss: 0.0998
2026-01-01 06:17:21 - INFO - PRINT: Epoch [2944], Train Loss: 0.0998, Validation Loss: 0.0998
2026-01-01 06:17:48 - INFO - PRINT: Epoch [2945], Train Loss: 0.0998, Validation Loss: 0.0998
2026-01-01 06:18:15 - INFO - PRINT: Epoch [2946], Train Loss: 0.1001, Validation Loss: 0.0998
2026-01-01 06:18:41 - INFO - PRINT: Epoch [2947], Train Loss: 0.0995, Validation Loss: 0.0998
2026-01-01 06:19:08 - INFO - PRINT: Epoch [2948], Train Loss: 0.1003, Validation Loss: 0.0998
2026-01-01 06:19:35 - INFO - PRINT: Epoch [2949], Train Loss: 0.1003, Validation Loss: 0.0998
2026-01-01 06:20:01 - INFO - PRINT: Epoch [2950], Train Loss: 0.0996, Validation Loss: 0.0998
2026-01-01 06:20:28 - INFO - PRINT: Epoch [2951], Train Loss: 0.0996, Validation Loss: 0.0998
2026-01-01 06:20:55 - INFO - PRINT: Epoch [2952], Train Loss: 0.1014, Validation Loss: 0.0998
2026-01-01 06:21:22 - INFO - PRINT: Epoch [2953], Train Loss: 0.1005, Validation Loss: 0.0998
2026-01-01 06:21:48 - INFO - PRINT: Epoch [2954], Train Loss: 0.0996, Validation Loss: 0.0998
2026-01-01 06:22:15 - INFO - PRINT: Epoch [2955], Train Loss: 0.1002, Validation Loss: 0.0998
2026-01-01 06:22:42 - INFO - PRINT: Epoch [2956], Train Loss: 0.0999, Validation Loss: 0.0998
2026-01-01 06:23:09 - INFO - PRINT: Epoch [2957], Train Loss: 0.1000, Validation Loss: 0.0998
2026-01-01 06:23:35 - INFO - PRINT: Epoch [2958], Train Loss: 0.1004, Validation Loss: 0.0998
2026-01-01 06:24:02 - INFO - PRINT: Epoch [2959], Train Loss: 0.0999, Validation Loss: 0.0998
2026-01-01 06:24:33 - INFO - PRINT: Epoch [2960], Train Loss: 0.0998, Validation Loss: 0.0985
2026-01-01 06:25:00 - INFO - PRINT: Epoch [2961], Train Loss: 0.0996, Validation Loss: 0.0985
2026-01-01 06:25:26 - INFO - PRINT: Epoch [2962], Train Loss: 0.1001, Validation Loss: 0.0985
2026-01-01 06:25:53 - INFO - PRINT: Epoch [2963], Train Loss: 0.1001, Validation Loss: 0.0985
2026-01-01 06:26:20 - INFO - PRINT: Epoch [2964], Train Loss: 0.1007, Validation Loss: 0.0985
2026-01-01 06:26:46 - INFO - PRINT: Epoch [2965], Train Loss: 0.1014, Validation Loss: 0.0985
2026-01-01 06:27:13 - INFO - PRINT: Epoch [2966], Train Loss: 0.0997, Validation Loss: 0.0985
2026-01-01 06:27:40 - INFO - PRINT: Epoch [2967], Train Loss: 0.0999, Validation Loss: 0.0985
2026-01-01 06:28:07 - INFO - PRINT: Epoch [2968], Train Loss: 0.1005, Validation Loss: 0.0985
2026-01-01 06:28:33 - INFO - PRINT: Epoch [2969], Train Loss: 0.0999, Validation Loss: 0.0985
2026-01-01 06:29:00 - INFO - PRINT: Epoch [2970], Train Loss: 0.0995, Validation Loss: 0.0985
2026-01-01 06:29:27 - INFO - PRINT: Epoch [2971], Train Loss: 0.1010, Validation Loss: 0.0985
2026-01-01 06:29:53 - INFO - PRINT: Epoch [2972], Train Loss: 0.0997, Validation Loss: 0.0985
2026-01-01 06:30:20 - INFO - PRINT: Epoch [2973], Train Loss: 0.0995, Validation Loss: 0.0985
2026-01-01 06:30:47 - INFO - PRINT: Epoch [2974], Train Loss: 0.1002, Validation Loss: 0.0985
2026-01-01 06:31:14 - INFO - PRINT: Epoch [2975], Train Loss: 0.1001, Validation Loss: 0.0985
2026-01-01 06:31:40 - INFO - PRINT: Epoch [2976], Train Loss: 0.1000, Validation Loss: 0.0985
2026-01-01 06:32:07 - INFO - PRINT: Epoch [2977], Train Loss: 0.1001, Validation Loss: 0.0985
2026-01-01 06:32:34 - INFO - PRINT: Epoch [2978], Train Loss: 0.1003, Validation Loss: 0.0985
2026-01-01 06:33:00 - INFO - PRINT: Epoch [2979], Train Loss: 0.1007, Validation Loss: 0.0985
2026-01-01 06:33:32 - INFO - PRINT: Epoch [2980], Train Loss: 0.0997, Validation Loss: 0.0987
2026-01-01 06:33:58 - INFO - PRINT: Epoch [2981], Train Loss: 0.0998, Validation Loss: 0.0987
2026-01-01 06:34:25 - INFO - PRINT: Epoch [2982], Train Loss: 0.1000, Validation Loss: 0.0987
2026-01-01 06:34:51 - INFO - PRINT: Epoch [2983], Train Loss: 0.0999, Validation Loss: 0.0987
2026-01-01 06:35:18 - INFO - PRINT: Epoch [2984], Train Loss: 0.1003, Validation Loss: 0.0987
2026-01-01 06:35:45 - INFO - PRINT: Epoch [2985], Train Loss: 0.1004, Validation Loss: 0.0987
2026-01-01 06:36:12 - INFO - PRINT: Epoch [2986], Train Loss: 0.1001, Validation Loss: 0.0987
2026-01-01 06:36:38 - INFO - PRINT: Epoch [2987], Train Loss: 0.1004, Validation Loss: 0.0987
2026-01-01 06:37:05 - INFO - PRINT: Epoch [2988], Train Loss: 0.0998, Validation Loss: 0.0987
2026-01-01 06:37:32 - INFO - PRINT: Epoch [2989], Train Loss: 0.1006, Validation Loss: 0.0987
2026-01-01 06:37:58 - INFO - PRINT: Epoch [2990], Train Loss: 0.1001, Validation Loss: 0.0987
2026-01-01 06:38:25 - INFO - PRINT: Epoch [2991], Train Loss: 0.1004, Validation Loss: 0.0987
2026-01-01 06:38:52 - INFO - PRINT: Epoch [2992], Train Loss: 0.0999, Validation Loss: 0.0987
2026-01-01 06:39:19 - INFO - PRINT: Epoch [2993], Train Loss: 0.1008, Validation Loss: 0.0987
2026-01-01 06:39:45 - INFO - PRINT: Epoch [2994], Train Loss: 0.0998, Validation Loss: 0.0987
2026-01-01 06:40:12 - INFO - PRINT: Epoch [2995], Train Loss: 0.1002, Validation Loss: 0.0987
2026-01-01 06:40:39 - INFO - PRINT: Epoch [2996], Train Loss: 0.1003, Validation Loss: 0.0987
2026-01-01 06:41:05 - INFO - PRINT: Epoch [2997], Train Loss: 0.1002, Validation Loss: 0.0987
2026-01-01 06:41:32 - INFO - PRINT: Epoch [2998], Train Loss: 0.0998, Validation Loss: 0.0987
2026-01-01 06:41:59 - INFO - PRINT: Epoch [2999], Train Loss: 0.0996, Validation Loss: 0.0987
2026-01-01 06:41:59 - INFO - PRINT: ----> Saving model from epoch 2600 (val loss: 0.0974807034432888). Luxurious!
2026-01-01 06:41:59 - INFO - PRINT: ----> Running test eval on model from Epoch 2600
2026-01-01 06:41:59 - INFO - PRINT: WELCOME TO THE EXPERIMENT: 25_12_31_08:14:58_pet_pika_nearwall_boxfilter_4x_sr
2026-01-01 06:41:59 - INFO - PRINT: default dtype: torch.float32
2026-01-01 06:41:59 - INFO - PRINT: Getting timesteps available for channel_nearwall_filtered_fs4 in /home/rmcconke/orcd/scratch/numpy_4000
2026-01-01 06:41:59 - INFO - PRINT: Number of timesteps available: 4000
2026-01-01 06:42:14 - INFO - PRINT: Getting timesteps available for channel5200_middle_filtered_fs4 in /home/rmcconke/orcd/scratch/numpy_4000
2026-01-01 06:42:14 - INFO - PRINT: Number of timesteps available: 11
2026-01-01 06:42:15 - INFO - PRINT: Getting timesteps available for channel5200_nearwall_filtered_fs4 in /home/rmcconke/orcd/scratch/numpy_4000
2026-01-01 06:42:15 - INFO - PRINT: Number of timesteps available: 11
2026-01-01 06:42:22 - INFO - PRINT: Test Loss: 0.0857
2026-01-01 06:53:42 - INFO - PRINT: Test Re5200_middle Loss: 0.1021
2026-01-01 06:54:28 - INFO - PRINT: Test Re5200_nearwall Loss: 0.2018
